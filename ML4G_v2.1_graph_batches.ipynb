{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ef63b0-c9f1-44b7-ae9e-09210b4395c9",
   "metadata": {},
   "source": [
    "# Create graph from n-triple file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b7f1d5-1f5a-44d8-b1f2-764a729e737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph\n",
    "import logging\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "\n",
    "data_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped.nt'\n",
    "mini_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped_mini.nt'\n",
    "test_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_test_set.nt\"\n",
    "folder = './Downloads/ml4g'\n",
    "subsample_chance = 1\n",
    "def create_mini_nt(infile, outfile):\n",
    "    with open(infile, 'r', encoding='utf-8') as inp, open(outfile, 'w', encoding='utf-8') as out:\n",
    "        for i, line in enumerate(inp):\n",
    "            if random.random() <= subsample_chance:\n",
    "                out.write(line)\n",
    "\n",
    "\n",
    "\n",
    "def create_new_graph(path, batch_size = 10000, test=True):\n",
    "    \"\"\"\n",
    "    Reads .nt file to produce rdflib graph object containing all tuples.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "    filename='rdf_parsing_errors.log',\n",
    "    filemode='w')\n",
    "    \n",
    "    graph = Graph()\n",
    "    batch_num = 0\n",
    "    i = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        while True:\n",
    "            batch = []\n",
    "            try:\n",
    "                \n",
    "                [batch.append(next(f)) for j in range(batch_size)]\n",
    "                i += j\n",
    "            except:\n",
    "                pass\n",
    "            if not batch:\n",
    "                break\n",
    "            batch_num += 1\n",
    "            nt_string = ''.join(batch)\n",
    "            try:\n",
    "                graph.parse(data=nt_string, format='nt')\n",
    "                if test:\n",
    "                    graph = Graph()\n",
    "            except ParseError as e:\n",
    "                logging.error(f\"in batch: {batch_num}:\\npproblematic data:\\n\\n{batch}\\n\\n\")\n",
    "                check(batch, batch_num, test=test)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def check(batch, batch_num, test = True):\n",
    "    graph = Graph()\n",
    "    for i, line in enumerate(batch):\n",
    "        try:\n",
    "            graph.parse(line)\n",
    "        except Exception as e:\n",
    "            logging.error(f'in line: {i}:\\n{line}\\n{e}')\n",
    "g=create_new_graph(data_loc, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146bf08-a367-406d-8a00-2f0c4c89d067",
   "metadata": {},
   "source": [
    "### grab train,test,val graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4d5b489a-42cd-4f78-bd7d-d8945ea010ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import defaultdict\n",
    "test_label_set = set()\n",
    "train_label_set = set()\n",
    "val_label_set = set()\n",
    "\n",
    "\n",
    "test_node_label_map = defaultdict(list)\n",
    "train_node_label_map = defaultdict(list)\n",
    "val_node_label_map = defaultdict(list)\n",
    "\n",
    "\n",
    "test_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_test_set.nt\"\n",
    "train_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_train_set.nt\"\n",
    "val_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_valid_set.nt\"\n",
    "\n",
    "graph_test = create_new_graph(test_loc, test=False)\n",
    "graph_train = create_new_graph(train_loc, test=False)\n",
    "graph_val = create_new_graph(val_loc, test=False)\n",
    "\n",
    "for s,p,o in graph_test:\n",
    "    test_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        test_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "for s,p,o in graph_train:\n",
    "    train_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        train_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "\n",
    "for s,p,o in graph_val:\n",
    "    val_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        val_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "\n",
    "subj_set = set()\n",
    "for s,p,o in graph_train:\n",
    "    subj_set.add(s.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefd34b-7e40-4583-8b6c-3468a39fa917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# unique_classes = set()\n",
    "# unique_predicates = set()\n",
    "# for s,p,o in graph2:\n",
    "#     unique_predicates.add(p)\n",
    "#     if i< 35:\n",
    "#         # print(s,p,o)\n",
    "#         pass\n",
    "#     unique_classes.add(o)\n",
    "# unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090de70-b583-46ee-95b9-66a4961c91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7b49d778-bc05-44ee-a7f5-cf50c2b9eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103838-->5001\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "import random\n",
    "def to_connected_small(g,subsample_chance=0.8, barrier=100,max_len=5000, verbose=True, save=True):\n",
    "    \"\"\"\n",
    "    Creates a small graph for development.\n",
    "    Uses similar logic to create_small_graphs()\n",
    "    \"\"\"\n",
    "    obj_set = set()\n",
    "    mini_graph = Graph()\n",
    "    i=0\n",
    "    j=0\n",
    "    for s,p,o in g:\n",
    "        if j > max_len:\n",
    "            break\n",
    "        i+=1\n",
    "        if random.random() >= subsample_chance and (i<barrier or s in obj_set or o in obj_set):\n",
    "            obj_set.add(s)\n",
    "            obj_set.add(o)\n",
    "            j+=1\n",
    "            mini_graph.add(triple=(s,p,o))\n",
    "    print(f\"{i}-->{j}\") if verbose else 0\n",
    "    return mini_graph\n",
    "graph = to_connected_small(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "892957c9-1e49-4af8-92aa-1c1d62bb5fd0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83561-->5001\n",
      "60048-->5001\n",
      "49227-->5001\n",
      "45508-->5001\n",
      "41648-->5001\n",
      "38941-->5001\n",
      "37888-->5001\n",
      "36168-->5001\n",
      "34804-->5001\n",
      "33049-->5001\n",
      "33457-->5001\n",
      "32821-->5001\n",
      "31643-->5001\n",
      "30544-->5001\n",
      "29562-->5001\n",
      "29948-->5001\n",
      "29402-->5001\n",
      "29121-->5001\n",
      "28779-->5001\n",
      "27989-->5001\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "import random\n",
    "import pickle\n",
    "def create_small_graphs(g,subsample_chance=0.8, barrier=100,max_len=5000, verbose=True, save=True, graph_loc=f'{folder}/graphs/'):\n",
    "    \"\"\"\n",
    "    Creates batches to be processed by preprocess().\n",
    "    Generator logic is currently unimplemented.    \n",
    "    \"\"\"\n",
    "    done=False\n",
    "    # while not done:\n",
    "    obj_set = set()\n",
    "    mini_graph = Graph()\n",
    "    i=0\n",
    "    j=0\n",
    "    graph_num=0\n",
    "    for s,p,o in g:\n",
    "        if j > max_len:\n",
    "            graph_name = f'graph_{graph_num}.pkl'\n",
    "            if save:\n",
    "                print(f\"{i}-->{j}\") if verbose else 0\n",
    "                with open(graph_loc+graph_name, 'wb') as f:\n",
    "                    pickle.dump(mini_graph, f)\n",
    "            mini_graph = Graph()\n",
    "            i,j = 0,0\n",
    "            graph_num += 1\n",
    "        i+=1\n",
    "        if random.random() >= subsample_chance and (i<barrier or s in obj_set or o in obj_set):\n",
    "            obj_set.add(s)\n",
    "            obj_set.add(o)\n",
    "            j+=1\n",
    "            mini_graph.add(triple=(s,p,o))\n",
    "        \n",
    "        \n",
    "        \n",
    "    # return mini_graph\n",
    "create_small_graphs(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d1db7-ff84-4dc5-bda5-e74a315a5d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for s,p,o in graph:\n",
    "    if i < 20:\n",
    "        # print(s,p,o)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd6b40-16da-40e5-8763-4444b9e31865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a57e8b-fb25-4b03-9865-3b0b786e61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733ae1d-f51b-4bf0-9fef-fa093f405b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(edge_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4645d0-128d-406b-9fff-711c039c4b6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file_path = os.path.join(folder, 'rdf_graph_l.pkl')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(g, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80590000-b2f4-4a06-80fc-d70ae965d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pickle\n",
    "import os\n",
    "import rdflib\n",
    "import logging\n",
    "folder = './Downloads/ml4g'\n",
    "\n",
    "pickle_file_path = os.path.join(folder, 'rdf_graph.pkl')\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ef0fb2b0-b4fc-42b1-8a3c-75e7637b6bc0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import defaultdict\n",
    "import torchvision.transforms as trv\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from string import printable\n",
    "import pickle\n",
    "import base64\n",
    "\n",
    "def decode_base64_image(encoded_str,log_note='pass values to decode_base_64_jpg'): # - to +, _ to /\n",
    "        \"\"\"\n",
    "        encoded_str: url safe base 64 jpg string --> image bytes string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_bytes = base64.urlsafe_b64decode(encoded_str)\n",
    "            image_obj = Image.open(BytesIO(image_bytes))\n",
    "            image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "            return image, image_obj\n",
    "        except Exception as e:\n",
    "            # logging.error(f\"{e} error encoding image at {log_note}\")\n",
    "            print(e)\n",
    "            return None, None\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, strings, character_map, max_length=5000):\n",
    "        # self.strings = strings\n",
    "        self.character_map = character_map\n",
    "        self.max_length = max_length\n",
    "        self.strings = strings\n",
    "        self.ids = []\n",
    "        self.pad = False\n",
    "        ids = []\n",
    "        for string in strings:\n",
    "            try:\n",
    "                self.ids.append(inv_node_map[string])\n",
    "            except KeyError:\n",
    "                self.ids.append(0) #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strings)\n",
    "\n",
    "    def __getitem__(self, i, pad=True):\n",
    "        s = self.strings[i]\n",
    "        tokens = tokenize_string(s, self.character_map, max_len=self.max_length)\n",
    "\n",
    "        return torch.tensor([self.ids[i]]+tokens, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, encoded_images, transform):\n",
    "        self.encoded_images = encoded_images\n",
    "        self.transform = transform\n",
    "        self.ids = [inv_node_map[encoded_image] for encoded_image in encoded_images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        encoded_str = self.encoded_images[i]\n",
    "        image, _ = decode_base64_image(encoded_str, log_note=f\"inside generator, image nr: {i}\")\n",
    "        # print(f\"img:{image}\")\n",
    "        if image:\n",
    "            # print(image)\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # print(image)\n",
    "            image = torch.zeros(128)\n",
    "            # image = torch.zeros(3, 224, 224)\n",
    "        return self.ids[i], image\n",
    "\n",
    "\n",
    "class NumericalDataset(Dataset):\n",
    "    def __init__(self, numbers, log_scale=False):\n",
    "        self.numbers = numbers\n",
    "        self.log_scale = log_scale\n",
    "        self.max_num = max(numbers)\n",
    "        self.ids = [inv_node_map[str(number)] for number in numbers]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        n = self.numbers[i]\n",
    "        norm = n / self.max_num\n",
    "        norm = math.log(norm + 1e-8) if self.log_scale else norm\n",
    "        return self.ids[i], torch.tensor([norm], dtype=torch.float32, device=device)\n",
    "\n",
    "class DateTimeDataset(Dataset):\n",
    "    # todo: check if there are other temporal feature types in the data. i.e. just years etc.\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates#torch.stack([get_dates(date) for date in dates], dim=0)\n",
    "        self.ids = [inv_node_map[date] for date in dates]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_date(date)\n",
    "        return self.ids[i], date_feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class YearDataset(Dataset):\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates\n",
    "        self.ids = [inv_node_map[date] for date in dates]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_year(date)\n",
    "        return self.ids[i], date_feature\n",
    "\n",
    "\n",
    "def process_point(point_str, highest_x, highest_y):\n",
    "    if 'POINT' in point_str:\n",
    "        point_str = point_str.split('POINT(')[1].split('))')[0]\n",
    "    elif 'Point' in point_str:\n",
    "        point_str = point_str.split('Point(')[1].split('))')[0]\n",
    "    point_list = point_str.strip(')').strip('(').split()\n",
    "    point = [float(coord) for coord in point_list]\n",
    "    point.extend([0,0,0])\n",
    "    point = tuple(point)\n",
    "    \n",
    "    point_x,point_y,_,_,_ = point\n",
    "    highest_x = point_x if point_x > highest_x else highest_x\n",
    "    highest_y = point_y if point_y > highest_y else highest_y\n",
    "    return point, highest_x, highest_y\n",
    "\n",
    "    \n",
    "def process_multipoly(multipoly_str, max_x, max_y):\n",
    "    \"\"\"\n",
    "    Delegates polygon processing for multipolygons\n",
    "    \"\"\"\n",
    "    multipoly_str = multipoly_str.split('(((')[-1].split(')))')[0]\n",
    "    sub_polys = multipoly_str.split(') (')\n",
    "    multipoly = []\n",
    "    for i, sub_poly in enumerate(sub_polys):\n",
    "        aligned_sub_poly = 'POLYGON ((' + sub_poly + '))'\n",
    "        multipoly.append(get_num_data(aligned_sub_poly, max_x, max_y, last_multi=i==len(sub_polys)))\n",
    "    return multipoly\n",
    "        \n",
    "\n",
    "def get_num_data(poly_str, maximum_x, maximum_y, last_multi=False):\n",
    "    \"\"\"\n",
    "    Processes polygons\n",
    "    str --> tuples, min/max data\n",
    "    \"\"\"\n",
    "    poly_str = poly_str.split('(')[-1].split(')')[0]\n",
    "    poly_combi_str_list = [poly for poly in poly_str.split(',')]\n",
    "    try:\n",
    "        poly_tupled = [(float(poly.split()[0]),float(poly.split()[1]),1,0,0) if i < len(poly_combi_str_list)-1  else (float(poly.split()[0]),float(poly.split()[1]),0,0,1) if last_multi else (float(poly.split()[0]),float(poly.split()[1]),0,1,0)\n",
    "                       for i,poly in enumerate(poly_combi_str_list)]\n",
    "    except ValueError:\n",
    "        poly_tupled = [(float(poly.strip(')').strip('(').split()[0]),float(poly.strip(')').strip('(').split()[1]),1,0,0) if i < len(poly_combi_str_list)-1  else (float(poly.split()[0]),float(poly.split()[1]),0,0,1) if last_multi else (float(poly.split()[0]),float(poly.split()[1]),0,0,1)\n",
    "                       for i,poly in enumerate(poly_combi_str_list)]\n",
    "    x_max, y_max = max([x for x,y,_,_,_ in poly_tupled]), max([y for x, y,_,_,_ in poly_tupled])\n",
    "\n",
    "    x_max =  x_max if x_max > maximum_x else maximum_x\n",
    "    y_max =  y_max if y_max > maximum_y else maximum_y\n",
    "    x_mean = sum([tup[0] for tup in poly_tupled])/len(poly_tupled)\n",
    "    y_mean = sum([tup[1] for tup in poly_tupled])/len(poly_tupled)\n",
    "    return poly_tupled, x_mean, y_mean, x_max, y_max\n",
    "\n",
    "def tokenize_string(s, character_map, max_len=5000):\n",
    "    s = s[:max_len]\n",
    "    tokens = [character_map[char] for char in s]\n",
    "    return tokens #\n",
    "\n",
    "def encode_image(encoded_str, transform):\n",
    "    #preprocess for encoder[2](2 layer cnn -->embedding_dim)\n",
    "    image, img_bytes = decode_base64_image(encoded_str)\n",
    "    return transform(image), img_bytes\n",
    "\n",
    "def encode_num(n, max_num, log=False):\n",
    "    v = n/max_num\n",
    "    v = math.log(v) if log else v\n",
    "    return torch.tensor((v),dtype=torch.float32, device=device)\n",
    "\n",
    "def encode_polygon(poly, global_mean_x, global_mean_y, x_max, y_max):\n",
    "    poly_tensor_x = (global_mean_x-torch.tensor([ point[0] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/x_max\n",
    "    poly_tensor_y = (global_mean_y-torch.tensor([ point[1] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/y_max\n",
    "    return torch.stack((poly_tensor_x,poly_tensor_y),dim=0)\n",
    "\n",
    "def encode_point(point, max_x, max_y):\n",
    "    \n",
    "    point = torch.tensor(point,dtype=torch.float32, device=device)\n",
    "    div = torch.tensor((max_x,max_y),dtype=torch.float32, device=device)\n",
    "    return point/div\n",
    "\n",
    "def encode_date(date):\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    split_str = date.split('-')\n",
    "    years_str = split_str[0]\n",
    "    month_str = split_str[1]\n",
    "    day_str = split_str[2]\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    months = cyclical(int(month_str), 12)\n",
    "    days = cyclical(int(day_str), 31)\n",
    "    return torch.cat((centuries, decades, years, months, days), dim=0)\n",
    "\n",
    "def encode_year(year):\n",
    "    #preprocess for temporal encoder[4](? layer ffnn -->embedding_dim)\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "        \n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    years_str = year\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    return torch.cat((centuries, decades, years), dim=0)\n",
    "\n",
    "\n",
    "class SpatialDataset(Dataset):\n",
    "    def __init__(self, spatial_data, global_mean_x, global_mean_y, x_max, y_max, ids):\n",
    "        self.spatial_data = spatial_data\n",
    "        self.global_mean_x = global_mean_x\n",
    "        self.global_mean_y = global_mean_y\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "        try:\n",
    "            self.max_len = max([len(data) for data in spatial_data])\n",
    "        except:\n",
    "            self.max_len = 0\n",
    "        self.ids = ids#[inv_node_map[spatial_datum] for spatial_datum in spatial_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spatial_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        spatial = self.spatial_data[i]\n",
    "        spatial_tensors = []\n",
    "        if isinstance(spatial, list):\n",
    "            for x,y,a,b,c in spatial:\n",
    "                \n",
    "                x = (x - self.global_mean_x) / self.x_max\n",
    "                y = (y - self.global_mean_y) / self.y_max\n",
    "                coord_tensor = torch.tensor([x,y,a,b,c], device=device)\n",
    "                spatial_tensors.append(coord_tensor)\n",
    "            spatial_tensor = torch.stack(spatial_tensors,dim=0)\n",
    "        elif isinstance(spatial, tuple):\n",
    "            x, y,_,_,_ = spatial\n",
    "            spatial_tensor = torch.tensor([(x / self.x_max, y / self.y_max,0,0,0)], dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            spatial_tensor = torch.zeros((1, 2), dtype=torch.float32, device=device)\n",
    "        return self.ids[i], spatial_tensor\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def save_bytes_to_jpg(image_bytes, item_num=0,folder='Downloads/ml4g/image_data/',name='Fred'):\n",
    "        filename = f'{folder}{name}_{item_num}.jpg'\n",
    "        with open(filename, 'wb') as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "        \n",
    "        class DateTimeDataset(Dataset):\n",
    "            # todo: check if there are other temporal feature types in the data. i.e. just years etc.\n",
    "            def __init__(self, dates):\n",
    "                \n",
    "                self.dates = dates\n",
    "                self.ids = [inv_node_map[date] for date in dates]\n",
    "    \n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.dates)\n",
    "    \n",
    "        def __getitem__(self, i):\n",
    "            date = self.dates[i]\n",
    "            date_feature = encode_date(date)\n",
    "            return self.ids[i], date_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "a1a7a1df-aa03-4867-b493-41b5e47c30d3",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def preprocess(graph):\n",
    "    \"\"\"\n",
    "    graph --> datasets\n",
    "    \"\"\"\n",
    "    global inv_node_map \n",
    "    global node_map\n",
    "    global edge_map\n",
    "    global inv_edge_map\n",
    "    global device\n",
    "    global img_list\n",
    "    global transform_temp\n",
    "    \n",
    "    \n",
    "    node_set = set()\n",
    "    http_set_s = set()\n",
    "    http_set_m = set()\n",
    "    http_set_l = set()\n",
    "    \n",
    "    edge_set = set()\n",
    "    string_set_s = set()\n",
    "    string_set_m = set()\n",
    "    string_set_l = set()\n",
    "    \n",
    "    image_set = set()\n",
    "    num_set = set()\n",
    "    poly_set = set()\n",
    "    datetime_set = set()\n",
    "    date_set = set()\n",
    "    year_set = set()\n",
    "    point_set = set()\n",
    "    \n",
    "    text_edge_set_s = set()\n",
    "    text_edge_set_m = set()\n",
    "    text_edge_set_l = set()\n",
    "    \n",
    "    image_edge_set = set()\n",
    "    num_edge_set = set()\n",
    "    spatial_edge_set = set()\n",
    "    temporal_edge_set = set()\n",
    "    encoder_map = defaultdict(list)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    dtype_set = set()\n",
    "    def is_date(date_string):\n",
    "        try:\n",
    "            rdflib.term.parse_datetime(date_string)\n",
    "            return 'datetime'\n",
    "        except ValueError:\n",
    "            try:\n",
    "                rdflib.term.parse_xsd_gyear(date_string)\n",
    "                return 'year'\n",
    "            except:\n",
    "                try:\n",
    "                    rdflib.term.parse_xsd_date(date_string)\n",
    "                    return 'date'\n",
    "                except:\n",
    "                    return False\n",
    "\n",
    "    def add_str_to_set(string,edge):\n",
    "        if len(string) < 20:\n",
    "            string_set_s.add(string)\n",
    "            text_edge_set_s.add(edge)\n",
    "        elif len(string) < 50:\n",
    "            string_set_m.add(string)\n",
    "            text_edge_set_m.add(edge)\n",
    "        else:\n",
    "            string_set_l.add(string)\n",
    "            text_edge_set_l.add(edge)\n",
    "    \n",
    "    \n",
    "    for s,p,o in graph:\n",
    "        i+=1\n",
    "        pi = p.identifier\n",
    "        if s.identifier in subj_set:\n",
    "            pass\n",
    "        for node in [s,o]:\n",
    "            ni = node.identifier\n",
    "            node_set.add(ni)\n",
    "            try:\n",
    "                dtype = node.datatype.identifier\n",
    "                dtype_set.add(dtype)\n",
    "            except AttributeError:\n",
    "                dtype = ''\n",
    "            if 'http' in ni[:200]: #200, because images sometimes have kgbench url attached\n",
    "                if len(ni) < 20:\n",
    "                    http_set_s.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "                elif len(ni) < 50:\n",
    "                    http_set_m.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "                else:\n",
    "                    http_set_l.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "    \n",
    "            else:\n",
    "                if is_date(ni) and ('Year' in dtype or 'date' in dtype):\n",
    "                    date_type = is_date(ni)\n",
    "                    if date_type == 'datetime':\n",
    "                        datetime_set.add(ni)\n",
    "                    elif date_type == 'year':\n",
    "                        year_set.add(ni)\n",
    "                    elif date_type == 'date':\n",
    "                        date_set.add(ni)\n",
    "                        \n",
    "                elif node.isalnum():\n",
    "                    if node.isnumeric():\n",
    "                        if node.isdigit():\n",
    "                            num_set.add(int(node.identifier))\n",
    "                            num_edge_set.add(pi)\n",
    "                        else:\n",
    "                            num_set.add(float(node.identifier))\n",
    "                elif ni.startswith('POINT') or ni.startswith('Point'): #didn't see any points, but according to the paper they can be included.\n",
    "                    point_set.add(ni)\n",
    "                    spatial_edge_set.add(pi)\n",
    "                elif node.isalpha():\n",
    "                   add_str_to_set(ni, pi)\n",
    "                   image_set.add(ni)\n",
    "                elif ni.startswith('_9j_'):\n",
    "                    image_set.add(ni) #might want to load this to hard drive if memory becomes an issue.\n",
    "                    image_edge_set.add(pi)\n",
    "                elif ni.startswith('POLYGON') or ni.startswith('Polygon'):\n",
    "                    poly_set.add(ni)\n",
    "                    spatial_edge_set.add(pi)                \n",
    "                    temporal_edge_set.add(pi)\n",
    "                elif ni.lower().startswith('multipolygon'):\n",
    "                    poly_set.add(ni)\n",
    "                elif ni.isascii():\n",
    "                    add_str_to_set(ni,pi)\n",
    "                elif ni.isprintable():\n",
    "                    add_str_to_set(ascii(ni),pi) #don't know if it's necessary, but it probably can't hurt\n",
    "                else:\n",
    "                    add_str_to_set(ascii(ni),pi)\n",
    "    \n",
    "                    \n",
    "    \n",
    "        edge_set.add(pi)\n",
    "\n",
    "    \n",
    "    transform_temp = trv.Compose([\n",
    "        trv.Resize((224, 224)),\n",
    "        trv.ToTensor(), trv.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], \n",
    "            std=[0.3, 0.3, 0.3])]) \n",
    "    \n",
    "\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    node_map = {i:node for i,node in enumerate(node_set)}\n",
    "    inv_node_map = {node:i for i,node in enumerate(node_set)}\n",
    "    \n",
    "    edge_map = {i:edge for i,edge in enumerate(edge_set)}\n",
    "    inv_edge_map = {edge:i for i,edge in enumerate(edge_set)}\n",
    "\n",
    "    # node_map = {i: node for i, node in enumerate(node_set)}\n",
    "    # inv_node_map = {node: i for i, node in enumerate(node_set)}\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    global_mean_x = 0\n",
    "    global_mean_y = 0\n",
    "    \n",
    "    max_x = 0\n",
    "    max_y = 0\n",
    "    \n",
    "    \n",
    "    points_tupled = []\n",
    "    point_ids = []\n",
    "    for i, point in enumerate(point_set):\n",
    "        point_tupled,max_x,max_y = process_point(point,max_x,max_y)\n",
    "        points_tupled.append(point_tupled)\n",
    "        point_ids.append(inv_node_map[point])\n",
    "    \n",
    "    \n",
    "    polys_tupled = []\n",
    "    poly_ids = []\n",
    "    x_max, y_max = 0,0\n",
    "    done = False\n",
    "    multipolys_tupled = []\n",
    "    for i, poly in enumerate(poly_set):\n",
    "        if 'multi' in poly.lower():\n",
    "            multipolys_tupled.append(process_multipoly(poly, x_max, y_max))\n",
    "            \n",
    "        poly_tupled, x_mean, y_mean, x_max, y_max = get_num_data(poly, \n",
    "                                                                 x_max, y_max)\n",
    "        global_mean_x += 1\n",
    "        global_mean_y += 1\n",
    "        polys_tupled.append(poly_tupled)\n",
    "        poly_ids.append(inv_node_map[poly])\n",
    "    i += 1\n",
    "    # print(poly_tupled[0])\n",
    "    global_mean_x, global_mean_y = global_mean_x/i, global_mean_y/i\n",
    "    \n",
    "    \n",
    "    character_map = {char:i for i,char in enumerate(printable)}\n",
    "    character_map['\\x7f'] = 101\n",
    "    \n",
    "    # node_set.update(string_set_s)\n",
    "    # node_set.update(string_set_m)\n",
    "    # node_set.update(string_set_l)\n",
    "    \n",
    "    text_dataset_s = TextDataset(list(string_set_s), character_map)\n",
    "    text_dataset_m = TextDataset(list(string_set_m), character_map)\n",
    "    text_dataset_l = TextDataset(list(string_set_l), character_map)\n",
    "\n",
    "    img_list = list(image_set)\n",
    "    image_dataset = ImageDataset(img_list, transform_temp)\n",
    "    # print(len(img_list))\n",
    "    # print(img_list[0])\n",
    "    numerical_dataset = NumericalDataset(list(num_set), log_scale=True)\n",
    "    \n",
    "    spatial_dataset = SpatialDataset(polys_tupled + points_tupled, global_mean_x, global_mean_y, x_max, y_max, ids=point_ids + poly_ids)\n",
    "    \n",
    "    datetime_dataset = DateTimeDataset(list(datetime_set)) #if possible combine\n",
    "    year_dataset = YearDataset(list(year_set))\n",
    "    return text_dataset_s, text_dataset_m, text_dataset_l, image_dataset , numerical_dataset, spatial_dataset, datetime_dataset, year_dataset, node_map, edge_map, inv_node_map, inv_edge_map\n",
    "        \n",
    "text_dataset_s, text_dataset_m, text_dataset_l, image_dataset , numerical_dataset, spatial_dataset, datetime_dataset, year_dataset,node_map, edge_map, inv_node_map, inv_edge_map = preprocess(graph)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b1647-6de4-4c73-a92c-4011723e5fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ca0a1-4a1f-4667-8cef-a13f07cfe925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff725b0-c7f0-43f1-835c-3be64d6d58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset_s\n",
    "# subj_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbbe6c-d039-4170-a6a1-7dedf5c5ec92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c59648f-28b8-4c11-ad6b-9d8b360b53d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f2dbe-df3b-421e-a165-16b8e26b118c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddc8a4-7f94-4e7d-91be-63fb572b0052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bcd61d-de84-4129-8a11-2f183f83955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c91c6-0465-4d80-a988-2ca665ac25d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"PyTorch CUDA available:\", torch.cuda.is_available())  \n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())  \n",
    "print(\"CUDA version:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())  \n",
    "print(\"Current CUDA device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ed9b3-45b0-4529-9e21-8836d09db813",
   "metadata": {},
   "source": [
    "# Grab train, valid, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a655d2b-46ab-4a31-b3eb-51fe0091f55b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "de1da1a0-8557-4339-8268-3dce458cb5c5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('graph_test.pkl', 'wb' ) as f:\n",
    "    pickle.dump(graph_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "df78dfac-61da-4cd3-82d7-39253078eeb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('graph_train.pkl', 'wb' ) as f:\n",
    "    pickle.dump(graph_train,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "2a09d10c-29a6-46c9-905b-5363731c689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('graph_val.pkl', 'wb' ) as f:\n",
    "    pickle.dump(graph_val,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab0a94-1c17-4fb4-9e9f-ef3a746b1e5f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09675e30-8e0a-484b-8949-c2ca0f0521f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# import base64\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "\n",
    "# byte_str = \"\"\"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\n",
    "# https://data.labs.pdok.nl/rce/id/image/20324688 http://dbpedia.org/ontology/thumbnail _9j_4AAQSkZJRgABAQAAAQABAAD_2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL_2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL_wAARCADIAIIDASIAAhEBAxEB_8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL_8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4-Tl5ufo6erx8vP09fb3-Pn6_8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL_8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3-Pn6_9oADAMBAAIRAxEAPwD3FEwAD1pWjXHQVI4A5HWoyw6UAIqD0GRUyog5CgH1xUasM0_eMUAI6K4wyg_Wo1tYgPu5J9TT9-TTwc9KAK7QoGIAwDxTwFUBQOKkwST6U3ABoAjliVkwOD2qt5DknI4q8Oa5_wAT-IY9FtCiENdyDCJ6D1NAGL4u8QjTYGs7Vh9rcYJBzsB7_Wue8Eau1pfvYTufLuDuQt2f_wCuP1ArDmeS5neWZmeRzliTyc96fcSmWWGVVCvGoGV4JIyc_wAvyoA9cxTWqloOprq2lxzEgzKAsgHqO_49a0CtAEBFAXkE-tSFRnmpIkBOT0FADfLPoaKtbB6UUAX24UkmoAfmqVs4NNGDyRzQAxiAeKaXOetDkbqYTQAomHmlOdwUHOOMHI_pU6ydKy0nRtUnhAG-OKNmPf5i3f8A4DVwE0AWi9NDA1EDWNq3iKDSr6O3lzl03AgZA5xyaALmta1Do1kZXIaVuI0zyx_wryu-up768a5uD5kjnJOeB7D0q3qt_Lq2pPNI4ZQSEA6AVTRCJCGOADjpxQARPaxSFrmFnQA5VDg59RyOlS3UWlyRb7OabzMBvKlXsevPTjND20bxusrkIc4IGSPwqJ7aJFBjuUdVXBUghhn2P0oA0PDOo_2XqYVyRBOQrDPQ9j7V6QVyMgjB6Yrx58odygk9RXovhXVv7T0sRuf9IgG1gT1GODQBrkYpyvjtSsOTj1phGDQBLv8AeiouaKANQuCKj3EA4qEOxPFLlsZwcUAOYkmmcnik3c0uc96AOe06Yv411hQcqYYlA_3Qc_qxroxXH6BL5njDUW4xIJCPoGUD9K6q7uorG2aeZgqqMnPf2oAqa9qjaRpE11EqPOoHlRscBjkZHUdj_L1rzZr-bWZZL67eOOVl3LEWwxGOcDvUPiTXbrWdQUIMxB-ASCFUEEk-5yQPrmn2EELxQy3N15aALHwMlVCjnH-etADbdITDMZCQ-P3Z56_hUYDYwGIJA61PcW0UUzC2uVnjGMOBjI-lNiVsgEkL1JI7UASqG8vDEHtwKR4cZLKcEdaeXJbZEQoPcjJ_-tUU0k4IaZ_MQALuAwVwMD-VAEThcH0HtVrRNSbStWjmBJjYhXUdwf69D-FVZyQBggg9D61FII1t0K5EwYk8nHsaAPXg6SruRgwPORSEc1zfgiWafTrgzPI4DgrvJIAI7A9PwrpiDQA3FFPxRQAJOiziIn5ypYDHYED-oqwTkVyXizVrrRjBPabN5YKwdcgggk_qBWRB491EACW0t3HfGQf5mgD0Bl5z60ydjHbSuOCqE5-gJrkofHyEATaey-6Sg_oR_Wp5_G-nyWUyrDOJWQhVcDaSRjkjNAHN-FL9YvEcEjzjbIsvmEg9ME55HqB-VR-LPEc-q3H2W0BWEZwewHcn-grCjgFsW8rYbplAJycRqARk_mc9M_TObCWCz6cEtXkllZjuCqckg4P54NAEAt4YtOR4ZtzswJIPJ-Yd_ersSg2kZIOc9cegAqeLSEiso4bmZY9oAKINzcHp1wPz_CnSLFH5aQoyRjgZbJPueAM_QCgCvEuCxI4B6A9atZJUKOQeT_hUcIGCQAQDzxVhMCTAAwBnp7_zoAswQQOphLDfIvBzgA9h9c1XvLKeBSJ4yMA4AI5x1P0qSQw7WJA8zggY55681HJK11eqbp2CGPCkHABA6d-o_lQBQaIrHsyMHleenqKi2B5ER3CAsAXPQcjk1cn25jZCSucYPWqt2MxkgDBBHFAHpHhW3SDRUVZUlO4guhyCB0rZIwa5zwEAdClAGMTnjrjgV05Q0AR4op-2igDhPHLrJKsYIyrIcZ68NzXLrEDgj0rfvydTu577a6RqmcZAIUE8EEHnn8KxpLhIzkmEKThTIuCT2GaAGpbtKxWMAkEAD1JIGP1qaTS7yAgtAV5wC5AA9STSRkySoAEUlgFMRyWOQQAPTuT2HuQDYnS5KmBy4JwSHyDx7HmgCCOGytoJYipuJJANzZwM4OeepHOKkS4cRiKMLFHyAiDAI9-5_GqrQujEOxHepUQlgQSQKAJEQliSc47USg7o-gAJwKsJEAec5NR3AUGMA5O7GOlAC2ZwrEAHJPapG2-btUAEg4z9aZageUSOMseD9acQxugQMkqTx9aAElRngBXBkGcgHAxUAiLMglB2EjkAcHtmrCA5YFSMdhTST5ZOTgd8UAQXCAuEGQAfTt7VXuogISQQQKtS8mPJyeuc1FdAeQ5yOBQBueFdeXSAba5X_R5WDBx1UkAcjuOK9BjkjniWSJwyMMgg5BryLZmBDgnAGDWpouvXOkTbTuktiRujJ6e49DQB6ViistPFOklFPnNyPSigDymWdbmeTyw6BmBwGOADgkficn8aW6gD28YLkBW3YzyfYZ_nVK0LYG_gkAA9zg8Z-oIrWsoIk3TXBLuHICZyR6D2GcUAO0OwL3k0-zywWByuRjAB4PX0-pPNdNeWkA05btLtmugACPMyRzzg9ax4ZQ8QUgALkADjk5Ofrzj8BVOaV8RqowC2Rxzg0AaAub4glb6YqTxuCtx-INU77X20-VIp5Ecv3Nuhx6E8D9KtqcADPt7Vxnid2bX4QArAKvXHHNAHaLczlQfJs2BAPEJGR-DUzZJe3cNskEMchJwVY4PtyeKW3lP2ePGMbQP0qG4mZZ4njYqwYEEDkHNAE72ktjK0EwG8HJAORzz1qFiROpGeFIqxHcyzM7zyM7gkAtyeOlTJA9xMXjhdwFwSoyAaAKSSMC5GQDURci3Knvk1p_Y5gz7reQD12Gq5tmEDFoXBGeqmgClKcmIAdgDx7VFdoDA_GMDNW7hEiSNnBUDGSeAOKpX2oWsUDGOVHccgD5gSOe30oAtQQSvaoVQsAB0pkltcA5ETj8OlQ3uvaZcukqwvA4UBgo4JHcdKz5NegXlFuDj_AGiP60Aaeyb-63_fJorK_wCElH924_7-_wD1qKALUTxNexrtdcnchxgEYXBxj2PFW7VTJLMSSSZSSSeetZ1mf9NgYEABccdhhsfyq_Yjd5jYyTKRn8TQBYUqpKhg2CMbSCDwOQR1HHX3ps5AEBAxk8nHtT8AM6joHxj04qObpbjoQeKALSgEDBxnnr_9aqF_4XsdTvY72XU2hkVQCn2csOD6jrV5QOCMk1IBnqKADykhUIkgZFGA54BHrg1XlAaeJQRjI5z71l-KgRpqEMwO44wcdqzfCe5YZCzMx80HJJOOlAHSX88tjp9zcQpE8ykBBICVySOwIJ4z3qPw9qd5qcE5nWGJ1YbTBuXjnrknOeKh1tydLuF3hQXHJ6DkVW8Ms0UE4DhuRkg8d_6UAdSlyImKSXVwXUZYCQjA9etC6qvmrEtxc7nXcoM3UHoep9DXnfi-9uIL-MwyuhZQGw2Mj0NVVv4gHLGTdAoBOTnBOKAPQtW1kPpc5MskkZPlkFwec46Y_rXGtt-YADB6c4rmo7yU6oI1kbyXkyUPQ5710BcYOKAEYgKQDk-hHFViAepPPapGPOM_Wm5HX0oAbsHoKKdn_OKKANRbu7tLnIVJArZZSM8EnH8zW5pU9lPCC0jW7mTJyMqSSeB3FVobOe7BmW2IflSOhOMjn_Peon0S_WRJoIWR85IB4JoA35dNuI2kcJ5gMhAMZDAcDGcHIz71XktLl2hAtpjgnkRmq1omt2kUiokis7hmIYjP5Vau7zVnAjEtwgJAJLkdj05oAuJYXeM_Zpsf7hrkPEniWfRdQNsQE46OMEdPX61ufbdTRAGnmyc4XzjzjqaydT0q31W4W4vLOO4uCoyzuenvyKAOZu_Fg1KPyblwFJ42gZB9qbZa-mmLIiOMltwDDnHarOu6FY2ot5La0jgKgk7CTnPTqTTtF0aC9jE01rDN8wUmTqBj0_GgAbxKdVtJ7ZwpBAbI4III_wAa2_DJBtZSvcjnpk85_lVu10iHTj5lpY6coZQH3oCDg55BFXC8s2UtxaRqMcLGFye54HI5oA4vxmMalbjGQQKrGJGa7KgjcF_ABq6jWvCjamy3L30SOmAABkE1S1Cym0u0aUSQozZAZSGJODwAfYGgDjI3U6spA6ODz6YrolkycHtXIWyTTXjTyMW2tkkd-a6K3kZzkUAX-uDjikJwOhxTcsCOh9qUnjBzQA7cPX9KKblaKAO08L6sl5JNG42nIbGeeQB_MH866XCEYU5P1rza1uzaTRyQoqELgHkjtV2XxLqRbAuVUdchQf6UAd0yEkYOMg9Cev8AjWHq8WpowktWDquSQV56Vzra7qL4BvXyQORjAz7iom1e-wVa7kbkcgmgCGXUtSTAZ1V4ycAjoD2zVX7bqLoDvYk-g7Z6VbkuZXJYyMxOSSTn_PSkMvUKWPXJzigCrdtdT6ezXBYkHCk-lS6XpuoXdorWYcruOSGwAfrT7gySadMXlLKpJUHGFyBUNtaPfeXAuvPp6FQBCqg7jzk59f8ACgBmopq-nEi6L7CQAQ-QfyNZj6lcnK-c3UYOSMV1UVloWkwFLzXmnGcv5jA7j7gg1m3eqeBwSkazTSN0MOcg-2TigDEN3cOcNMxGPU0rTzOoDElSOh5qzNpccy-bYOzKRlUZgXH1Axn8KoeTdgMpjfC8HKkAfmKAAJEDnaAc9hjFIysDlWIx0xxTRIBkEnOfwp5KnBDcfWgB_wBquo8fMGwO4zT01GUYMkSse2DjFMY4YHaME_pTvlIJ6Ee1AE_9pr_zwP50VW2j1ooA2WJyGBwo4Hv-FRliELA4JPA9fwpuGUgoCCQCCACSP_retMZyCEAZieSO_vQBIJcggnAJOAO5wO341ds9Lvr6LfDGCnQknBrPwowQoBxznrxzz-dRRavfaVcma3nJUkgo4OGH07UAdInhvUT_AHE4xndSy-Hr6BQ4US8HIQ5Irlbnx5rIkKhmJIBOFOASAcDGOnSnaJ4l1nVNat4Z3mELP83UZA59aAN6ezuINLKso3MckdCB_jVP7LE8CSsAcHHB5B7c9utdD4gu4_KSFWUZALEn8hXMpcxI0kRlXaQTnPGf_rGgCCXTLCSRpHtQzdckZP61raXB4eEgikiWOboAQADj3xVFXdjgAs-B90Zz78VIdKmuUH-iSkg5BKnGPb079-9AG9LqnhzTfvPbKV7AAkfnWXefErSIQyQo056EAYFYU2gWxnP2m0IcHkSHt-PrUlrpOmiZVMSQ57sMj60ASSalaayvmNaRWhY5VkXOc-oH-FVJdGuFRnhAmB6GPnH4da6-28OWIVZC5kGMApgA_lWpbWVpbEGGBVI4DYyfzoA8ya3uo4g_lSFBnJZDgUxLklTlScDHB6V6vLLFEhMzRIncuQB-tctq8vhicmMgNMxwPs4wSf5GgDlvPH-TRVz-yIn-ZZE2nkZnjzj86KALDSsFZcgFfUdcd_T0qe202-njRxbOVkyc44x685qh5-7AALMDk4xz_hXY-Gb8XenrE4AkhJU_TjFAGOnhy_clmVQc5ALgAfgPqelTL4UncgvLApHbk5966onBIIwDQTkYNAHB6po39nygyhWBBKsBgEcf1q14aEUmvRAKuWJJ6-nT-tdRfW6Xdq0UyBlJGPUe4rndLK6HqxSdFHmMTFNj3ORQBF4js2GvzrvHl7VGO54rKgtogC7IGYHnJznFbmuzGfWZZdhwVXHIAyAB1JrKkkjiLM8kaqThhuyceowKAN3TvFdhcIkTMInHBBGPzH-FbSyiRQQxIPQg5FeTX9qnmu0Eu8KchhwRnpxWjoniC9sJkgnkEkfA3g8D2PvzQB6VJDDcx7JolkX0YZx9KxNR8LQ3KH7JII2AOEc8E5z1HNc_c_EEJuWJU4JAwCScfU1lzePb2WMGN2XJIwAByAP8aANQprWhOGUsY-CQSSpHsaq3_ifxFcsVt0WFT0xwT-XP5EVTi1me7UtLLI2AMgsSBVwSjaoUsRnGe2KAMKew1i7k3Xt84J9M5_U5qay0IW0nmmV3YnIJ9vp_nmt3JkUMSAAcMOo59KFAI3qxCk4GBx7nmgCn9k_2n_76P-FFWPtco43t-YooAikOxiyFQAcABeQB0z2yef8AJrQ8NX7Wmrlp2YJKh3E5I45zz7f4VUWeMRFCgZyQSNxHBHTHtWXKzITIoIYkkAZ9wKAPQZvFenLIQFkcAdVXGTn3xWbceO7OEnbCM9i8oU_kM157qBuJ5iFRgMKMcjOBjOPfGapf2VNKSz5GfbpQB3U_xBdyPKEIAP8ACpY_mSBWfPr9zqseDKSAd2NoXGT7ZrnItEmwmX4JwR71p2tmEDDcVx3J9-aALJLOS5ZndjnJPT-XXHpTzbkwlmAAOASSOOBx04_CrdvEpj-4pIIGM8nn_wCvUgjEjuu5SC2cdR0HrQBUSBZFjBViDzkDOfoe9R3Fovm8FVLKeoIBIHsME5B4AFXreBwdyHDR5XAOMjp_I0k6ANuUgkE5BPQd_wCdAGLJosAIf7xORtBIIx1GMU6DTVUiN4ioUkAquckjPOfpW-0Kzq4TLELkBAc5_Ciz0i7mYSLAFUvglztAIAzkn3zQBmQ2QQgsAoIyo6dO5q6AsSghxnHBwCCRnoe3_wBarcumWcU6G61K3jBTdiP5yORwPfmq9zdaRFGEt47q4lJ4LnCkZ6juD_KgBsaMY2TcGLAEk4BB6ke31GKuWelXckKPDauyAljlQFx1HzGov-EjngtyltaWtvhcfKm5umM5NZ1zqF5cqFnvJHwudhYhT2A44oA3ovD0vlJzb_dH_LYf40VymU_54v8An_8AXooAvqJTGQIYuCf4skA9xj0xj8KYEjSUFQrcYJOQPzP-BoooAJgouDgKobkjGVBz9OB3-tJsQxbAAzdz2B9MCiigBOpAIDKepORg446_54pYEXzHiYAAHJPcA9MD09_cUUUAaEdlJLEDFHJKSdo8pSeOxz0B-pqz_Yl6DMXEUaoASZnC8kHgY9gKKKAK9u-lW1w_2rUVKjokCknpjg8jt60y51fSYgVtNMuJyWDb5mxu9RgZoooAiHi3UVUxwrb2iAdI1BYAdOTk5rJuNTnnLuZ5JgWLFSxGSc5P1NFFAEUUpkAYxmMkZAB4x-FWnlbAYMAMYwTySaKKAIYovODBmcYySTgn271NJaKxwsoIGCAXzg_QgUUUAR7X_wCfmP8AMUUUUAf_2Q==\n",
    "# https://data.labs.pdok.nl/rce/id/image/20636163 http://dbpedia.org/ontology/thumbnail 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\n",
    "# \"\"\"\n",
    "\n",
    "# def decode_base64_image(encoded_str,log_note='pass values to decode_base_64_jpg'): # - to +, _ to /\n",
    "#     \"\"\"\n",
    "#     encoded_str: url safe base 64 jpg string --> image bytes string\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         image_bytes = base64.urlsafe_b64decode(encoded_str)\n",
    "#         image_obj = Image.open(BytesIO(image_bytes))\n",
    "#         image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "#         return image, image_obj\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"{e} error encoding image at {log_note}\")\n",
    "#         print(e)\n",
    "#         return None, None\n",
    "\n",
    "# def save_bytes_to_jpg(image_bytes, item_num=0,folder='Downloads/ml4g/image_data/',name='Fred'):\n",
    "#     filename = f'{folder}{name}_{item_num}.jpg'\n",
    "#     with open(filename, 'wb') as img_file:\n",
    "#         img_file.write(image_bytes)\n",
    "\n",
    "\n",
    "# image, image_bytes = encode_image(byte_str, transform_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acb8ed-bac7-4450-84a5-dedbc8aef71c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09cd4067-852c-4807-bc0b-417be80a827f",
   "metadata": {},
   "source": [
    "# Convert raw values to consistent feature vectors for the encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "34660fba-188d-4dfd-902e-b9c2173915e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_spatial(batch):\n",
    "    nodes_ids, batch = zip(*batch)\n",
    "    padded = nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "    padded = padded.permute(0, 2, 1)\n",
    "    return nodes_ids, padded\n",
    "\n",
    "def collate_text(batch, min_size=20):\n",
    "    if batch[0].size(0) < min_size:\n",
    "        pad_num = min_size - batch[0].size(0)\n",
    "        pads = torch.zeros(pad_num, dtype=torch.long, device=device)\n",
    "        batch[0] = torch.cat([batch[0], pads], dim=0)\n",
    "    return nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "spatial_dataloader = DataLoader(spatial_dataset, batch_size=batch_size, collate_fn=collate_spatial)\n",
    "text_dataloader_s = DataLoader(text_dataset_s, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_m = DataLoader(text_dataset_m, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_l = DataLoader(text_dataset_l, batch_size=1, collate_fn=collate_text) #it crashes every time it starts :'(\n",
    "image_dataloader = DataLoader(image_dataset, batch_size=1)\n",
    "datetime_dataloader = DataLoader(datetime_dataset, batch_size=batch_size)\n",
    "year_dataloader = DataLoader(year_dataset, batch_size=batch_size)\n",
    "numerical_dataloader = DataLoader(numerical_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "8fa37dd0-337b-45ea-8ad0-50b9be965bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del polys_tupled, points_tupled,node_set, http_set, edge_set , string_set, string_set, string_set,image_set ,num_set ,poly_set ,datetime_set ,date_set ,year_set ,point_set ,text_edge_set ,text_edge_set ,text_edge_set ,image_edge_set ,num_edge_set ,spatial_edge_set ,temporal_edge_set ,encoder_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e1e85-5dd0-491e-b949-60ca053ce7f1",
   "metadata": {},
   "source": [
    "# Multimodal Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "7198e43b-a426-4ee6-8d74-6ea1b2652130",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import torch.functional as f\n",
    "import torch.nn as nn\n",
    "\n",
    "# ]1] temporal conv\n",
    "# Layer Filters Kernel Padding Pool\n",
    "# 1 64 7 3 max(2/2)\n",
    "# 2 64 7 3 max(2/2)\n",
    "# 3 64 7 3 -\n",
    "# 4 64 7 2 max(·)\n",
    "# Layer Dimensions\n",
    "# 5 512\n",
    "# 6 128\n",
    "# 7 128\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=102, dropout=0.4, size_type='medium'): #filters = 64\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.size_type = size_type.lower()\n",
    "        self.mlp_hidden_dim = 1024 if self.size_type == 'large' else 512 if self.size_type == 'medium' else 256\n",
    "        self.tcnn_hidden_dim = 128 if self.size_type == 'large' else 64 if self.size_type == 'medium' else 32\n",
    "        self.dilation_vals = (1,1,1) if self.size_type == 'small' else (2,4,8)\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) #used embedding dict instead of ohe to save space\n",
    "                                                            #sparse should probably work too\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            embed_dim, self.tcnn_hidden_dim , kernel_size=7, padding=3, dilation=1)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.pool1 = nn.MaxPool1d(2,2)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.pool2 = nn.MaxPool1d(2,2)\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm3 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        self.conv4 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=2)#, dilation=self.dilation_vals[2])\n",
    "        \n",
    "        self.norm4 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop4 = nn.Dropout(dropout)\n",
    "        self.pool4 = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.lin1 = nn.Linear(self.tcnn_hidden_dim, self.mlp_hidden_dim)\n",
    "        self.lin2 = nn.Linear(self.mlp_hidden_dim, embed_dim)\n",
    "        self.lin3 = nn.Linear(embed_dim , embed_dim)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        residual = embedded\n",
    "        #conv\n",
    "        \n",
    "        x = self.conv1(embedded)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop4(x)\n",
    "        # if residual.size(1) != x.size(1):\n",
    "        #   residual = nn.utils.rnn.pad_sequence([residual, x], batch_first=True)\n",
    "        x = self.pool4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #ffnn\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        # x += residual\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "# [2] MobileNet \n",
    "# Type / Stride Filter Shape Input Size\n",
    "# Conv / s2 3 × 3 × 3 × 32 224 × 224 × 3\n",
    "# Conv dw / s1 3 × 3 × 32 dw 112 × 112 × 32\n",
    "# Conv / s1 1 × 1 × 32 × 64 112 × 112 × 32\n",
    "# Conv dw / s2 3 × 3 × 64 dw 112 × 112 × 64\n",
    "# Conv / s1 1 × 1 × 64 × 128 56 × 56 × 64\n",
    "# Conv dw / s1 3 × 3 × 128 dw 56 × 56 × 128\n",
    "# Conv / s1 1 × 1 × 128 × 128 56 × 56 × 128\n",
    "# Conv dw / s2 3 × 3 × 128 dw 56 × 56 × 128\n",
    "# Conv / s1 1 × 1 × 128 × 256 28 × 28 × 128\n",
    "# Conv dw / s1 3 × 3 × 256 dw 28 × 28 × 256\n",
    "# Conv / s1 1 × 1 × 256 × 256 28 × 28 × 256\n",
    "# Conv dw / s2 3 × 3 × 256 dw 28 × 28 × 256\n",
    "# Conv / s1 1 × 1 × 256 × 512 14 × 14 × 256\n",
    "# 5×\n",
    "# Conv dw / s1 3 × 3 × 512 dw 14 × 14 × 512\n",
    "# Conv / s1 1 × 1 × 512 × 512 14 × 14 × 512\n",
    "# Conv dw / s2 3 × 3 × 512 dw 14 × 14 × 512\n",
    "# Conv / s1 1 × 1 × 512 × 1024 7 × 7 × 512\n",
    "# Conv dw / s2 3 × 3 × 1024 dw 7 × 7 × 1024\n",
    "# Conv / s1 1 × 1 × 1024 × 1024 7 × 7 × 1024\n",
    "# Avg Pool / s1 Pool 7 × 7 7 × 7 × 1024\n",
    "# FC / s1 1024 × 1000 1 × 1 × 1024\n",
    "# Softmax / s1 Classifier 1 × 1 × 1000\n",
    "# Table 2. Resource Per Layer Type\n",
    "# Type Mult-Adds Parameters\n",
    "# Conv 1 × 1 94.86% 74.59%\n",
    "# Conv DW 3 × 3 3.06% 1.06%\n",
    "# Conv 3 × 3 1.19% 0.02%\n",
    "# Fully Connected 0.18% 24.33%\n",
    "\n",
    "#idea: pass identity vector with batches to encoders (don't process, just pass back), use that to map embeddings to nodes later\n",
    "class ImageEncoder(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 256, stride=2, alpha=0.5)\n",
    "        self.image6 = MobileBlock(256, 512, stride=1, alpha=0.5)\n",
    "        self.image7 = MobileBlock(512, 512, stride=2, alpha=0.5)\n",
    "        self.image8 = MobileBlock(512, 1024, stride=1, alpha=0.5)\n",
    "        self.middle_conv = nn.Sequential(*[MobileBlock(\n",
    "                1024, 1024, stride=1, alpha=0.5) for i in range(5)])\n",
    "        \n",
    "        self.image9 = MobileBlock(1024, 1024, stride=1, alpha=0.5)\n",
    "        \n",
    "        self.image10 = MobileBlock(1024, 2048, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(1024,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        # middle_conv = [MobileBlock() for i in range(5)]\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.image(batch)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.image6(x)\n",
    "        x = self.image7(x)\n",
    "        x = self.image8(x)\n",
    "        x = self.middle_conv(x)\n",
    "        x = self.image9(x)\n",
    "        x = self.image10(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class MiniImgEnc(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(MiniImgEnc, self).__init__()\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 512, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(256,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image(x)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        # x = self.lin2(x)\n",
    "        # x = self.norm(x)\n",
    "        return self.act(x)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class MobileBlock(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_size, stride=1, normal_conv=False, alpha=0.5, dilation=1):\n",
    "        super(MobileBlock, self).__init__()\n",
    "        old_in_channels = in_channels\n",
    "        in_channels = int(alpha * in_channels)\n",
    "        embedding_size = int(alpha * embedding_size)\n",
    "        # depthwise conv\n",
    "        self.normal_conv = normal_conv\n",
    "        if self.normal_conv:\n",
    "            # in_channels = in_channels//2\n",
    "            pass\n",
    "        # 3x3x3x32\n",
    "        self.conv = nn.Conv2d(old_in_channels, in_channels, 3, padding=1, stride=2, dilation=1)\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, 3, stride=stride,\n",
    "                                   padding=1, groups=in_channels)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        # pointwise conv\n",
    "        self.pointwise = nn.Conv2d(in_channels, embedding_size, 1, stride=1, padding=0)\n",
    "        self.norm2 = nn.BatchNorm2d(embedding_size)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(self.conv.kernel_size)\n",
    "        if self.normal_conv:\n",
    "            x = self.conv(x)\n",
    "        # print(x.size())\n",
    "        x = self.depthwise(x)\n",
    "        # print(x.size())\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.pointwise(x)\n",
    "        # print(x.size())\n",
    "        x = self.norm2(x)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [3] temporal conv\n",
    "# layer filters kernel padding pool\n",
    "# 1 16 5 2 max(3/3)\n",
    "# 2 32 5 2 -\n",
    "# 3 64 5 2 avg(·)\n",
    "# layer dimensions\n",
    "# 4 512\n",
    "# 5 128\n",
    "# 6 128\n",
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128, dropout=0.2):\n",
    "        super(SpatialEncoder, self).__init__()\n",
    "        \n",
    "        # temp cnn\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=3, ceil_mode=True)\n",
    "        self.norm1 = nn.BatchNorm1d(16)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm1d(32)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        \n",
    "        # dense\n",
    "        self.lin1 = nn.Linear(64, 512)\n",
    "        self.lin2 = nn.Linear(512, 128)\n",
    "        self.lin3 = nn.Linear(128, embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, spatial):\n",
    "        # x = self.pad(spatial, batch_first=True)\n",
    "        x = self.conv1(spatial)\n",
    "        x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.drop1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #flatten\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# [4] mlp h == in dim 1st col, out 2nd col\n",
    "# XSD:gYear 6 2\n",
    "# XSD:date 10 4\n",
    "# XSD:dateTime 14 6\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super(TemporalEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = input_dim\n",
    "        self.out_dim = 2 if input_dim <= 6 else 4 if input_dim <= 9 else 6 #I messed something up but its very specific...\n",
    "                                                                        #norm_cent returns a digit so idk if that should be 2?\n",
    "                                                                        #there's no sin/cos, so it seems weird to encode it digitwise\n",
    "        num_layers = 2\n",
    "        self.mlp = nn.Sequential(*[nn.Linear(input_dim, input_dim), \n",
    "                                   nn.ReLU(),nn.BatchNorm1d(input_dim)]*num_layers)\n",
    "        self.out = nn.Linear(input_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "\n",
    "#[5] one to one encoding for numerical\n",
    "class NumericalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NumericalEncoder, self).__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a7ed0-4c4b-40bb-9d0e-f92bd25a7d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82a8c0f3-7864-4598-a4e8-ecc714a2e6ae",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6d64ee80-fe29-4dc5-95d7-26a40bc0d75b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "46ff724b-ac91-443b-8c30-fa9f606d0359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80122/2817622372.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    }
   ],
   "source": [
    "text_encoder_s, text_encoder_m, text_encoder_l = TextEncoder(128, size_type='s'), TextEncoder(128, size_type='m'), TextEncoder(128, size_type='m')\n",
    "datetime_encoder = TemporalEncoder(9)\n",
    "spatial_encoder = SpatialEncoder(5)\n",
    "year_encoder = TemporalEncoder(5)\n",
    "image_encoder = ImageEncoder(128)\n",
    "numerical_encoder = NumericalEncoder()\n",
    "\n",
    "class MultiModalEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_matrix,\n",
    "                 encoders_config, dropout=False):\n",
    "        super(MultiModalEncoder, self).__init__()\n",
    "\n",
    "        # self.dataloaders = [config['dataloader'] for config in encoders_config]\n",
    "        self.encoders_config = encoders_config\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.column_map = {'text':(0,128),\n",
    "                           'image':(129,257),\n",
    "                           'numerical':(258,259),\n",
    "                           'spatial':(260, 388),\n",
    "                           'datetime':(389,393),\n",
    "                           'year':(394,396)}\n",
    "        self.x_indices = []#torch.zeros(8, device=device)\n",
    "        self.y_indices = []\n",
    "        self.values = []#torch.zeros((8,128), device=device)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.num_nodes = feature_matrix.size()[0]\n",
    "\n",
    "    def forward(self):\n",
    "        for config in self.encoders_config:\n",
    "            self.x_indices = []\n",
    "            self.y_indices = []\n",
    "            self.values = []\n",
    "            name = config['name']\n",
    "            encoder = config['encoder'].to(device)\n",
    "            dataloader = config['dataloader']\n",
    "            # print(f\"encoder: {name}\")\n",
    "            i = 0\n",
    "            # if 'image' not in name:\n",
    "                \n",
    "            for j, batch in enumerate(dataloader):\n",
    "                if self.dropout:\n",
    "                    x,y = self.column_map[name]\n",
    "                    output_dim = y-x\n",
    "                    embeddings = torch.zeros(y-x, dtype=torch.float32,device=device).unsqueeze(0)\n",
    "                    continue\n",
    "                    \n",
    "                if 'text' in name:\n",
    "                    cols = self.column_map['text']\n",
    "                    node_ids = batch[:, 0]\n",
    "                    features = batch[:, 1:]\n",
    "                else:\n",
    "                    cols = self.column_map[name]\n",
    "                    node_ids, features = batch\n",
    "                    node_ids = torch.tensor(node_ids,device=device)\n",
    "                # I confused myself. passing the node_ids through the forward pass was completely unnecessary.\n",
    "                # change to sparse and use automatic column assignment base on thingy\n",
    "                try:\n",
    "                    embeddings = encoder(features)\n",
    "                except ValueError: #if batch size is 0 or 1 when it's supposed to be more.\n",
    "                    x,y = self.column_map[name]\n",
    "                    output_dim = y-x\n",
    "                    embeddings = torch.zeros(y-x, dtype=torch.float32,device=device).unsqueeze(0)\n",
    "                # embeddings = embeddings.to(device)\n",
    "                # self.feature_matrix[node_ids, cols[0]:cols[1]] = embeddings\n",
    "                # sparse assignment\n",
    "\n",
    "                # if embeddings.size(0) <= 1:\n",
    "                #     embeddings = embeddings.unsqueeze(0)\n",
    "                #     print('unsqueezed', embeddings.size())\n",
    "                batch_size = node_ids.size(0)\n",
    "                y_cols = torch.arange(cols[0], cols[1], device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "                # print(y_cols.size())\n",
    "                embedding_size = embeddings.size(1)\n",
    "                x_rows = node_ids.unsqueeze(1).repeat(1, embedding_size)\n",
    "                self.x_indices.append(x_rows)\n",
    "                self.y_indices.append(y_cols)\n",
    "                self.values.append(embeddings.view(-1))\n",
    "                i+=1\n",
    "\n",
    "        else:\n",
    "            for img in dataloader:\n",
    "                pass\n",
    "        try:\n",
    "            \n",
    "            x = torch.cat(self.x_indices).reshape(-1)\n",
    "\n",
    "            y = torch.cat(self.y_indices).reshape(-1)\n",
    "            indices = torch.stack([x, y], dim=0)\n",
    "            values = torch.cat(self.values)\n",
    "            self.feature_matrix = torch.zeros((self.num_nodes, 396), dtype=torch.float32, device='cpu')\n",
    "            self.feature_matrix.index_put_(indices, values)\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # print('magic has occurred stay spiffy\\n', e)\n",
    "\n",
    "        return self.feature_matrix\n",
    "\n",
    "\n",
    "encoders_config = [{'name': 'text_s', 'encoder': text_encoder_s, 'dataloader': text_dataloader_s},\n",
    "                   {'name': 'text_m', 'encoder': text_encoder_m, 'dataloader': text_dataloader_m},\n",
    "                   {'name': 'text_l', 'encoder': text_encoder_l, 'dataloader': text_dataloader_l},\n",
    "                   {'name': 'image', 'encoder': image_encoder, 'dataloader': image_dataloader},\n",
    "                   {'name': 'numerical', 'encoder': numerical_encoder, 'dataloader': numerical_dataloader},\n",
    "                   {'name': 'spatial', 'encoder': spatial_encoder, 'dataloader': spatial_dataloader},\n",
    "                   {'name': 'datetime', 'encoder': datetime_encoder, 'dataloader': datetime_dataloader},\n",
    "                   {'name': 'year', 'encoder': year_encoder, 'dataloader': year_dataloader},]\n",
    "\n",
    "\n",
    "num_nodes = len(node_map)\n",
    "\n",
    "embed_size = 128 + 128 + 1 + 128 + 4 + 2\n",
    "\n",
    "feature_matrix = torch.zeros((num_nodes, embed_size), dtype=torch.float32, device='cpu')#.to_sparse()\n",
    "\n",
    "\n",
    "\n",
    "multi_modal_encoder = MultiModalEncoder(\n",
    "    feature_matrix=feature_matrix,\n",
    "    encoders_config=encoders_config).to(device)\n",
    "\n",
    "# multi_modal_encoder = MultiModalEncoder(encoders_config, num_nodes)\n",
    "\n",
    "feature_matrix = multi_modal_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "2091f46f-a3b9-4a05-92a8-54994bbc2f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233,\n",
       " tensor([[[1.4575, 1.4575, 1.4444,  ..., 1.3137, 1.3137, 1.3137],\n",
       "          [1.4314, 1.4314, 1.4183,  ..., 1.2876, 1.2876, 1.2876],\n",
       "          [1.4052, 1.3922, 1.3922,  ..., 1.2614, 1.2614, 1.2614],\n",
       "          ...,\n",
       "          [0.1242, 0.1895, 0.2157,  ..., 0.0458, 0.0719, 0.1111],\n",
       "          [0.0458, 0.1242, 0.1634,  ..., 0.0588, 0.0850, 0.1242],\n",
       "          [0.0065, 0.0850, 0.1242,  ..., 0.0719, 0.0980, 0.1373]],\n",
       " \n",
       "         [[1.4575, 1.4575, 1.4444,  ..., 1.3137, 1.3137, 1.3137],\n",
       "          [1.4314, 1.4314, 1.4183,  ..., 1.2876, 1.2876, 1.2876],\n",
       "          [1.4052, 1.3922, 1.3922,  ..., 1.2614, 1.2614, 1.2614],\n",
       "          ...,\n",
       "          [0.1242, 0.1895, 0.2157,  ..., 0.0458, 0.0719, 0.1111],\n",
       "          [0.0458, 0.1242, 0.1634,  ..., 0.0588, 0.0850, 0.1242],\n",
       "          [0.0065, 0.0850, 0.1242,  ..., 0.0719, 0.0980, 0.1373]],\n",
       " \n",
       "         [[1.4575, 1.4575, 1.4444,  ..., 1.3137, 1.3137, 1.3137],\n",
       "          [1.4314, 1.4314, 1.4183,  ..., 1.2876, 1.2876, 1.2876],\n",
       "          [1.4052, 1.3922, 1.3922,  ..., 1.2614, 1.2614, 1.2614],\n",
       "          ...,\n",
       "          [0.1242, 0.1895, 0.2157,  ..., 0.0458, 0.0719, 0.1111],\n",
       "          [0.0458, 0.1242, 0.1634,  ..., 0.0588, 0.0850, 0.1242],\n",
       "          [0.0065, 0.0850, 0.1242,  ..., 0.0719, 0.0980, 0.1373]]]))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02434eaa-0c73-4afc-8d66-a9267ec9d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e5a41-a268-4e15-9027-ec4dbbcda71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db021f8-a311-4c37-ac60-c60392877d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fd9b6-b914-44b8-bdc4-c565e0906194",
   "metadata": {},
   "source": [
    "# Connect to R-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "ef175e6b-b693-4709-ba84-f3bf9761ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4911, 4911])\n",
      "torch.Size([4911, 391])\n"
     ]
    }
   ],
   "source": [
    "length = len(node_map.keys())\n",
    "\n",
    "indices = torch.zeros((1),device=device)\n",
    "indices = torch.arange(0, length, device=device).repeat(2, 1)\n",
    "\n",
    "values = torch.ones(length, device=device)\n",
    "identity = torch.sparse_coo_tensor(indices, values, (length, length), device=device).to_dense()\n",
    "print(identity.size())\n",
    "print(feature_matrix.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4b6a63e8-df02-41c4-997c-7bb7c22b955b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity.is_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "19c9b30e-69aa-43ac-ad3b-e36dbc66ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4911, 5302]) Great success!device\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_cat = torch.cat((identity,feature_matrix),dim=1)\n",
    "print(feature_matrix_cat.size(), 'Great success!device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff547a-e563-4a62-a268-081364440c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a9217-e178-4ecc-ae9d-41270a55e860",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "87512148-5532-423a-b7b4-999c6ede7bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from typing import defaultdict\n",
    "adjacency = defaultdict(torch.sparse_coo_tensor)\n",
    "\n",
    "# inv_edge_map\n",
    "indices_x = defaultdict(list)\n",
    "indices_y = defaultdict(list)\n",
    "\n",
    "values = defaultdict(list)\n",
    "i=0\n",
    "for s, p, o in graph:\n",
    "    edge_id = inv_edge_map[p.identifier]\n",
    "    # print(edge_id)\n",
    "    s_id = inv_node_map[s.identifier]\n",
    "    o_id = inv_node_map[o.identifier]\n",
    "    \n",
    "    indices_x[edge_id].append(s_id)\n",
    "    indices_y[edge_id].append(o_id)\n",
    "    values[edge_id].append(1)\n",
    "    i += 1\n",
    "\n",
    "graph_size = max(node_map.keys()) +1\n",
    "\n",
    "for edge_id in indices_x.keys():\n",
    "    ind_x, ind_y = torch.tensor(indices_x[edge_id]), torch.tensor(indices_y[edge_id])\n",
    "    ind = torch.stack((ind_x, ind_y))\n",
    "    val = values[edge_id]\n",
    "    adjacency[edge_id] = torch.sparse_coo_tensor(indices=ind,values=val,size=(graph_size,graph_size),device=device)\n",
    "    \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "61b4d3af-c4fc-4645-8680-27c331b6bab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4911"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e02d68b9-3be5-4562-8ef8-e0ae2e34ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great success! 0\n",
      "Great success! 1\n",
      "Great success! 2\n",
      "Great success! 3\n",
      "Great success! 4\n",
      "Great success! 5\n",
      "Great success! 6\n",
      "Great success! 7\n",
      "Great success! 8\n",
      "Great success! 9\n",
      "Great success! 10\n",
      "Great success! 11\n",
      "Great success! 12\n",
      "Great success! 13\n",
      "Great success! 14\n",
      "Great success! 15\n",
      "Great success! 16\n",
      "Great success! 17\n",
      "Great success! 18\n",
      "Great success! 19\n",
      "Great success! 20\n",
      "Great success! 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    1,    2,  ..., 4908, 4909, 4910],\n",
       "                       [   0,    1,    2,  ..., 4908, 4909, 4910]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(4911, 4911), nnz=4912, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_adjacency(A, self=True): #gotta still add self-loops. See if it works without first.\n",
    "    #it still crashes :(\n",
    "    size = A.size()[0]\n",
    "    A = A.coalesce()\n",
    "    # A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "    eye_indices = torch.arange(0,size, device=device).repeat(2,1)\n",
    "    eye_values = torch.ones(size, device=device)\n",
    "    indices = torch.cat((A.indices(), eye_indices), dim=1)\n",
    "    values = torch.cat((A.values(), eye_values), dim=0)\n",
    "    A = torch.sparse_coo_tensor(indices, values, A.size(), device=device).coalesce()\n",
    "    # print('to_dense()?')\n",
    "    degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "    # print('no')\n",
    "    # print(degree.size())\n",
    "    d_inv_sqrt = degree.pow(-0.5)\n",
    "    D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    # A = A.to_dense()\n",
    "    normalized_values = values * d_inv_sqrt[indices[0]] * d_inv_sqrt[indices[1]]\n",
    "    \n",
    "    normalized_A = torch.sparse_coo_tensor(indices, normalized_values, A.size(), device=device).coalesce()\n",
    "    # normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "\n",
    "\n",
    "for i, key in enumerate(adjacency.keys()):\n",
    "    A = adjacency[key]\n",
    "    adjacency[key] = normalize_adjacency(A)\n",
    "    del A\n",
    "    print(f'Great success! {i}')\n",
    "    k = key\n",
    "adjacency[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "05eeb229-0bab-46de-980e-34b2f4a2ea3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1048, 0.0745, 0.6773, 0.0754, 0.0680],\n",
       "        [0.9622, 0.0066, 0.0138, 0.0109, 0.0065],\n",
       "        [0.1295, 0.4958, 0.0510, 0.1462, 0.1776],\n",
       "        ...,\n",
       "        [0.0219, 0.0631, 0.0932, 0.7270, 0.0948],\n",
       "        [0.1732, 0.1156, 0.0579, 0.5538, 0.0995],\n",
       "        [0.1416, 0.3795, 0.2852, 0.1018, 0.0919]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##save non-error version\n",
    "def block_diag (weights):\n",
    "    block_diag_matrix = torch.block_diag(*weights)#.to_sparse()\n",
    "    return block_diag_matrix\n",
    "    \n",
    "\n",
    "class R_GCNLayer(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, num_relations, block_split):\n",
    "        super(R_GCNLayer, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.num_relations = num_relations\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        max_block_size = 1\n",
    "        for i in range(1,11):\n",
    "            if x_dim % i == 0 and y_dim % i == 0:\n",
    "                max_block_size +=1 \n",
    "        # print(max_block_size)\n",
    "        x_block_size = x_dim // block_split\n",
    "        y_block_size = y_dim // block_split\n",
    "        self.W = nn.ParameterList()\n",
    "        for _ in range(num_relations):\n",
    "            wr = nn.ParameterList()\n",
    "            # relation_weights = []\n",
    "            for i in range(block_split):\n",
    "                w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                # relation_weights.append(w)\n",
    "             #move blocks to params, not block diag\n",
    "            # print(block_diag_matrix)\n",
    "                wr.append(w)\n",
    "                # self.Wr.append(wr)\n",
    "            x_remainder = x_dim % block_split\n",
    "            y_remainder = y_dim % block_split\n",
    "            if  x_remainder > 0 and y_remainder > 0:\n",
    "                w = nn.Parameter(torch.randn(x_remainder, y_remainder)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                wr.append(w)\n",
    "            self.W.append(wr)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "        for r in range(self.num_relations):\n",
    "            X = X.to_dense()\n",
    "            if X.is_sparse:\n",
    "                diag = block_diag(self.W[r]).to_dense()\n",
    "                correct_indices = (indices >= 0).all(dim=0)\n",
    "                X = X.coalesce()\n",
    "                # X.indices = X.indices()[:, correct_indices]\n",
    "                # X.values = X.values()[correct_indices] \n",
    "                if diag.is_sparse: #this really defeats the point of using sparse matrices, but torch.sparse only supports sparse, dense mm\n",
    "                    # print(dir(X))\n",
    "                    diag = diag.to_dense()\n",
    "                    \n",
    "                weighted = torch.matmul(X, diag)\n",
    "            else:\n",
    "                weighted = torch.matmul(X, block_diag(self.W[r])) \n",
    "                # print(weighted.size())\n",
    "            if A[r].is_sparse:\n",
    "                # transformed = torch.sparse.mm(A[r], weighted)\n",
    "                transformed = torch.matmul(A[r], weighted)\n",
    "            else:\n",
    "                transformed = torch.matmul(A[r], weighted)\n",
    "            # print(transformed.size(), aggregated.size())\n",
    "            aggregated += transformed\n",
    "        aggregated += self.bias\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "# class R_GCNLayer(nn.Module):\n",
    "#     def __init__(self, x_dim, y_dim, num_relations):\n",
    "#         super(R_GCNLayer, self).__init__()\n",
    "#         self.W = nn.ModuleList([nn.Linear(x_dim, y_dim) for _ in range(num_relations)])\n",
    "#         self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "\n",
    "#     def forward(self, A, X):\n",
    "#         out = torch.zeros_like(X)\n",
    "#         for r, (A_r, W_r) in enumerate(zip(A, self.W)):\n",
    "#             out += torch.spmm(A_r, W_r(X))\n",
    "#         out += self.bias\n",
    "#         return out\n",
    "\n",
    "# class R_GCNLayer(nn.Module): \n",
    "#     def __init__(self, x_dim, y_dim, num_relations, bases=5, block_split=2):\n",
    "#         super(R_GCNLayer, self).__init__()\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.num_relations = num_relations\n",
    "#         gain = nn.init.calculate_gain('relu')\n",
    "#         max_block_size = 1\n",
    "#         x_block_size = x_dim // block_split\n",
    "#         y_block_size = y_dim // block_split\n",
    "#         self.W = nn.ParameterList()\n",
    "#         self.relation_vectors = nn.ParameterList()\n",
    "#         for _ in range(num_relations):\n",
    "#             self.relation_vectors.append(nn.Parameter(torch.randn(1,x_dim)))\n",
    "#         wr = nn.ParameterList()\n",
    "#         for i in range(block_split):\n",
    "#             w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "#             nn.init.kaiming_uniform_(w, a=gain)\n",
    "#             # relation_weights.append(w)\n",
    "#          #move blocks to params, not block diag\n",
    "#         # print(block_diag_matrix)\n",
    "#             wr.append(w)\n",
    "#             # self.Wr.append(wr)\n",
    "#         x_remainder = x_dim % block_split\n",
    "#         y_remainder = y_dim % block_split\n",
    "#         if  x_remainder > 0 and y_remainder > 0:\n",
    "#             w = nn.Parameter(torch.randn(x_remainder, y_remainder)) \n",
    "#             nn.init.kaiming_uniform_(w, a=gain)\n",
    "#             wr.append(w)\n",
    "#         self.W = wr\n",
    "#         print(f\"W: {self.W[0].size()}\\nVr:{self.relation_vectors[0].size()}\")\n",
    "\n",
    "#         self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "#         nn.init.zeros_(self.bias)\n",
    "#     def __init__(self, x_dim, y_dim, num_relations, num_bases=30):\n",
    "#         super(R_GCNLayer, self).__init__()\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.num_relations = num_relations\n",
    "#         self.num_bases = num_bases\n",
    "\n",
    "#         gain = nn.init.calculate_gain('relu')\n",
    "#         self.W_b = nn.Parameter(torch.Tensor(self.num_bases, self.x_dim, self.y_dim))\n",
    "#         nn.init.kaiming_uniform_(self.W_b, a=gain)\n",
    "#         self.W_r = nn.Parameter(torch.Tensor(self.num_relations, self.num_bases))\n",
    "#         nn.init.kaiming_uniform_(self.W_r, a=gain)\n",
    "#         self.bias = nn.Parameter(torch.Tensor(self.y_dim))\n",
    "#         nn.init.zeros_(self.bias)\n",
    "\n",
    "#     def forward(self, A, X):\n",
    "#         aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "#         weights = torch.matmul(self.W_r, self.W_b.view(self.num_bases, -1))\n",
    "#         weights = weights.view(self.num_relations, self.x_dim, self.y_dim)        \n",
    "#         for r in range(self.num_relations):\n",
    "#             A_r = A[r]\n",
    "#             relation_weights = weights[r]\n",
    "#             weighted = torch.sparse.mm(A_r, X)\n",
    "#             print(weighted.size(), relation_weights.size())\n",
    "#             weighted = torch.matmul(weighted, relation_weights)\n",
    "#             print(weighted)\n",
    "#             aggregated += weighted\n",
    "        \n",
    "#         aggregated += self.bias\n",
    "#         return aggregated\n",
    "\n",
    "class R_GCN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, y_dim, max_k_hop, num_relations):\n",
    "        super(R_GCN, self).__init__()\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.num_relations = num_relations\n",
    "        self.x_dim = x_dim\n",
    "        self.block_split = 1\n",
    "        self.bases = 1\n",
    "        self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations, self.bases) for _ in range(max_k_hop)])\n",
    "        # self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations) for _ in range(max_k_hop)])\n",
    "        self.final_r_gcn = R_GCNLayer(h_dim, y_dim,num_relations, self.bases )\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.norm = nn.BatchNorm1d(y_dim)\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        # if X.size(0) % 2 !=0:\n",
    "        #     X = X[:-1,:]\n",
    "        # outer layers\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(self.max_k_hop):\n",
    "            H = self.gcns[i](A, X)\n",
    "            H = self.act(H)\n",
    "            H = self.drop(H)\n",
    "        Y = self.final_r_gcn(A, H)\n",
    "        out = self.act(Y)\n",
    "        out = self.norm(out)\n",
    "        out = self.soft(out)\n",
    "        return out#nn.LogSoftmax(Y)\n",
    "\n",
    "#feature_matrix.size(0)+1\n",
    "num_classes=5\n",
    "r_gcn = R_GCN(feature_matrix_cat.size(1), 32, num_classes, max_k_hop=2, num_relations=i+1).to(device)\n",
    "out = r_gcn([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], feature_matrix_cat)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc635fd6-d7b2-4ca0-9fa6-d59d3f29bfb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "coalesce expected sparse coordinate tensor layout but got Strided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mindices()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: coalesce expected sparse coordinate tensor layout but got Strided"
     ]
    }
   ],
   "source": [
    "feature_matrix.coalesce().indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3ec9a-84e0-41dc-97a0-2b8cb5489fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c226473-516c-430b-9108-ef4c06d2d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "6338/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea53cc-d72e-48d2-a3b9-dba17fe6a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4efe3-34ad-48c7-b56e-4d040b9710c7",
   "metadata": {},
   "source": [
    "# Why are these zeros...?           ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c855e-463c-4006-afa6-ea8a1508684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_list = [normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e3316a6c-75ac-4126-9291-f84f19333d76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MR_GCN(nn.Module):\n",
    "    def __init__(self, A_list, eye, num_nodes, rgcn_inp_dim, embed_size=391, num_classes=5):\n",
    "        super(MR_GCN, self).__init__()\n",
    "        \n",
    "        self.A_list = A_list\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding_matrix = torch.zeros((num_nodes, embed_size), dtype=torch.float32, device=device)#.to_sparse()\n",
    "        self.MME= MultiModalEncoder(feature_matrix=self.embedding_matrix, encoders_config=encoders_config).to(device)\n",
    "        self.graph_dim = A_list[0].size(0)\n",
    "        self.R_GCN = R_GCN(rgcn_inp_dim, 128, num_classes, max_k_hop=2, num_relations=len(A_list)).to(device)\n",
    "        self.eye = eye\n",
    "        self.feature_matrix = []\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        X = self.MME()\n",
    "        X = torch.cat((self.eye,X), dim=1)\n",
    "        print(self.A_list[0].size(),X.size())\n",
    "        X = self.R_GCN(self.A_list, X)\n",
    "        return X\n",
    "mr_gcn = MR_GCN([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], identity, num_nodes, feature_matrix_cat.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "8a992be4-5f05-41ef-8ae7-fdbd1537c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80122/2817622372.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1788, 0.1469, 0.2777, 0.2607, 0.1359],\n",
       "        [0.1877, 0.1543, 0.3718, 0.1437, 0.1427],\n",
       "        [0.1530, 0.1258, 0.0947, 0.5102, 0.1163],\n",
       "        ...,\n",
       "        [0.0723, 0.2053, 0.5932, 0.0742, 0.0550],\n",
       "        [0.2077, 0.0940, 0.5423, 0.0691, 0.0869],\n",
       "        [0.2656, 0.2183, 0.1536, 0.1606, 0.2019]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_gcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "5c272ea1-1625-4f5c-9c78-bd430ca27928",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_label_set|val_label_set|test_label_set)\n",
    "num_classes = len(labels)\n",
    "ohe = torch.eye(num_classes)\n",
    "label_to_ohe = {label:ohe[i] for i, label in enumerate(labels)}\n",
    "label_to_num = {label:i for i, label in enumerate(labels)}\n",
    "\n",
    "labels = torch.full((num_nodes,), -1, dtype=torch.long, device=device)\n",
    "\n",
    "train_labels = labels#.clone()\n",
    "for node_id, label in train_node_label_map.items():\n",
    "    train_labels[node_id] = label_to_num[label]\n",
    "\n",
    "train_mask = labels>=0\n",
    "\n",
    "test_labels = labels#.clone()\n",
    "for node_id, label in test_node_label_map.items():\n",
    "    test_labels[node_id] = label_to_num[label]\n",
    "\n",
    "test_mask = labels>=0\n",
    "\n",
    "val_labels = labels#.clone()\n",
    "for node_id, label in val_node_label_map.items():\n",
    "    val_labels[node_id] = label_to_num[label]\n",
    "\n",
    "val_mask = labels>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5d8eae3a-c7e3-426d-bdcf-ac3bb0984de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5984a686-acd2-4811-b7bf-bd0977cd330f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4911])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b3f4f-db9d-4c26-8a58-31fe9853667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada288fa-c4f4-4b12-badb-9e2025f6ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[train_mask].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f2028-dfff-44fa-940a-f4161e873aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[train_mask].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "80c6ba89-dbd3-45d8-a638-259c0128f88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([773,  57,  21,  10,  10,  27,  13,  14,  23,  11,  27,  24,  14,  20,\n",
       "         73,  94,  36,  75,  45,  75])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset_s.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "575fdbe3-ade7-46a0-b438-d653a81cd406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1, -1,  ..., -1, -1, -1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "520b4aca-b6c3-4f8b-b485-55c1f929a5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://data.labs.pdok.nl/rce/def/agrarischGebouw',\n",
       " 'https://data.labs.pdok.nl/rce/def/kasteelLandhuis',\n",
       " 'https://data.labs.pdok.nl/rce/def/kerk',\n",
       " 'https://data.labs.pdok.nl/rce/def/molen',\n",
       " 'https://data.labs.pdok.nl/rce/def/woonhuis'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "47e36100-8d42-4ce0-85c9-52586c5c64cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80122/2817622372.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80122/2817622372.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/royal-cookings/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "avg loss: 1.5321375131607056, val_loss: 1.6412467956542969 val accuracy: 0.16666666666666666\n",
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80122/2817622372.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/royal-cookings/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "avg loss: 1.5935717821121216, val_loss: 1.6373592615127563 val accuracy: 0.16666666666666666\n",
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80122/2817622372.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/royal-cookings/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "avg loss: 1.5430285930633545, val_loss: 1.6352816820144653 val accuracy: 0.16666666666666666\n",
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80122/2817622372.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/royal-cookings/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "avg loss: 1.5844247341156006, val_loss: 1.6334501504898071 val accuracy: 0.16666666666666666\n",
      "torch.Size([4911, 4911]) torch.Size([4911, 5302])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[360], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m mr_gcn\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmake_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmr_gcn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_gcn_torchviz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     28\u001b[0m     val_outputs \u001b[38;5;241m=\u001b[39m mr_gcn()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchviz/dot.py:65\u001b[0m, in \u001b[0;36mmake_dot\u001b[0;34m(var, params, show_attrs, show_saved, max_attr_chars)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_dot\u001b[39m(var, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, show_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, show_saved\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_attr_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Produces Graphviz representation of PyTorch autograd graph.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    If a node represents a backward function, it is gray. Otherwise, the node\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m            to display for any given attribute.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mLooseVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m LooseVersion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.9\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m     66\u001b[0m         (show_attrs \u001b[38;5;129;01mor\u001b[39;00m show_saved):\n\u001b[1;32m     67\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_dot: showing grad_fn attributes and saved variables\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m requires PyTorch version >= 1.9. (This does NOT apply to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m saved tensors saved by custom autograd functions.)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/setuptools/_distutils/version.py:55\u001b[0m, in \u001b[0;36mVersion.__init__\u001b[0;34m(self, vstring)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vstring:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse(vstring)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mwarnings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistutils Version classes are deprecated. \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUse packaging.version instead.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;167;43;01mDeprecationWarning\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(mr_gcn.parameters())\n",
    "optimizer = torch.optim.Adam(params,lr=0.1)\n",
    "\n",
    "num_epochs = 200\n",
    "mr_gcn = MR_GCN([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], identity, num_nodes, feature_matrix_cat.size(1), num_classes=num_classes)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mr_gcn.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = mr_gcn()\n",
    "    # print(outputs.size())\n",
    "    loss = criterion(outputs[train_mask], labels[train_mask])\n",
    "    # print(f\"loss: {loss}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    for p in mr_gcn.parameters():\n",
    "        pass\n",
    "        # print(p.grad.norm())\n",
    "    optimizer.step()\n",
    "    \n",
    "    mr_gcn.eval()\n",
    "    make_dot(labels, params=dict(list(mr_gcn.named_parameters()))).render(\"r_gcn_torchviz\", format=\"png\")\n",
    "    with torch.no_grad():\n",
    "        val_outputs = mr_gcn()\n",
    "        val_loss = criterion(val_outputs[val_mask], labels[val_mask])\n",
    "        \n",
    "        _, predicted = torch.max(val_outputs[val_mask], dim=1)\n",
    "        correct = (predicted == labels[val_mask]).sum().item()\n",
    "        tot = val_mask.sum().item()\n",
    "        if tot:\n",
    "            \n",
    "            val_acc = correct / tot\n",
    "        if epoch % 1 == 0 and tot:\n",
    "            print(f\"epoch: {epoch}\\navg loss: {loss}, val_loss: {val_loss} val accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "52980ce3-c2b8-4603-ab73-7e6fc7478abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f28a9-1f12-4f1f-a7fb-f65757b31eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "adfd1621-8248-4369-b861-91ae5e9dee71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1, 2, 2, 3, 1])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1648e2-3f0a-42f6-82be-2afcbb8d05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask[335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83015b-7c0a-419f-98aa-21a15d2f0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_node_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94939049-3943-4c41-a3fe-d82101c73017",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c43b9a-e892-4816-9772-e97bc79383fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23826ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
