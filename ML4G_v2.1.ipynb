{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9a3cc9-f2e3-473a-8286-d9377071d310",
   "metadata": {},
   "source": [
    "# Version 2 (sparse matrices...)\n",
    "### I created the last notebook naively and completely missed that the discusses method operates on sparse matrices, so I'm starting again from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c292f-2131-4f8e-a58b-03aff54d508a",
   "metadata": {},
   "source": [
    "# Generating a sparse matrix representation of a graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4159d-e344-4900-9c07-e02cd1556604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508eddc7-743a-4d1e-a43c-aad72c579ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "num_nodes = 100\n",
    "average_out_degree = 9\n",
    "total_edge_classes = 36\n",
    "embedding_size = 128\n",
    "output_dim = 4\n",
    "\n",
    "\n",
    "\n",
    "# First we generate some arbitrary graph\n",
    "def generate_graph(num_nodes, average_out_degree, total_edge_classes):\n",
    "    num_edges = average_out_degree*num_nodes\n",
    "    indices = torch.randint(0, num_nodes, (2, num_edges))\n",
    "    print(indices.size())\n",
    "    edges = torch.ones(num_edges)\n",
    "    print(edges.size())\n",
    "    A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes))\n",
    "    return A\n",
    "# A = generate_graph(15,3,5).to_dense()\n",
    "\n",
    "# Now maybe it's useful to ensure it's connected...\n",
    "def generate_connected_graph(num_nodes, average_out_degree, total_edge_classes, embedding_size):\n",
    "    edges_set ,\n",
    "    num_edges = average_out_degree * num_nodes\n",
    "\n",
    "    def sample_random_edge_class():\n",
    "        return torch.randint(0, total_edge_classes, (1,)).item()\n",
    "    \n",
    "    def generate_random_edge():\n",
    "        u, v = torch.randint(0, num_nodes, (2,)).tolist()\n",
    "        #no self loops\n",
    "        while u == v:\n",
    "            u, v = torch.randint(0, num_nodes, (2,)).tolist()\n",
    "        return (u, v)\n",
    "    \n",
    "    def generate_random_node():\n",
    "        return torch.randint(0, num_nodes, (1,)).item()\n",
    "    \n",
    "    def sample_random_node(A):\n",
    "        connected_nodes = A._indices()[1].tolist() #select from incoming to...\n",
    "        if not connected_nodes:\n",
    "            return generate_random_node()\n",
    "        return connected_nodes[torch.randint(0, len(connected_nodes), (1,)).item()]#torch.nonzero(A)\n",
    "    \n",
    "    initial_edge = generate_random_edge()\n",
    "    edges_set ,.add(initial_edge)\n",
    "    indices = torch.tensor([[initial_edge[0]], [initial_edge[1]]], dtype=torch.long, device=device)\n",
    "    edges = torch.ones(1, device=device)\n",
    "    A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes), device=device)\n",
    "    A_list = []\n",
    "    for edge_class in range(total_edge_classes):\n",
    "        for _ in range(num_edges - 1):\n",
    "            u = sample_random_node(A)\n",
    "            v = generate_random_node()\n",
    "            while u == v:\n",
    "                v = generate_random_node()\n",
    "            new_edge = (u, v)#torch.cat([u, v], dim=0).unsqueeze(1)\n",
    "            if new_edge in edges_set ,:\n",
    "                continue\n",
    "            edges_set ,.add(new_edge)\n",
    "            edge_tensor = torch.tensor([[u], [v]], dtype=torch.long, device=device)\n",
    "            indices = torch.cat([indices, edge_tensor], dim=1)\n",
    "            edges = torch.ones(indices.size(1), device=device)\n",
    "            A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes), device=device)\n",
    "            # indices = torch.stack((indices,torch.cat([u,v]).unsqueeze(1)),dim=0)\n",
    "        # print(edges_set ,)\n",
    "        A_list.append(A)\n",
    "    random_labels = torch.rand((num_nodes, embedding_size), device=device)\n",
    "    return A_list, random_labels \n",
    "\n",
    "A, X = generate_connected_graph(num_nodes,average_out_degree,total_edge_classes, embedding_size)\n",
    "\n",
    "# Now per edge type? or randomly assign edges...? maybe multinomial_sample(1/k)^k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687621c-5ccc-4eba-b2e1-9006d1bcd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c52b3f-618e-4ab2-ac37-251e542ace6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cb952-daaf-4992-8e60-22b83795f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175c831-5866-40bd-8f7c-3ce5e4d1587a",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db5cdd-6881-47d5-92e8-b3742e910537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## stopped working after some edits to the graph generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300d86c-3c4b-49a5-b447-6f48bce0629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.functional as f\n",
    "\n",
    "# k_hop = 3\n",
    "\n",
    "# def normalize_adjacency(A, self=True): # Self-loop doesn't work with R-GCN\n",
    "#     size = A.size()[0]\n",
    "#     A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "#     degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "#     # print(degree.size())\n",
    "#     d_inv_sqrt = degree.pow(-0.5)\n",
    "#     D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "#     A = A.to_dense()\n",
    "#     normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "#     return normalized_A\n",
    "# A_prime = normalize_adjacency(A)\n",
    "\n",
    "\n",
    "# class GCNLayer(nn.Module):\n",
    "#     def __init__(self, x_dim, y_dim):\n",
    "#         super(GCNLayer, self).__init__()\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         # self.A_norm = A_norm\n",
    "\n",
    "#         self.lin = nn.Linear(x_dim, y_dim)\n",
    "\n",
    "#     def forward(self, A_norm, X):\n",
    "#         device = next(self.parameters()).device\n",
    "#         # print(A_norm, X)\n",
    "#         transformed = self.lin(X)\n",
    "#         aggregated = torch.matmul(A_norm, transformed)\n",
    "#         return aggregated\n",
    "\n",
    "\n",
    "# class GCN(nn.Module):\n",
    "#     def __init__(self, x_dim, h_dim, y_dim, max_k_hop):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.max_k_hop =max_k_hop\n",
    "#         self.x_dim = x_dim\n",
    "#         self.gcns = nn.ModuleList([GCNLayer(x_dim, h_dim) for _ in range(max_k_hop)])\n",
    "#         self.final_gcn = GCNLayer(x_dim, y_dim) \n",
    "#         self.act = nn.ReLU()\n",
    "#         self.drop = nn.Dropout(p=0.2)\n",
    "#         # self.norm = nn.LayerNorm(y_dim)\n",
    "    \n",
    "#     def forward(self, A, X):\n",
    "#         # outer layers\n",
    "#         device = next(self.parameters()).device\n",
    "#         for i in range(self.max_k_hop):\n",
    "#             H = self.gcns[i](A, X)\n",
    "#             H = self.act(H)\n",
    "#             H = self.drop(H)\n",
    "#         Y = self.final_gcn(A, H)\n",
    "#         out = self.act(Y)\n",
    "#         return out#nn.LogSoftmax(Y)\n",
    "# model = GCN(X.size(1), embedding_size, output_dim, k_hop).to(device)\n",
    "# out = model(A_prime, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870f0f8-f8b3-4003-ac61-f7d6c47a1e46",
   "metadata": {},
   "source": [
    "# R-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b77995-b03b-409e-ade0-c2e40cbc34a9",
   "metadata": {},
   "source": [
    "### Block diagonal weight matrix (one is held in memory for each relational weight per layer, so block diagonal sparse matrices save some memory at the cost of some layer-level information flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e4257-f40f-44c9-8322-22e4cf8fc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = embedding_size//4 #some partition... ensure it's a round number\n",
    "relation_weights = [torch.randn(block_size, block_size), torch.randn(block_size, block_size)]\n",
    "\n",
    "block_diag_matrix = torch.block_diag(*relation_weights).to_sparse()\n",
    "block_diag_matrix.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d2a2a-7cb7-4133-9c54-dc0d6022b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( embedding_size//4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c8b48-1db0-4837-95f6-cb2978bc6949",
   "metadata": {},
   "source": [
    "# R-GCN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b2e37-fd0f-4e3d-88a5-ec99fe15d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A, self=True):\n",
    "    size = A.size()[0]\n",
    "    A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "    degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "    d_inv_sqrt = degree.pow(-0.5)\n",
    "    D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    A = A.to_dense()\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "\n",
    "A_list = [normalize_adjacency(a, False) for a in A]\n",
    "\n",
    "\n",
    "##save non-error version\n",
    "def block_diag (weights):\n",
    "    block_diag_matrix = torch.block_diag(*weights).to_sparse()\n",
    "    return block_diag_matrix\n",
    "    \n",
    "\n",
    "class R_GCNLayer(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, num_relations, block_split):\n",
    "        super(R_GCNLayer, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.num_relations = num_relations\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        max_block_size = 1\n",
    "        for i in range(1,11):\n",
    "            if x_dim % i == 0 and y_dim % i == 0:\n",
    "                max_block_size +=1 \n",
    "        x_block_size = x_dim // block_split\n",
    "        y_block_size = y_dim // block_split\n",
    "        self.W = nn.ParameterList()\n",
    "        for _ in range(num_relations):\n",
    "            wr = nn.ParameterList()\n",
    "            for i in range(block_split):\n",
    "                w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                wr.append(w)\n",
    "            self.W.append(wr)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "        for r in range(self.num_relations):\n",
    "            weighted = torch.matmul(X, block_diag(self.W[r]))\n",
    "            transformed = torch.sparse.mm(A[r], weighted)\n",
    "            aggregated += transformed\n",
    "        aggregated += self.bias\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class R_GCN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, y_dim, max_k_hop, num_relations):\n",
    "        super(R_GCN, self).__init__()\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.num_relations = num_relations\n",
    "        self.x_dim = x_dim\n",
    "        self.block_split = 2\n",
    "        self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations, self.block_split) for _ in range(max_k_hop)])\n",
    "        self.final_r_gcn = R_GCNLayer(x_dim, y_dim,num_relations, self.block_split) \n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.norm = nn.BatchNorm1d(y_dim)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(self.max_k_hop):\n",
    "            H = self.gcns[i](A, X)\n",
    "            H = self.act(H)\n",
    "            H = self.drop(H)\n",
    "        Y = self.final_r_gcn(A, H)\n",
    "        out = self.act(Y)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "model = R_GCN(X.size(1), embedding_size, output_dim, k_hop, total_edge_classes).to(device)\n",
    "out = model(A_list, X)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738743cd-c50b-4103-8130-e28d8746ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7295ba-260f-4eb1-86c4-6c1c2dd67825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f738fa3-694a-46dc-bdf6-9438be741943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc738f59-b939-4037-8859-d4e70f0b9150",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c0aef-fcd4-4f35-bb0a-23bb5d85e2a2",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "from tqdm import tqdm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(model.parameters())\n",
    "optimizer = torch.optim.Adam(params,lr=0.01)\n",
    "\n",
    "num_epochs = 600\n",
    "\n",
    "true_labels = torch.randint(0, output_dim, (num_nodes,), dtype=torch.long).to(device)\n",
    "\n",
    "def save_gradient_hook(grad):\n",
    "    gradients.append(grad)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "\n",
    "    predicted_labels = model(A_list, X)\n",
    "    \n",
    "    loss = criterion(predicted_labels, true_labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    i += 1\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    _, predicted = torch.max(predicted_labels.data, 1)\n",
    "    total += true_labels.size(0)\n",
    "    correct += (predicted == true_labels).sum().item()\n",
    "    avg_loss = epoch_loss\n",
    "    accuracy = 100 * correct / total\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch: {epoch}\\navg loss: {avg_loss} accuracy: {accuracy}\")\n",
    "        # make_dot(predicted_labels, params=dict(list(model.named_parameters()))).render(\"r_gcn_torchviz\", format=\"png\")\n",
    "\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8288b8-687d-4645-b942-75ab8bebe744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c268095-1f9a-401d-aa33-b27ec4da3408",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92ff5184-21dd-40ee-ad5d-10a0e7632c6c",
   "metadata": {},
   "source": [
    "# I'm confused about the rdf stuff. I don't see any multimodal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6513d3-0373-42a9-b114-907ea0e5fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d0e07-a704-4346-b9b4-92d49d45becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef63b0-c9f1-44b7-ae9e-09210b4395c9",
   "metadata": {},
   "source": [
    "# Create graph from n-triple file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b7f1d5-1f5a-44d8-b1f2-764a729e737d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph\n",
    "import logging\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "\n",
    "data_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped.nt'\n",
    "mini_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped_mini.nt'\n",
    "test_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_test_set.nt\"\n",
    "folder = './Downloads/ml4g'\n",
    "subsample_chance = 1\n",
    "def create_mini_nt(infile, outfile):\n",
    "    with open(infile, 'r', encoding='utf-8') as inp, open(outfile, 'w', encoding='utf-8') as out:\n",
    "        for i, line in enumerate(inp):\n",
    "            if random.random() <= subsample_chance:\n",
    "                out.write(line)\n",
    "\n",
    "\n",
    "\n",
    "def create_new_graph(path, batch_size = 10000, test=True):\n",
    "    logging.basicConfig(\n",
    "    filename='rdf_parsing_errors.log',\n",
    "    filemode='w')\n",
    "    \n",
    "    graph = Graph()\n",
    "    batch_num = 0\n",
    "    i = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        while True:\n",
    "            batch = []\n",
    "            try:\n",
    "                \n",
    "                [batch.append(next(f)) for j in range(batch_size)]\n",
    "                i += j\n",
    "            except:\n",
    "                pass\n",
    "            if not batch:\n",
    "                break\n",
    "            batch_num += 1\n",
    "            nt_string = ''.join(batch)\n",
    "            try:\n",
    "                graph.parse(data=nt_string, format='nt')\n",
    "                if test:\n",
    "                    graph = Graph()\n",
    "            except ParseError as e:\n",
    "                logging.error(f\"in batch: {batch_num}:\\npproblematic data:\\n\\n{batch}\\n\\n\")\n",
    "                check(batch, batch_num, test=test)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def check(batch, batch_num, test = True):\n",
    "    graph = Graph()\n",
    "    for i, line in enumerate(batch):\n",
    "        try:\n",
    "            graph.parse(line)\n",
    "        except Exception as e:\n",
    "            logging.error(f'in line: {i}:\\n{line}\\n{e}')\n",
    "g=create_new_graph(data_loc, test=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5b489a-42cd-4f78-bd7d-d8945ea010ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import defaultdict\n",
    "test_label_set = set()\n",
    "train_label_set = set()\n",
    "val_label_set = set()\n",
    "\n",
    "\n",
    "test_node_label_map = defaultdict(list)\n",
    "train_node_label_map = defaultdict(list)\n",
    "val_node_label_map = defaultdict(list)\n",
    "\n",
    "\n",
    "test_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_test_set.nt\"\n",
    "train_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_train_set.nt\"\n",
    "val_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_valid_set.nt\"\n",
    "\n",
    "graph_test = create_new_graph(test_loc, test=False)\n",
    "graph_train = create_new_graph(train_loc, test=False)\n",
    "graph_val = create_new_graph(val_loc, test=False)\n",
    "\n",
    "for s,p,o in graph_test:\n",
    "    test_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        test_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "for s,p,o in graph_train:\n",
    "    train_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        train_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "\n",
    "for s,p,o in graph_val:\n",
    "    val_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        val_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "\n",
    "subj_set = set()\n",
    "for s,p,o in graph_train:\n",
    "    subj_set.add(s.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefd34b-7e40-4583-8b6c-3468a39fa917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# unique_classes = set()\n",
    "# unique_predicates = set()\n",
    "# for s,p,o in graph2:\n",
    "#     unique_predicates.add(p)\n",
    "#     if i< 35:\n",
    "#         # print(s,p,o)\n",
    "#         pass\n",
    "#     unique_classes.add(o)\n",
    "# unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090de70-b583-46ee-95b9-66a4961c91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b49d778-bc05-44ee-a7f5-cf50c2b9eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198889-->15001\n"
     ]
    }
   ],
   "source": [
    "# from rdflib import Graph\n",
    "# import random\n",
    "# def to_connected_small(g,subsample_chance=0.8, barrier=100,max_len=15000, verbose=True, save=True):\n",
    "#     obj_set = set()\n",
    "#     mini_graph = Graph()\n",
    "#     i=0\n",
    "#     j=0\n",
    "#     for s,p,o in g:\n",
    "#         if j > max_len:\n",
    "#             break\n",
    "#         i+=1\n",
    "#         if random.random() >= subsample_chance and (i<barrier or s in obj_set or o in obj_set):\n",
    "#             obj_set.add(s)\n",
    "#             obj_set.add(o)\n",
    "#             j+=1\n",
    "#             mini_graph.add(triple=(s,p,o))\n",
    "#     print(f\"{i}-->{j}\") if verbose else 0\n",
    "#     return mini_graph\n",
    "# graph = to_connected_small(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892957c9-1e49-4af8-92aa-1c1d62bb5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79799-->5001\n",
      "56791-->5001\n",
      "50847-->5001\n",
      "45801-->5001\n",
      "42079-->5001\n",
      "40678-->5001\n",
      "37573-->5001\n",
      "35753-->5001\n",
      "34018-->5001\n",
      "33302-->5001\n",
      "32205-->5001\n",
      "31450-->5001\n",
      "31236-->5001\n",
      "30512-->5001\n",
      "30101-->5001\n",
      "29370-->5001\n",
      "29192-->5001\n",
      "29660-->5001\n",
      "28802-->5001\n",
      "28357-->5001\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "import random\n",
    "import pickle\n",
    "def create_small_graphs(g,subsample_chance=0.8, barrier=100,max_len=5000, verbose=True, save=True, graph_loc=f'{folder}/graphs/'):\n",
    "    done=False\n",
    "    # while not done:\n",
    "    obj_set = set()\n",
    "    mini_graph = Graph()\n",
    "    i=0\n",
    "    j=0\n",
    "    graph_num=0\n",
    "    for s,p,o in g:\n",
    "        if j > max_len:\n",
    "            graph_name = f'graph_{graph_num}.pkl'\n",
    "            if save:\n",
    "                print(f\"{i}-->{j}\") if verbose else 0\n",
    "                with open(graph_loc+graph_name, 'wb') as f:\n",
    "                    pickle.dump(mini_graph, f)\n",
    "            mini_graph = Graph()\n",
    "            i,j = 0,0\n",
    "            graph_num += 1\n",
    "        i+=1\n",
    "        if random.random() >= subsample_chance and (i<barrier or s in obj_set or o in obj_set):\n",
    "            obj_set.add(s)\n",
    "            obj_set.add(o)\n",
    "            j+=1\n",
    "            mini_graph.add(triple=(s,p,o))\n",
    "        \n",
    "        \n",
    "        \n",
    "    # return mini_graph\n",
    "create_small_graphs(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d1db7-ff84-4dc5-bda5-e74a315a5d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for s,p,o in graph:\n",
    "    if i < 20:\n",
    "        # print(s,p,o)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd6b40-16da-40e5-8763-4444b9e31865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a57e8b-fb25-4b03-9865-3b0b786e61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733ae1d-f51b-4bf0-9fef-fa093f405b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(edge_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4645d0-128d-406b-9fff-711c039c4b6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file_path = os.path.join(folder, 'rdf_graph_l.pkl')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(g, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80590000-b2f4-4a06-80fc-d70ae965d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pickle\n",
    "import os\n",
    "import rdflib\n",
    "import logging\n",
    "folder = './Downloads/ml4g'\n",
    "\n",
    "pickle_file_path = os.path.join(folder, 'rdf_graph.pkl')\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef0fb2b0-b4fc-42b1-8a3c-75e7637b6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import defaultdict\n",
    "import torchvision.transforms as trv\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from string import printable\n",
    "import pickle\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, strings, character_map, max_length=5000):\n",
    "        # self.strings = strings\n",
    "        self.character_map = character_map\n",
    "        self.max_length = max_length\n",
    "        self.strings = strings\n",
    "        self.ids = []\n",
    "        self.pad = False\n",
    "        ids = []\n",
    "        for string in strings:\n",
    "            try:\n",
    "                self.ids.append(inv_node_map[string])\n",
    "            except KeyError:\n",
    "                self.ids.append(0) #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strings)\n",
    "\n",
    "    def __getitem__(self, i, pad=True):\n",
    "        s = self.strings[i]\n",
    "        tokens = tokenize_string(s, self.character_map, max_len=self.max_length)\n",
    "\n",
    "        return torch.tensor([self.ids[i]]+tokens, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, encoded_images, transform):\n",
    "        self.encoded_images = encoded_images\n",
    "        self.transform = transform\n",
    "        self.ids = [inv_node_map[encoded_image] for encoded_image in encoded_images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        encoded_str = self.encoded_images[i]\n",
    "        image, _ = decode_base64_image(encoded_str, log_note=f\"inside generator, image nr: {i}\")\n",
    "        image = self.transform(image)\n",
    "        return self.ids[i], image\n",
    "\n",
    "class NumericalDataset(Dataset):\n",
    "    def __init__(self, numbers, log_scale=False):\n",
    "        self.numbers = numbers\n",
    "        self.log_scale = log_scale\n",
    "        self.max_num = max(numbers)\n",
    "        self.ids = [inv_node_map[str(number)] for number in numbers]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        n = self.numbers[i]\n",
    "        norm = n / self.max_num\n",
    "        norm = math.log(norm + 1e-8) if self.log_scale else norm\n",
    "        return self.ids[i], torch.tensor([norm], dtype=torch.float32, device=device)\n",
    "\n",
    "class DateTimeDataset(Dataset):\n",
    "    # todo: check if there are other temporal feature types in the data. i.e. just years etc.\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates#torch.stack([get_dates(date) for date in dates], dim=0)\n",
    "        self.ids = [inv_node_map[date] for date in dates]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_date(date)\n",
    "        return self.ids[i], date_feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class YearDataset(Dataset):\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates\n",
    "        self.ids = [inv_node_map[date] for date in dates]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_year(date)\n",
    "        return self.ids[i], date_feature\n",
    "\n",
    "\n",
    "def process_point(point_str, highest_x, highest_y):\n",
    "    if 'POINT' in point_str:\n",
    "        point_str = point_str.split('POINT(')[1].split('))')[0]\n",
    "    elif 'Point' in point_str:\n",
    "        point_str = point_str.split('Point(')[1].split('))')[0]\n",
    "    point_list = point_str.strip(')').strip('(').split()\n",
    "    point = [float(coord) for coord in point_list]\n",
    "    point.extend([0,0,0])\n",
    "    point = tuple(point)\n",
    "    \n",
    "    point_x,point_y,_,_,_ = point\n",
    "    highest_x = point_x if point_x > highest_x else highest_x\n",
    "    highest_y = point_y if point_y > highest_y else highest_y\n",
    "    return point, highest_x, highest_y\n",
    "\n",
    "    \n",
    "def process_multipoly(multipoly_str, max_x, max_y):\n",
    "    multipoly_str = multipoly_str.split('(((')[-1].split(')))')[0]\n",
    "    sub_polys = multipoly_str.split(') (')\n",
    "    multipoly = []\n",
    "    for i, sub_poly in enumerate(sub_polys):\n",
    "        aligned_sub_poly = 'POLYGON ((' + sub_poly + '))'\n",
    "        multipoly.append(get_num_data(aligned_sub_poly, max_x, max_y, last_multi=i==len(sub_polys)))\n",
    "    return multipoly\n",
    "        \n",
    "\n",
    "def get_num_data(poly_str, maximum_x, maximum_y, last_multi=False):\n",
    "    poly_str = poly_str.split('(')[-1].split(')')[0]\n",
    "    poly_combi_str_list = [poly for poly in poly_str.split(',')]\n",
    "    try:\n",
    "        poly_tupled = [(float(poly.split()[0]),float(poly.split()[1]),1,0,0) if i < len(poly_combi_str_list)-1  else (float(poly.split()[0]),float(poly.split()[1]),0,0,1) if last_multi else (float(poly.split()[0]),float(poly.split()[1]),0,1,0)\n",
    "                       for i,poly in enumerate(poly_combi_str_list)]\n",
    "    except ValueError:\n",
    "        poly_tupled = [(float(poly.strip(')').strip('(').split()[0]),float(poly.strip(')').strip('(').split()[1]),1,0,0) if i < len(poly_combi_str_list)-1  else (float(poly.split()[0]),float(poly.split()[1]),0,0,1) if last_multi else (float(poly.split()[0]),float(poly.split()[1]),0,0,1)\n",
    "                       for i,poly in enumerate(poly_combi_str_list)]\n",
    "    x_max, y_max = max([x for x,y,_,_,_ in poly_tupled]), max([y for x, y,_,_,_ in poly_tupled])\n",
    "\n",
    "    x_max =  x_max if x_max > maximum_x else maximum_x\n",
    "    y_max =  y_max if y_max > maximum_y else maximum_y\n",
    "    x_mean = sum([tup[0] for tup in poly_tupled])/len(poly_tupled)\n",
    "    y_mean = sum([tup[1] for tup in poly_tupled])/len(poly_tupled)\n",
    "    return poly_tupled, x_mean, y_mean, x_max, y_max\n",
    "\n",
    "def tokenize_string(s, character_map, max_len=5000):\n",
    "    s = s[:max_len]\n",
    "    tokens = [character_map[char] for char in s]\n",
    "    return tokens #\n",
    "\n",
    "def encode_image(encoded_str, transform):\n",
    "    #preprocess for encoder[2](2 layer cnn -->embedding_dim)\n",
    "    image, img_bytes = decode_base64_image(encoded_str)\n",
    "    return transform(image), img_bytes\n",
    "\n",
    "def encode_num(n, max_num, log=False):\n",
    "    v = n/max_num\n",
    "    v = math.log(v) if log else v\n",
    "    return torch.tensor((v),dtype=torch.float32, device=device)\n",
    "\n",
    "def encode_polygon(poly, global_mean_x, global_mean_y, x_max, y_max):\n",
    "    poly_tensor_x = (global_mean_x-torch.tensor([ point[0] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/x_max\n",
    "    poly_tensor_y = (global_mean_y-torch.tensor([ point[1] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/y_max\n",
    "    return torch.stack((poly_tensor_x,poly_tensor_y),dim=0)\n",
    "\n",
    "def encode_point(point, max_x, max_y):\n",
    "    \n",
    "    point = torch.tensor(point,dtype=torch.float32, device=device)\n",
    "    div = torch.tensor((max_x,max_y),dtype=torch.float32, device=device)\n",
    "    return point/div\n",
    "\n",
    "def encode_date(date):\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    split_str = date.split('-')\n",
    "    years_str = split_str[0]\n",
    "    month_str = split_str[1]\n",
    "    day_str = split_str[2]\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    months = cyclical(int(month_str), 12)\n",
    "    days = cyclical(int(day_str), 31)\n",
    "    return torch.cat((centuries, decades, years, months, days), dim=0)\n",
    "\n",
    "def encode_year(year):\n",
    "    #preprocess for temporal encoder[4](? layer ffnn -->embedding_dim)\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "        \n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    years_str = year\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    return torch.cat((centuries, decades, years), dim=0)\n",
    "\n",
    "\n",
    "class SpatialDataset(Dataset):\n",
    "    def __init__(self, spatial_data, global_mean_x, global_mean_y, x_max, y_max, ids):\n",
    "        self.spatial_data = spatial_data\n",
    "        self.global_mean_x = global_mean_x\n",
    "        self.global_mean_y = global_mean_y\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "        self.max_len = max([len(data) for data in spatial_data])\n",
    "        self.ids = ids#[inv_node_map[spatial_datum] for spatial_datum in spatial_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spatial_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        spatial = self.spatial_data[i]\n",
    "        spatial_tensors = []\n",
    "        if isinstance(spatial, list):\n",
    "            for x,y,a,b,c in spatial:\n",
    "                \n",
    "                x = (x - self.global_mean_x) / self.x_max\n",
    "                y = (y - self.global_mean_y) / self.y_max\n",
    "                coord_tensor = torch.tensor([x,y,a,b,c], device=device)\n",
    "                spatial_tensors.append(coord_tensor)\n",
    "            spatial_tensor = torch.stack(spatial_tensors,dim=0)\n",
    "        elif isinstance(spatial, tuple):\n",
    "            x, y,_,_,_ = spatial\n",
    "            spatial_tensor = torch.tensor([(x / self.x_max, y / self.y_max,0,0,0)], dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            spatial_tensor = torch.zeros((1, 2), dtype=torch.float32, device=device)\n",
    "        return self.ids[i], spatial_tensor\n",
    "\n",
    "    \n",
    "\n",
    "    def decode_base64_image(encoded_str,log_note='pass values to decode_base_64_jpg'): # - to +, _ to /\n",
    "        \"\"\"\n",
    "        encoded_str: url safe base 64 jpg string --> image bytes string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_bytes = base64.urlsafe_b64decode(encoded_str)\n",
    "            image_obj = Image.open(BytesIO(image_bytes))\n",
    "            image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "            return image, image_obj\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{e} error encoding image at {log_note}\")\n",
    "            print(e)\n",
    "            return None, None\n",
    "    \n",
    "    def save_bytes_to_jpg(image_bytes, item_num=0,folder='Downloads/ml4g/image_data/',name='Fred'):\n",
    "        filename = f'{folder}{name}_{item_num}.jpg'\n",
    "        with open(filename, 'wb') as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "        \n",
    "        class DateTimeDataset(Dataset):\n",
    "            # todo: check if there are other temporal feature types in the data. i.e. just years etc.\n",
    "            def __init__(self, dates):\n",
    "                \n",
    "                self.dates = dates\n",
    "                self.ids = [inv_node_map[date] for date in dates]\n",
    "    \n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.dates)\n",
    "    \n",
    "        def __getitem__(self, i):\n",
    "            date = self.dates[i]\n",
    "            date_feature = encode_date(date)\n",
    "            return self.ids[i], date_feature\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1a7a1df-aa03-4867-b493-41b5e47c30d3",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(graph):\n",
    "    global inv_node_map \n",
    "    global node_map\n",
    "    global edge_map\n",
    "    global inv_edge_map\n",
    "    global device\n",
    "    \n",
    "    \n",
    "    node_set = set()\n",
    "    http_set_s = set()\n",
    "    http_set_m = set()\n",
    "    http_set_l = set()\n",
    "    \n",
    "    edge_set = set()\n",
    "    string_set_s = set()\n",
    "    string_set_m = set()\n",
    "    string_set_l = set()\n",
    "    \n",
    "    image_set = set()\n",
    "    num_set = set()\n",
    "    poly_set = set()\n",
    "    datetime_set = set()\n",
    "    date_set = set()\n",
    "    year_set = set()\n",
    "    point_set = set()\n",
    "    \n",
    "    text_edge_set_s = set()\n",
    "    text_edge_set_m = set()\n",
    "    text_edge_set_l = set()\n",
    "    \n",
    "    image_edge_set = set()\n",
    "    num_edge_set = set()\n",
    "    spatial_edge_set = set()\n",
    "    temporal_edge_set = set()\n",
    "    encoder_map = defaultdict(list)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    dtype_set = set()\n",
    "    def is_date(date_string):\n",
    "        try:\n",
    "            rdflib.term.parse_datetime(date_string)\n",
    "            return 'datetime'\n",
    "        except ValueError:\n",
    "            try:\n",
    "                rdflib.term.parse_xsd_gyear(date_string)\n",
    "                return 'year'\n",
    "            except:\n",
    "                try:\n",
    "                    rdflib.term.parse_xsd_date(date_string)\n",
    "                    return 'date'\n",
    "                except:\n",
    "                    return False\n",
    "\n",
    "    def add_str_to_set(string,edge):\n",
    "        if len(string) < 20:\n",
    "            string_set_s.add(string)\n",
    "            text_edge_set_s.add(edge)\n",
    "        elif len(string) < 50:\n",
    "            string_set_m.add(string)\n",
    "            text_edge_set_m.add(edge)\n",
    "        else:\n",
    "            string_set_l.add(string)\n",
    "            text_edge_set_l.add(edge)\n",
    "    \n",
    "    \n",
    "    for s,p,o in graph:\n",
    "        i+=1\n",
    "        pi = p.identifier\n",
    "        if s.identifier in subj_set:\n",
    "            pass\n",
    "        for node in [s,o]:\n",
    "            ni = node.identifier\n",
    "            node_set.add(ni)\n",
    "            try:\n",
    "                dtype = node.datatype.identifier\n",
    "                dtype_set.add(dtype)\n",
    "            except AttributeError:\n",
    "                dtype = ''\n",
    "            if 'http' in ni[:200]: #200, because images sometimes have kgbench url attached\n",
    "                if len(ni) < 20:\n",
    "                    http_set_s.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "                elif len(ni) < 50:\n",
    "                    http_set_m.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "                else:\n",
    "                    http_set_l.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "    \n",
    "            else:\n",
    "                if is_date(ni) and ('Year' in dtype or 'date' in dtype):\n",
    "                    date_type = is_date(ni)\n",
    "                    if date_type == 'datetime':\n",
    "                        datetime_set.add(ni)\n",
    "                    elif date_type == 'year':\n",
    "                        year_set.add(ni)\n",
    "                    elif date_type == 'date':\n",
    "                        date_set.add(ni)\n",
    "                        \n",
    "                elif node.isalnum():\n",
    "                    if node.isnumeric():\n",
    "                        if node.isdigit():\n",
    "                            num_set.add(int(node.identifier))\n",
    "                            num_edge_set.add(pi)\n",
    "                        else:\n",
    "                            num_set.add(float(node.identifier))\n",
    "                elif ni.startswith('POINT') or ni.startswith('Point'): #didn't see any points, but according to the paper they can be included.\n",
    "                    point_set.add(ni)\n",
    "                    spatial_edge_set.add(pi)\n",
    "                elif node.isalpha(): #maybe elif maybe not dunno if it filters out strings with numbers\n",
    "                    add_str_to_set(ni.pi)\n",
    "                elif ni.startswith('_9j_'):\n",
    "                    image_set.add(ni) #might want to load this to hard drive if memory becomes an issue.\n",
    "                    image_edge_set.add(pi)\n",
    "                elif ni.startswith('POLYGON') or ni.startswith('Polygon'):\n",
    "                    poly_set.add(ni)\n",
    "                    spatial_edge_set.add(pi)                \n",
    "                    temporal_edge_set.add(pi)\n",
    "                elif ni.lower().startswith('multipolygon'):\n",
    "                    poly_set.add(ni)\n",
    "                elif ni.isascii():\n",
    "                    add_str_to_set(ni,pi)\n",
    "                elif ni.isprintable():\n",
    "                    add_str_to_set(ascii(ni),pi) #don't know if it's necessary, but it probably can't hurt\n",
    "                else:\n",
    "                    add_str_to_set(ascii(ni),pi)\n",
    "    \n",
    "                    \n",
    "    \n",
    "        edge_set.add(pi)\n",
    "\n",
    "    \n",
    "    transform_temp = trv.Compose([\n",
    "        trv.Resize((224, 224)),\n",
    "        trv.ToTensor(), trv.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], \n",
    "            std=[0.3, 0.3, 0.3])]) \n",
    "    \n",
    "\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    node_map = {i:node for i,node in enumerate(node_set)}\n",
    "    inv_node_map = {node:i for i,node in enumerate(node_set)}\n",
    "    \n",
    "    edge_map = {i:edge for i,edge in enumerate(edge_set)}\n",
    "    inv_edge_map = {edge:i for i,edge in enumerate(edge_set)}\n",
    "\n",
    "    # node_map = {i: node for i, node in enumerate(node_set)}\n",
    "    # inv_node_map = {node: i for i, node in enumerate(node_set)}\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    global_mean_x = 0\n",
    "    global_mean_y = 0\n",
    "    \n",
    "    max_x = 0\n",
    "    max_y = 0\n",
    "    \n",
    "    \n",
    "    points_tupled = []\n",
    "    point_ids = []\n",
    "    for i, point in enumerate(point_set):\n",
    "        point_tupled,max_x,max_y = process_point(point,max_x,max_y)\n",
    "        points_tupled.append(point_tupled)\n",
    "        point_ids.append(inv_node_map[point])\n",
    "    \n",
    "    \n",
    "    polys_tupled = []\n",
    "    poly_ids = []\n",
    "    x_max, y_max = 0,0\n",
    "    done = False\n",
    "    multipolys_tupled = []\n",
    "    for i, poly in enumerate(poly_set):\n",
    "        if 'multi' in poly.lower():\n",
    "            multipolys_tupled.append(process_multipoly(poly, x_max, y_max))\n",
    "            \n",
    "        poly_tupled, x_mean, y_mean, x_max, y_max = get_num_data(poly, \n",
    "                                                                 x_max, y_max)\n",
    "        global_mean_x += 1\n",
    "        global_mean_y += 1\n",
    "        polys_tupled.append(poly_tupled)\n",
    "        poly_ids.append(inv_node_map[poly])\n",
    "    i += 1\n",
    "    # print(poly_tupled[0])\n",
    "    global_mean_x, global_mean_y = global_mean_x/i, global_mean_y/i\n",
    "    \n",
    "    \n",
    "    character_map = {char:i for i,char in enumerate(printable)}\n",
    "    character_map['\\x7f'] = 101\n",
    "    \n",
    "    # node_set.update(string_set_s)\n",
    "    # node_set.update(string_set_m)\n",
    "    # node_set.update(string_set_l)\n",
    "    \n",
    "    text_dataset_s = TextDataset(list(string_set_s), character_map)\n",
    "    text_dataset_m = TextDataset(list(string_set_m), character_map)\n",
    "    text_dataset_l = TextDataset(list(string_set_l), character_map)\n",
    "\n",
    "    image_dataset = ImageDataset(list(image_set), transform_temp)\n",
    "    \n",
    "    numerical_dataset = NumericalDataset(list(num_set), log_scale=True)\n",
    "    \n",
    "    spatial_dataset = SpatialDataset(polys_tupled + points_tupled, global_mean_x, global_mean_y, x_max, y_max, ids=point_ids + poly_ids)\n",
    "    \n",
    "    datetime_dataset = DateTimeDataset(list(datetime_set)) #if possible combine\n",
    "    year_dataset = YearDataset(list(year_set))\n",
    "    return text_dataset_s, text_dataset_m, text_dataset_l, image_dataset , numerical_dataset, spatial_dataset, datetime_dataset, year_dataset, node_map, edge_map, inv_node_map, inv_edge_map\n",
    "        \n",
    "text_dataset_s, text_dataset_m, text_dataset_l, image_dataset , numerical_dataset, spatial_dataset, datetime_dataset, year_dataset,node_map, edge_map, inv_node_map, inv_edge_map = preprocess(graph)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff725b0-c7f0-43f1-835c-3be64d6d58df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.preprocess.<locals>.TextDataset at 0x7c1ce53f39b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset_s\n",
    "# subj_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbbe6c-d039-4170-a6a1-7dedf5c5ec92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c59648f-28b8-4c11-ad6b-9d8b360b53d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1f2dbe-df3b-421e-a165-16b8e26b118c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43bcd61d-de84-4129-8a11-2f183f83955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/royal-cookings/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c91c6-0465-4d80-a988-2ca665ac25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch CUDA available:\", torch.cuda.is_available())  \n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())  \n",
    "print(\"CUDA version:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())  \n",
    "print(\"Current CUDA device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddc8a4-7f94-4e7d-91be-63fb572b0052",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d96ed9b3-45b0-4529-9e21-8836d09db813",
   "metadata": {},
   "source": [
    "# Grab train, valid, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a655d2b-46ab-4a31-b3eb-51fe0091f55b",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b230ef5-257c-4d47-a930-c0aa1a152158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de1da1a0-8557-4339-8268-3dce458cb5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('graph_test.pkl', 'wb' ) as f:\n",
    "    pickle.dump(graph_test,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df78dfac-61da-4cd3-82d7-39253078eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('graph_train.pkl', 'wb' ) as f:\n",
    "    pickle.dump(graph_train,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a09d10c-29a6-46c9-905b-5363731c689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('graph_val.pkl', 'wb' ) as f:\n",
    "    pickle.dump(graph_val,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab0a94-1c17-4fb4-9e9f-ef3a746b1e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09675e30-8e0a-484b-8949-c2ca0f0521f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[1;32m     27\u001b[0m         img_file\u001b[38;5;241m.\u001b[39mwrite(image_bytes)\n\u001b[0;32m---> 30\u001b[0m image, image_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mencode_image\u001b[49m(byte_str, transform_temp)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_image' is not defined"
     ]
    }
   ],
   "source": [
    "# import base64\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "\n",
    "# byte_str = \"\"\"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\n",
    "# https://data.labs.pdok.nl/rce/id/image/20324688 http://dbpedia.org/ontology/thumbnail _9j_4AAQSkZJRgABAQAAAQABAAD_2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL_2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL_wAARCADIAIIDASIAAhEBAxEB_8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL_8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4-Tl5ufo6erx8vP09fb3-Pn6_8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL_8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3-Pn6_9oADAMBAAIRAxEAPwD3FEwAD1pWjXHQVI4A5HWoyw6UAIqD0GRUyog5CgH1xUasM0_eMUAI6K4wyg_Wo1tYgPu5J9TT9-TTwc9KAK7QoGIAwDxTwFUBQOKkwST6U3ABoAjliVkwOD2qt5DknI4q8Oa5_wAT-IY9FtCiENdyDCJ6D1NAGL4u8QjTYGs7Vh9rcYJBzsB7_Wue8Eau1pfvYTufLuDuQt2f_wCuP1ArDmeS5neWZmeRzliTyc96fcSmWWGVVCvGoGV4JIyc_wAvyoA9cxTWqloOprq2lxzEgzKAsgHqO_49a0CtAEBFAXkE-tSFRnmpIkBOT0FADfLPoaKtbB6UUAX24UkmoAfmqVs4NNGDyRzQAxiAeKaXOetDkbqYTQAomHmlOdwUHOOMHI_pU6ydKy0nRtUnhAG-OKNmPf5i3f8A4DVwE0AWi9NDA1EDWNq3iKDSr6O3lzl03AgZA5xyaALmta1Do1kZXIaVuI0zyx_wryu-up768a5uD5kjnJOeB7D0q3qt_Lq2pPNI4ZQSEA6AVTRCJCGOADjpxQARPaxSFrmFnQA5VDg59RyOlS3UWlyRb7OabzMBvKlXsevPTjND20bxusrkIc4IGSPwqJ7aJFBjuUdVXBUghhn2P0oA0PDOo_2XqYVyRBOQrDPQ9j7V6QVyMgjB6Yrx58odygk9RXovhXVv7T0sRuf9IgG1gT1GODQBrkYpyvjtSsOTj1phGDQBLv8AeiouaKANQuCKj3EA4qEOxPFLlsZwcUAOYkmmcnik3c0uc96AOe06Yv411hQcqYYlA_3Qc_qxroxXH6BL5njDUW4xIJCPoGUD9K6q7uorG2aeZgqqMnPf2oAqa9qjaRpE11EqPOoHlRscBjkZHUdj_L1rzZr-bWZZL67eOOVl3LEWwxGOcDvUPiTXbrWdQUIMxB-ASCFUEEk-5yQPrmn2EELxQy3N15aALHwMlVCjnH-etADbdITDMZCQ-P3Z56_hUYDYwGIJA61PcW0UUzC2uVnjGMOBjI-lNiVsgEkL1JI7UASqG8vDEHtwKR4cZLKcEdaeXJbZEQoPcjJ_-tUU0k4IaZ_MQALuAwVwMD-VAEThcH0HtVrRNSbStWjmBJjYhXUdwf69D-FVZyQBggg9D61FII1t0K5EwYk8nHsaAPXg6SruRgwPORSEc1zfgiWafTrgzPI4DgrvJIAI7A9PwrpiDQA3FFPxRQAJOiziIn5ypYDHYED-oqwTkVyXizVrrRjBPabN5YKwdcgggk_qBWRB491EACW0t3HfGQf5mgD0Bl5z60ydjHbSuOCqE5-gJrkofHyEATaey-6Sg_oR_Wp5_G-nyWUyrDOJWQhVcDaSRjkjNAHN-FL9YvEcEjzjbIsvmEg9ME55HqB-VR-LPEc-q3H2W0BWEZwewHcn-grCjgFsW8rYbplAJycRqARk_mc9M_TObCWCz6cEtXkllZjuCqckg4P54NAEAt4YtOR4ZtzswJIPJ-Yd_ersSg2kZIOc9cegAqeLSEiso4bmZY9oAKINzcHp1wPz_CnSLFH5aQoyRjgZbJPueAM_QCgCvEuCxI4B6A9atZJUKOQeT_hUcIGCQAQDzxVhMCTAAwBnp7_zoAswQQOphLDfIvBzgA9h9c1XvLKeBSJ4yMA4AI5x1P0qSQw7WJA8zggY55681HJK11eqbp2CGPCkHABA6d-o_lQBQaIrHsyMHleenqKi2B5ER3CAsAXPQcjk1cn25jZCSucYPWqt2MxkgDBBHFAHpHhW3SDRUVZUlO4guhyCB0rZIwa5zwEAdClAGMTnjrjgV05Q0AR4op-2igDhPHLrJKsYIyrIcZ68NzXLrEDgj0rfvydTu577a6RqmcZAIUE8EEHnn8KxpLhIzkmEKThTIuCT2GaAGpbtKxWMAkEAD1JIGP1qaTS7yAgtAV5wC5AA9STSRkySoAEUlgFMRyWOQQAPTuT2HuQDYnS5KmBy4JwSHyDx7HmgCCOGytoJYipuJJANzZwM4OeepHOKkS4cRiKMLFHyAiDAI9-5_GqrQujEOxHepUQlgQSQKAJEQliSc47USg7o-gAJwKsJEAec5NR3AUGMA5O7GOlAC2ZwrEAHJPapG2-btUAEg4z9aZageUSOMseD9acQxugQMkqTx9aAElRngBXBkGcgHAxUAiLMglB2EjkAcHtmrCA5YFSMdhTST5ZOTgd8UAQXCAuEGQAfTt7VXuogISQQQKtS8mPJyeuc1FdAeQ5yOBQBueFdeXSAba5X_R5WDBx1UkAcjuOK9BjkjniWSJwyMMgg5BryLZmBDgnAGDWpouvXOkTbTuktiRujJ6e49DQB6ViistPFOklFPnNyPSigDymWdbmeTyw6BmBwGOADgkficn8aW6gD28YLkBW3YzyfYZ_nVK0LYG_gkAA9zg8Z-oIrWsoIk3TXBLuHICZyR6D2GcUAO0OwL3k0-zywWByuRjAB4PX0-pPNdNeWkA05btLtmugACPMyRzzg9ax4ZQ8QUgALkADjk5Ofrzj8BVOaV8RqowC2Rxzg0AaAub4glb6YqTxuCtx-INU77X20-VIp5Ecv3Nuhx6E8D9KtqcADPt7Vxnid2bX4QArAKvXHHNAHaLczlQfJs2BAPEJGR-DUzZJe3cNskEMchJwVY4PtyeKW3lP2ePGMbQP0qG4mZZ4njYqwYEEDkHNAE72ktjK0EwG8HJAORzz1qFiROpGeFIqxHcyzM7zyM7gkAtyeOlTJA9xMXjhdwFwSoyAaAKSSMC5GQDURci3Knvk1p_Y5gz7reQD12Gq5tmEDFoXBGeqmgClKcmIAdgDx7VFdoDA_GMDNW7hEiSNnBUDGSeAOKpX2oWsUDGOVHccgD5gSOe30oAtQQSvaoVQsAB0pkltcA5ETj8OlQ3uvaZcukqwvA4UBgo4JHcdKz5NegXlFuDj_AGiP60Aaeyb-63_fJorK_wCElH924_7-_wD1qKALUTxNexrtdcnchxgEYXBxj2PFW7VTJLMSSSZSSSeetZ1mf9NgYEABccdhhsfyq_Yjd5jYyTKRn8TQBYUqpKhg2CMbSCDwOQR1HHX3ps5AEBAxk8nHtT8AM6joHxj04qObpbjoQeKALSgEDBxnnr_9aqF_4XsdTvY72XU2hkVQCn2csOD6jrV5QOCMk1IBnqKADykhUIkgZFGA54BHrg1XlAaeJQRjI5z71l-KgRpqEMwO44wcdqzfCe5YZCzMx80HJJOOlAHSX88tjp9zcQpE8ykBBICVySOwIJ4z3qPw9qd5qcE5nWGJ1YbTBuXjnrknOeKh1tydLuF3hQXHJ6DkVW8Ms0UE4DhuRkg8d_6UAdSlyImKSXVwXUZYCQjA9etC6qvmrEtxc7nXcoM3UHoep9DXnfi-9uIL-MwyuhZQGw2Mj0NVVv4gHLGTdAoBOTnBOKAPQtW1kPpc5MskkZPlkFwec46Y_rXGtt-YADB6c4rmo7yU6oI1kbyXkyUPQ5710BcYOKAEYgKQDk-hHFViAepPPapGPOM_Wm5HX0oAbsHoKKdn_OKKANRbu7tLnIVJArZZSM8EnH8zW5pU9lPCC0jW7mTJyMqSSeB3FVobOe7BmW2IflSOhOMjn_Peon0S_WRJoIWR85IB4JoA35dNuI2kcJ5gMhAMZDAcDGcHIz71XktLl2hAtpjgnkRmq1omt2kUiokis7hmIYjP5Vau7zVnAjEtwgJAJLkdj05oAuJYXeM_Zpsf7hrkPEniWfRdQNsQE46OMEdPX61ufbdTRAGnmyc4XzjzjqaydT0q31W4W4vLOO4uCoyzuenvyKAOZu_Fg1KPyblwFJ42gZB9qbZa-mmLIiOMltwDDnHarOu6FY2ot5La0jgKgk7CTnPTqTTtF0aC9jE01rDN8wUmTqBj0_GgAbxKdVtJ7ZwpBAbI4III_wAa2_DJBtZSvcjnpk85_lVu10iHTj5lpY6coZQH3oCDg55BFXC8s2UtxaRqMcLGFye54HI5oA4vxmMalbjGQQKrGJGa7KgjcF_ABq6jWvCjamy3L30SOmAABkE1S1Cym0u0aUSQozZAZSGJODwAfYGgDjI3U6spA6ODz6YrolkycHtXIWyTTXjTyMW2tkkd-a6K3kZzkUAX-uDjikJwOhxTcsCOh9qUnjBzQA7cPX9KKblaKAO08L6sl5JNG42nIbGeeQB_MH866XCEYU5P1rza1uzaTRyQoqELgHkjtV2XxLqRbAuVUdchQf6UAd0yEkYOMg9Cev8AjWHq8WpowktWDquSQV56Vzra7qL4BvXyQORjAz7iom1e-wVa7kbkcgmgCGXUtSTAZ1V4ycAjoD2zVX7bqLoDvYk-g7Z6VbkuZXJYyMxOSSTn_PSkMvUKWPXJzigCrdtdT6ezXBYkHCk-lS6XpuoXdorWYcruOSGwAfrT7gySadMXlLKpJUHGFyBUNtaPfeXAuvPp6FQBCqg7jzk59f8ACgBmopq-nEi6L7CQAQ-QfyNZj6lcnK-c3UYOSMV1UVloWkwFLzXmnGcv5jA7j7gg1m3eqeBwSkazTSN0MOcg-2TigDEN3cOcNMxGPU0rTzOoDElSOh5qzNpccy-bYOzKRlUZgXH1Axn8KoeTdgMpjfC8HKkAfmKAAJEDnaAc9hjFIysDlWIx0xxTRIBkEnOfwp5KnBDcfWgB_wBquo8fMGwO4zT01GUYMkSse2DjFMY4YHaME_pTvlIJ6Ee1AE_9pr_zwP50VW2j1ooA2WJyGBwo4Hv-FRliELA4JPA9fwpuGUgoCCQCCACSP_retMZyCEAZieSO_vQBIJcggnAJOAO5wO341ds9Lvr6LfDGCnQknBrPwowQoBxznrxzz-dRRavfaVcma3nJUkgo4OGH07UAdInhvUT_AHE4xndSy-Hr6BQ4US8HIQ5Irlbnx5rIkKhmJIBOFOASAcDGOnSnaJ4l1nVNat4Z3mELP83UZA59aAN6ezuINLKso3MckdCB_jVP7LE8CSsAcHHB5B7c9utdD4gu4_KSFWUZALEn8hXMpcxI0kRlXaQTnPGf_rGgCCXTLCSRpHtQzdckZP61raXB4eEgikiWOboAQADj3xVFXdjgAs-B90Zz78VIdKmuUH-iSkg5BKnGPb079-9AG9LqnhzTfvPbKV7AAkfnWXefErSIQyQo056EAYFYU2gWxnP2m0IcHkSHt-PrUlrpOmiZVMSQ57sMj60ASSalaayvmNaRWhY5VkXOc-oH-FVJdGuFRnhAmB6GPnH4da6-28OWIVZC5kGMApgA_lWpbWVpbEGGBVI4DYyfzoA8ya3uo4g_lSFBnJZDgUxLklTlScDHB6V6vLLFEhMzRIncuQB-tctq8vhicmMgNMxwPs4wSf5GgDlvPH-TRVz-yIn-ZZE2nkZnjzj86KALDSsFZcgFfUdcd_T0qe202-njRxbOVkyc44x685qh5-7AALMDk4xz_hXY-Gb8XenrE4AkhJU_TjFAGOnhy_clmVQc5ALgAfgPqelTL4UncgvLApHbk5966onBIIwDQTkYNAHB6po39nygyhWBBKsBgEcf1q14aEUmvRAKuWJJ6-nT-tdRfW6Xdq0UyBlJGPUe4rndLK6HqxSdFHmMTFNj3ORQBF4js2GvzrvHl7VGO54rKgtogC7IGYHnJznFbmuzGfWZZdhwVXHIAyAB1JrKkkjiLM8kaqThhuyceowKAN3TvFdhcIkTMInHBBGPzH-FbSyiRQQxIPQg5FeTX9qnmu0Eu8KchhwRnpxWjoniC9sJkgnkEkfA3g8D2PvzQB6VJDDcx7JolkX0YZx9KxNR8LQ3KH7JII2AOEc8E5z1HNc_c_EEJuWJU4JAwCScfU1lzePb2WMGN2XJIwAByAP8aANQprWhOGUsY-CQSSpHsaq3_ifxFcsVt0WFT0xwT-XP5EVTi1me7UtLLI2AMgsSBVwSjaoUsRnGe2KAMKew1i7k3Xt84J9M5_U5qay0IW0nmmV3YnIJ9vp_nmt3JkUMSAAcMOo59KFAI3qxCk4GBx7nmgCn9k_2n_76P-FFWPtco43t-YooAikOxiyFQAcABeQB0z2yef8AJrQ8NX7Wmrlp2YJKh3E5I45zz7f4VUWeMRFCgZyQSNxHBHTHtWXKzITIoIYkkAZ9wKAPQZvFenLIQFkcAdVXGTn3xWbceO7OEnbCM9i8oU_kM157qBuJ5iFRgMKMcjOBjOPfGapf2VNKSz5GfbpQB3U_xBdyPKEIAP8ACpY_mSBWfPr9zqseDKSAd2NoXGT7ZrnItEmwmX4JwR71p2tmEDDcVx3J9-aALJLOS5ZndjnJPT-XXHpTzbkwlmAAOASSOOBx04_CrdvEpj-4pIIGM8nn_wCvUgjEjuu5SC2cdR0HrQBUSBZFjBViDzkDOfoe9R3Fovm8FVLKeoIBIHsME5B4AFXreBwdyHDR5XAOMjp_I0k6ANuUgkE5BPQd_wCdAGLJosAIf7xORtBIIx1GMU6DTVUiN4ioUkAquckjPOfpW-0Kzq4TLELkBAc5_Ciz0i7mYSLAFUvglztAIAzkn3zQBmQ2QQgsAoIyo6dO5q6AsSghxnHBwCCRnoe3_wBarcumWcU6G61K3jBTdiP5yORwPfmq9zdaRFGEt47q4lJ4LnCkZ6juD_KgBsaMY2TcGLAEk4BB6ke31GKuWelXckKPDauyAljlQFx1HzGov-EjngtyltaWtvhcfKm5umM5NZ1zqF5cqFnvJHwudhYhT2A44oA3ovD0vlJzb_dH_LYf40VymU_54v8An_8AXooAvqJTGQIYuCf4skA9xj0xj8KYEjSUFQrcYJOQPzP-BoooAJgouDgKobkjGVBz9OB3-tJsQxbAAzdz2B9MCiigBOpAIDKepORg446_54pYEXzHiYAAHJPcA9MD09_cUUUAaEdlJLEDFHJKSdo8pSeOxz0B-pqz_Yl6DMXEUaoASZnC8kHgY9gKKKAK9u-lW1w_2rUVKjokCknpjg8jt60y51fSYgVtNMuJyWDb5mxu9RgZoooAiHi3UVUxwrb2iAdI1BYAdOTk5rJuNTnnLuZ5JgWLFSxGSc5P1NFFAEUUpkAYxmMkZAB4x-FWnlbAYMAMYwTySaKKAIYovODBmcYySTgn271NJaKxwsoIGCAXzg_QgUUUAR7X_wCfmP8AMUUUUAf_2Q==\n",
    "# https://data.labs.pdok.nl/rce/id/image/20636163 http://dbpedia.org/ontology/thumbnail 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\n",
    "# \"\"\"\n",
    "\n",
    "# def decode_base64_image(encoded_str,log_note='pass values to decode_base_64_jpg'): # - to +, _ to /\n",
    "#     \"\"\"\n",
    "#     encoded_str: url safe base 64 jpg string --> image bytes string\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         image_bytes = base64.urlsafe_b64decode(encoded_str)\n",
    "#         image_obj = Image.open(BytesIO(image_bytes))\n",
    "#         image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "#         return image, image_obj\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"{e} error encoding image at {log_note}\")\n",
    "#         print(e)\n",
    "#         return None, None\n",
    "\n",
    "# def save_bytes_to_jpg(image_bytes, item_num=0,folder='Downloads/ml4g/image_data/',name='Fred'):\n",
    "#     filename = f'{folder}{name}_{item_num}.jpg'\n",
    "#     with open(filename, 'wb') as img_file:\n",
    "#         img_file.write(image_bytes)\n",
    "\n",
    "\n",
    "# image, image_bytes = encode_image(byte_str, transform_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acb8ed-bac7-4450-84a5-dedbc8aef71c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09cd4067-852c-4807-bc0b-417be80a827f",
   "metadata": {},
   "source": [
    "# Convert raw values to consistent feature vectors for the encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34660fba-188d-4dfd-902e-b9c2173915e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_spatial(batch):\n",
    "    nodes_ids, batch = zip(*batch)\n",
    "    padded = nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "    padded = padded.permute(0, 2, 1)\n",
    "    return nodes_ids, padded\n",
    "\n",
    "def collate_text(batch, min_size=20):\n",
    "    if batch[0].size(0) < min_size:\n",
    "        pad_num = min_size - batch[0].size(0)\n",
    "        pads = torch.zeros(pad_num, dtype=torch.long, device=device)\n",
    "        batch[0] = torch.cat([batch[0], pads], dim=0)\n",
    "    return nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "spatial_dataloader = DataLoader(spatial_dataset, batch_size=batch_size, collate_fn=collate_spatial)\n",
    "text_dataloader_s = DataLoader(text_dataset_s, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_m = DataLoader(text_dataset_m, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_l = DataLoader(text_dataset_l, batch_size=1, collate_fn=collate_text) #it crashes every time it starts :'(\n",
    "image_dataloader = DataLoader(image_dataset, batch_size=1)\n",
    "datetime_dataloader = DataLoader(datetime_dataset, batch_size=batch_size)\n",
    "year_dataloader = DataLoader(year_dataset, batch_size=batch_size)\n",
    "numerical_dataloader = DataLoader(numerical_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fa37dd0-337b-45ea-8ad0-50b9be965bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del polys_tupled, points_tupled,node_set, http_set, edge_set , string_set, string_set, string_set,image_set ,num_set ,poly_set ,datetime_set ,date_set ,year_set ,point_set ,text_edge_set ,text_edge_set ,text_edge_set ,image_edge_set ,num_edge_set ,spatial_edge_set ,temporal_edge_set ,encoder_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e1e85-5dd0-491e-b949-60ca053ce7f1",
   "metadata": {},
   "source": [
    "# Multimodal Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7198e43b-a426-4ee6-8d74-6ea1b2652130",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import torch.functional as f\n",
    "import torch.nn as nn\n",
    "\n",
    "# ]1] temporal conv\n",
    "# Layer Filters Kernel Padding Pool\n",
    "# 1 64 7 3 max(2/2)\n",
    "# 2 64 7 3 max(2/2)\n",
    "# 3 64 7 3 -\n",
    "# 4 64 7 2 max(·)\n",
    "# Layer Dimensions\n",
    "# 5 512\n",
    "# 6 128\n",
    "# 7 128\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=102, dropout=0.4, size_type='medium'): #filters = 64\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.size_type = size_type.lower()\n",
    "        self.mlp_hidden_dim = 1024 if self.size_type == 'large' else 512 if self.size_type == 'medium' else 256\n",
    "        self.tcnn_hidden_dim = 128 if self.size_type == 'large' else 64 if self.size_type == 'medium' else 32\n",
    "        self.dilation_vals = (1,1,1) if self.size_type == 'small' else (2,4,8)\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) #used embedding dict instead of ohe to save space\n",
    "                                                            #sparse should probably work too\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            embed_dim, self.tcnn_hidden_dim , kernel_size=7, padding=3, dilation=1)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.pool1 = nn.MaxPool1d(2,2)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.pool2 = nn.MaxPool1d(2,2)\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm3 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        self.conv4 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=2)#, dilation=self.dilation_vals[2])\n",
    "        \n",
    "        self.norm4 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop4 = nn.Dropout(dropout)\n",
    "        self.pool4 = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.lin1 = nn.Linear(self.tcnn_hidden_dim, self.mlp_hidden_dim)\n",
    "        self.lin2 = nn.Linear(self.mlp_hidden_dim, embed_dim)\n",
    "        self.lin3 = nn.Linear(embed_dim , embed_dim)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        residual = embedded\n",
    "        #conv\n",
    "        \n",
    "        x = self.conv1(embedded)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop4(x)\n",
    "        # if residual.size(1) != x.size(1):\n",
    "        #   residual = nn.utils.rnn.pad_sequence([residual, x], batch_first=True)\n",
    "        x = self.pool4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #ffnn\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        # x += residual\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "# [2] MobileNet \n",
    "# Type / Stride Filter Shape Input Size\n",
    "# Conv / s2 3 × 3 × 3 × 32 224 × 224 × 3\n",
    "# Conv dw / s1 3 × 3 × 32 dw 112 × 112 × 32\n",
    "# Conv / s1 1 × 1 × 32 × 64 112 × 112 × 32\n",
    "# Conv dw / s2 3 × 3 × 64 dw 112 × 112 × 64\n",
    "# Conv / s1 1 × 1 × 64 × 128 56 × 56 × 64\n",
    "# Conv dw / s1 3 × 3 × 128 dw 56 × 56 × 128\n",
    "# Conv / s1 1 × 1 × 128 × 128 56 × 56 × 128\n",
    "# Conv dw / s2 3 × 3 × 128 dw 56 × 56 × 128\n",
    "# Conv / s1 1 × 1 × 128 × 256 28 × 28 × 128\n",
    "# Conv dw / s1 3 × 3 × 256 dw 28 × 28 × 256\n",
    "# Conv / s1 1 × 1 × 256 × 256 28 × 28 × 256\n",
    "# Conv dw / s2 3 × 3 × 256 dw 28 × 28 × 256\n",
    "# Conv / s1 1 × 1 × 256 × 512 14 × 14 × 256\n",
    "# 5×\n",
    "# Conv dw / s1 3 × 3 × 512 dw 14 × 14 × 512\n",
    "# Conv / s1 1 × 1 × 512 × 512 14 × 14 × 512\n",
    "# Conv dw / s2 3 × 3 × 512 dw 14 × 14 × 512\n",
    "# Conv / s1 1 × 1 × 512 × 1024 7 × 7 × 512\n",
    "# Conv dw / s2 3 × 3 × 1024 dw 7 × 7 × 1024\n",
    "# Conv / s1 1 × 1 × 1024 × 1024 7 × 7 × 1024\n",
    "# Avg Pool / s1 Pool 7 × 7 7 × 7 × 1024\n",
    "# FC / s1 1024 × 1000 1 × 1 × 1024\n",
    "# Softmax / s1 Classifier 1 × 1 × 1000\n",
    "# Table 2. Resource Per Layer Type\n",
    "# Type Mult-Adds Parameters\n",
    "# Conv 1 × 1 94.86% 74.59%\n",
    "# Conv DW 3 × 3 3.06% 1.06%\n",
    "# Conv 3 × 3 1.19% 0.02%\n",
    "# Fully Connected 0.18% 24.33%\n",
    "\n",
    "#idea: pass identity vector with batches to encoders (don't process, just pass back), use that to map embeddings to nodes later\n",
    "class ImageEncoder(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 256, stride=2, alpha=0.5)\n",
    "        self.image6 = MobileBlock(256, 512, stride=1, alpha=0.5)\n",
    "        self.image7 = MobileBlock(512, 512, stride=2, alpha=0.5)\n",
    "        self.image8 = MobileBlock(512, 1024, stride=1, alpha=0.5)\n",
    "        self.middle_conv = nn.Sequential(*[MobileBlock(\n",
    "                1024, 1024, stride=1, alpha=0.5) for i in range(5)])\n",
    "        \n",
    "        self.image9 = MobileBlock(1024, 1024, stride=1, alpha=0.5)\n",
    "        \n",
    "        self.image10 = MobileBlock(1024, 2048, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(1024,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        # middle_conv = [MobileBlock() for i in range(5)]\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.image(batch)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.image6(x)\n",
    "        x = self.image7(x)\n",
    "        x = self.image8(x)\n",
    "        x = self.middle_conv(x)\n",
    "        x = self.image9(x)\n",
    "        x = self.image10(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class MiniImgEnc(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(MiniImgEnc, self).__init__()\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 512, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(256,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image(x)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        # x = self.lin2(x)\n",
    "        # x = self.norm(x)\n",
    "        return self.act(x)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class MobileBlock(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_size, stride=1, normal_conv=False, alpha=0.5, dilation=1):\n",
    "        super(MobileBlock, self).__init__()\n",
    "        old_in_channels = in_channels\n",
    "        in_channels = int(alpha * in_channels)\n",
    "        embedding_size = int(alpha * embedding_size)\n",
    "        # depthwise conv\n",
    "        self.normal_conv = normal_conv\n",
    "        if self.normal_conv:\n",
    "            # in_channels = in_channels//2\n",
    "            pass\n",
    "        # 3x3x3x32\n",
    "        self.conv = nn.Conv2d(old_in_channels, in_channels, 3, padding=1, stride=2, dilation=1)\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, 3, stride=stride,\n",
    "                                   padding=1, groups=in_channels)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        # pointwise conv\n",
    "        self.pointwise = nn.Conv2d(in_channels, embedding_size, 1, stride=1, padding=0)\n",
    "        self.norm2 = nn.BatchNorm2d(embedding_size)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(self.conv.kernel_size)\n",
    "        if self.normal_conv:\n",
    "            x = self.conv(x)\n",
    "        # print(x.size())\n",
    "        x = self.depthwise(x)\n",
    "        # print(x.size())\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.pointwise(x)\n",
    "        # print(x.size())\n",
    "        x = self.norm2(x)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [3] temporal conv\n",
    "# layer filters kernel padding pool\n",
    "# 1 16 5 2 max(3/3)\n",
    "# 2 32 5 2 -\n",
    "# 3 64 5 2 avg(·)\n",
    "# layer dimensions\n",
    "# 4 512\n",
    "# 5 128\n",
    "# 6 128\n",
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128, dropout=0.2):\n",
    "        super(SpatialEncoder, self).__init__()\n",
    "        \n",
    "        # temp cnn\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=3, ceil_mode=True)\n",
    "        self.norm1 = nn.BatchNorm1d(16)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm1d(32)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        \n",
    "        # dense\n",
    "        self.lin1 = nn.Linear(64, 512)\n",
    "        self.lin2 = nn.Linear(512, 128)\n",
    "        self.lin3 = nn.Linear(128, embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, spatial):\n",
    "        # x = self.pad(spatial, batch_first=True)\n",
    "        x = self.conv1(spatial)\n",
    "        x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.drop1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #flatten\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# [4] mlp h == in dim 1st col, out 2nd col\n",
    "# XSD:gYear 6 2\n",
    "# XSD:date 10 4\n",
    "# XSD:dateTime 14 6\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super(TemporalEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = input_dim\n",
    "        self.out_dim = 2 if input_dim <= 6 else 4 if input_dim <= 9 else 6 #I messed something up but its very specific...\n",
    "                                                                        #norm_cent returns a digit so idk if that should be 2?\n",
    "                                                                        #there's no sin/cos, so it seems weird to encode it digitwise\n",
    "        num_layers = 2\n",
    "        self.mlp = nn.Sequential(*[nn.Linear(input_dim, input_dim), \n",
    "                                   nn.ReLU(),nn.BatchNorm1d(input_dim)]*num_layers)\n",
    "        self.out = nn.Linear(input_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "\n",
    "#[5] one to one encoding for numerical\n",
    "class NumericalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NumericalEncoder, self).__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8c0f3-7864-4598-a4e8-ecc714a2e6ae",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d64ee80-fe29-4dc5-95d7-26a40bc0d75b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46ff724b-ac91-443b-8c30-fa9f606d0359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder: text_s\n",
      "encoder 'text_s': processed 2\n",
      "encoder: text_m\n",
      "encoder 'text_m': processed 2\n",
      "encoder: text_l\n",
      "encoder 'text_l': processed 46\n",
      "encoder: image\n",
      "encoder 'image': processed 46\n",
      "magic has occurred stay spiffy\n",
      " torch.cat(): expected a non-empty list of Tensors\n",
      "encoder: numerical\n",
      "encoder 'numerical': processed 0\n",
      "encoder: spatial\n",
      "encoder 'spatial': processed 0\n",
      "encoder: datetime\n",
      "encoder 'datetime': processed 5\n",
      "encoder: year\n",
      "encoder 'year': processed 5\n",
      "magic has occurred stay spiffy\n",
      " torch.cat(): expected a non-empty list of Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76432/1891242362.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    }
   ],
   "source": [
    "text_encoder_s, text_encoder_m, text_encoder_l = TextEncoder(128, size_type='s'), TextEncoder(128, size_type='m'), TextEncoder(128, size_type='m')\n",
    "datetime_encoder = TemporalEncoder(9)\n",
    "spatial_encoder = SpatialEncoder(5)\n",
    "year_encoder = TemporalEncoder(5)\n",
    "image_encoder = ImageEncoder(128)\n",
    "numerical_encoder = NumericalEncoder()\n",
    "\n",
    "class MultiModalEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_matrix,\n",
    "                 encoders_config):\n",
    "        super(MultiModalEncoder, self).__init__()\n",
    "\n",
    "        # self.dataloaders = [config['dataloader'] for config in encoders_config]\n",
    "        self.encoders_config = encoders_config\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.column_map = {'text':(0,128),\n",
    "                           'image':(129,257),\n",
    "                           'numerical':(258,259),\n",
    "                           'spatial':(260, 388),\n",
    "                           'datetime':(389,393),\n",
    "                           'year':(394,396)}\n",
    "        self.x_indices = []#torch.zeros(8, device=device)\n",
    "        self.y_indices = []\n",
    "        self.values = []#torch.zeros((8,128), device=device)\n",
    "\n",
    "        self.num_nodes = feature_matrix.size()[0]\n",
    "\n",
    "    def forward(self):\n",
    "        for config in self.encoders_config:\n",
    "            self.x_indices = []\n",
    "            self.y_indices = []\n",
    "            self.values = []\n",
    "            name = config['name']\n",
    "            encoder = config['encoder'].to(device)\n",
    "            dataloader = config['dataloader']\n",
    "            print(f\"encoder: {name}\")\n",
    "            if 'image' not in name:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    if 'text' in name:\n",
    "                        cols = self.column_map['text']\n",
    "                        node_ids = batch[:, 0]\n",
    "                        features = batch[:, 1:]\n",
    "                    else:\n",
    "                        cols = self.column_map[name]\n",
    "                        node_ids, features = batch\n",
    "                        node_ids = torch.tensor(node_ids,device=device)\n",
    "                    # I confused myself. passing the node_ids through the forward pass was completely unnecessary.\n",
    "                    # change to sparse and use automatic column assignment base on thingy\n",
    "                    try:\n",
    "                        embeddings = encoder(features)\n",
    "                    except ValueError: #if batch size is 0 or 1 when it's supposed to be more.\n",
    "                        x,y = self.column_map[name]\n",
    "                        output_dim = y-x\n",
    "                        embeddings = torch.zeros(y-x, dtype=torch.float32,device=device).unsqueeze(0)\n",
    "                    # embeddings = embeddings.to(device)\n",
    "                    # self.feature_matrix[node_ids, cols[0]:cols[1]] = embeddings\n",
    "                    # sparse assignment\n",
    "\n",
    "                    # if embeddings.size(0) <= 1:\n",
    "                    #     embeddings = embeddings.unsqueeze(0)\n",
    "                    #     print('unsqueezed', embeddings.size())\n",
    "                    batch_size = node_ids.size(0)\n",
    "                    y_cols = torch.arange(cols[0], cols[1], device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "                    # print(y_cols.size())\n",
    "                    embedding_size = embeddings.size(1)\n",
    "                    x_rows = node_ids.unsqueeze(1).repeat(1, embedding_size)\n",
    "                    self.x_indices.append(x_rows)\n",
    "                    self.y_indices.append(y_cols)\n",
    "                    self.values.append(embeddings.view(-1))\n",
    "\n",
    "                    \n",
    "            print(f\"encoder '{name}': processed {i}\")\n",
    "\n",
    "            # del dataset, dataloader\n",
    "            try:\n",
    "                \n",
    "                x = torch.cat(self.x_indices).reshape(-1)\n",
    "    \n",
    "                # print(x.size())\n",
    "                y = torch.cat(self.y_indices).reshape(-1)\n",
    "                # print(y.size())\n",
    "                indices = torch.stack([x, y], dim=0)\n",
    "                # print(indices.size())\n",
    "                values = torch.cat(self.values)\n",
    "                # print(values.size())\n",
    "                self.feature_matrix = torch.sparse_coo_tensor(\n",
    "                    indices=indices,\n",
    "                    values=values,\n",
    "                    size=(self.num_nodes,396)\n",
    "                    , device='cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print('magic has occurred stay spiffy\\n', e)\n",
    "\n",
    "        return self.feature_matrix\n",
    "\n",
    "encoders_config = [{'name': 'text_s', 'encoder': text_encoder_s, 'dataloader': text_dataloader_s},\n",
    "                   {'name': 'text_m', 'encoder': text_encoder_m, 'dataloader': text_dataloader_m},\n",
    "                   {'name': 'text_l', 'encoder': text_encoder_l, 'dataloader': text_dataloader_l},\n",
    "                   {'name': 'image', 'encoder': image_encoder, 'dataloader': image_dataloader},\n",
    "                   {'name': 'numerical', 'encoder': numerical_encoder, 'dataloader': numerical_dataloader},\n",
    "                   {'name': 'spatial', 'encoder': spatial_encoder, 'dataloader': spatial_dataloader},\n",
    "                   {'name': 'datetime', 'encoder': datetime_encoder, 'dataloader': datetime_dataloader},\n",
    "                   {'name': 'year', 'encoder': year_encoder, 'dataloader': year_dataloader},]\n",
    "\n",
    "\n",
    "num_nodes = len(node_map)\n",
    "\n",
    "embed_size = 128 + 128 + 1 + 128 + 4 + 2\n",
    "\n",
    "feature_matrix = torch.zeros((num_nodes, embed_size), dtype=torch.float32, device='cpu').to_sparse()\n",
    "\n",
    "\n",
    "\n",
    "multi_modal_encoder = MultiModalEncoder(\n",
    "    feature_matrix=feature_matrix,\n",
    "    encoders_config=encoders_config).to(device)\n",
    "\n",
    "feature_matrix = multi_modal_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02434eaa-0c73-4afc-8d66-a9267ec9d97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[10873, 10873, 10873,  ..., 12898, 12898, 12898],\n",
       "                       [  389,   390,   391,  ...,   390,   391,   392]]),\n",
       "       values=tensor([ 2.1579e-01,  9.0801e-01, -6.2694e-01,  8.8655e-02,\n",
       "                       2.9705e-01,  2.1936e-01,  4.5548e-02,  6.2830e-01,\n",
       "                       8.9318e-01,  1.4639e-01, -2.4036e-01, -3.8674e-01,\n",
       "                       6.3910e-01,  3.8219e-01, -4.1676e-01, -1.2358e+00,\n",
       "                       3.6658e-01,  9.1405e-01, -4.3511e-01,  2.4728e-01,\n",
       "                       3.9745e-01, -5.0088e-03, -4.0033e-01,  2.9997e-01,\n",
       "                       3.9647e-01, -2.7153e-01, -4.1791e-01,  3.9787e-01,\n",
       "                      -1.7304e-02,  2.4832e-01, -3.4706e-01, -3.0040e-01,\n",
       "                      -1.3448e-02, -1.0570e-01, -3.4252e-01, -5.2620e-01,\n",
       "                       3.9345e-02,  3.1474e-01, -2.5165e-01,  9.3509e-01,\n",
       "                       2.4554e-02,  5.3729e-01, -7.1566e-01,  8.4123e-01,\n",
       "                       2.0000e-01, -1.0897e-01, -4.0530e-01, -1.5539e-02,\n",
       "                       1.4000e-01,  1.1782e+00, -9.3665e-03,  8.6749e-01,\n",
       "                       1.5334e-01,  5.5540e-01, -3.4887e-01,  1.8663e-01,\n",
       "                       1.4397e+00,  9.9878e-02, -3.3079e-01, -1.6309e+00,\n",
       "                      -6.1783e-02,  2.0920e-01, -9.7395e-01, -2.1258e-01,\n",
       "                       2.9641e-01,  2.0513e-01, -2.8091e-01,  5.4474e-01,\n",
       "                       1.4262e-01,  4.6801e-01, -6.4786e-01, -6.9482e-01,\n",
       "                      -9.5449e-01, -2.6389e-01,  1.6505e-01,  1.0336e+00,\n",
       "                      -3.6485e-03, -5.9776e-01, -3.9653e-01,  9.0955e-01,\n",
       "                       1.1822e+00,  5.7206e-01, -4.8330e-01, -2.7775e-01,\n",
       "                      -1.6214e+00, -6.5104e-01,  1.1314e-01,  9.1333e-02,\n",
       "                       1.3700e-01,  5.7330e-01, -2.9200e-01,  6.9347e-01,\n",
       "                      -6.6012e-01, -2.1324e-01,  1.5480e-01,  7.0536e-01,\n",
       "                       5.6634e-01, -3.6345e-01, -4.8735e-01,  4.0995e-01,\n",
       "                      -1.8176e-01, -4.1003e-01, -1.0477e-01, -1.0871e+00,\n",
       "                      -2.5139e-01, -3.7833e-01, -8.4344e-02,  1.2129e+00,\n",
       "                       1.1372e+00,  4.3474e-01, -3.2581e-01, -5.2524e-01,\n",
       "                       4.8697e-01,  1.0861e+00, -5.7147e-01,  4.2646e-01,\n",
       "                       1.4111e-01, -2.4615e-01, -3.2834e-01,  6.9385e-01,\n",
       "                       1.1843e-01,  7.9184e-01,  4.5139e-01, -9.2613e-01,\n",
       "                       1.0125e+00, -1.7225e-01, -3.5395e-01, -8.5203e-01,\n",
       "                       6.6764e-01, -1.6202e-02, -5.3558e-01, -8.3081e-01,\n",
       "                      -1.1709e-03, -7.3953e-01, -4.6977e-01,  9.2207e-01,\n",
       "                       2.3546e-01, -1.5447e-01, -2.8754e-01,  7.1580e-01,\n",
       "                      -4.1700e-01,  2.0335e-01, -4.9093e-02,  9.9777e-01,\n",
       "                       5.7433e-01,  7.6072e-01, -5.7035e-01, -4.8897e-02,\n",
       "                       4.0959e-01, -1.3143e-01, -4.4693e-01,  4.3675e-01,\n",
       "                       4.0916e-01,  1.0344e-02, -3.1026e-01, -1.1016e+00,\n",
       "                       2.9128e-02,  8.9761e-01,  4.3191e-01, -2.1289e-01,\n",
       "                       6.3884e-01, -3.4190e-01, -4.1421e-01, -2.0221e+00,\n",
       "                       6.1425e-01,  6.2739e-01, -5.7850e-01,  2.0469e-01,\n",
       "                       3.6921e-01,  6.4895e-01, -8.3591e-01, -5.0690e-02,\n",
       "                       8.1646e-01, -1.8171e-01, -6.8179e-01, -1.2029e-01,\n",
       "                       3.1821e-01, -3.5911e-02, -3.1035e-01,  1.3947e-01,\n",
       "                       2.5010e-01, -1.4043e-02, -9.6979e-02,  7.7593e-01,\n",
       "                       1.3867e-01,  6.4783e-01, -4.0146e-01,  1.7121e-01,\n",
       "                       4.1783e-01,  9.6972e-01, -6.0833e-01,  2.2453e-01,\n",
       "                       3.0220e-01, -4.9861e-02, -3.1401e-01,  1.0785e-01,\n",
       "                      -9.6795e-01, -1.1724e-01,  9.3961e-02,  9.7725e-01,\n",
       "                       1.9243e-01, -4.3674e-01, -3.2360e-01,  6.9408e-01,\n",
       "                       2.9297e-01,  7.7300e-01, -6.2402e-01, -9.5660e-02,\n",
       "                      -5.0848e-01, -1.4446e-01, -8.9730e-02, -2.5787e-01,\n",
       "                      -6.4075e-01, -2.0006e-01,  1.8677e-02,  2.5744e-01,\n",
       "                      -8.2994e-01, -1.1230e-02, -5.3483e-02,  2.0266e-01,\n",
       "                       4.2148e-01,  8.7608e-01, -5.4224e-01,  3.0778e-01,\n",
       "                       2.1845e-02,  6.8987e-01, -3.7880e-01,  1.6044e-01,\n",
       "                       4.1250e-01,  8.0623e-01, -6.4054e-01,  4.8979e-02,\n",
       "                       3.5826e-01,  1.9913e-01,  5.8273e-02,  7.1549e-01,\n",
       "                       3.7036e-01, -2.3513e-01, -3.0299e-01, -1.1138e+00,\n",
       "                       5.7553e-01, -2.2975e-01, -4.1659e-01,  4.3865e-01,\n",
       "                       5.7316e-01, -1.5185e-01, -7.8079e-01, -2.6606e-01,\n",
       "                       3.0222e-01,  1.0720e+00,  7.6054e-01, -5.7130e-01,\n",
       "                       3.1142e-01,  6.6125e-02,  1.1207e-02,  7.3567e-01,\n",
       "                       7.3178e-02,  6.8059e-01,  7.3310e-01, -1.2195e+00,\n",
       "                       1.2838e-01, -2.4266e-01, -2.6876e-01,  1.0339e+00,\n",
       "                       1.0628e+00,  2.1626e-01, -1.5918e-01, -2.8077e-01,\n",
       "                       1.8442e-01,  4.7494e-01,  2.7955e-01,  8.0209e-02,\n",
       "                      -4.5776e-01,  8.2170e-02, -9.6332e-02, -4.6785e-01,\n",
       "                       3.9001e-01,  2.9702e-01, -3.5443e-01, -1.6282e-01,\n",
       "                       1.9233e-01, -6.7320e-03, -3.1503e-01, -1.3290e-01,\n",
       "                      -6.8518e-02, -7.4697e-01, -4.2444e-01,  1.0557e+00,\n",
       "                       8.9117e-01,  2.8819e-01, -2.8491e-01, -4.7271e-01,\n",
       "                       4.3485e-01, -2.5953e-02, -4.1634e-01,  3.7364e-01,\n",
       "                       3.7549e-02,  6.3924e-03, -1.6574e-01, -1.1758e-02,\n",
       "                       2.9133e-01, -9.5304e-02, -2.2081e-01,  6.0234e-01,\n",
       "                       1.3777e+00,  9.9391e-02, -6.2158e-01, -1.4190e+00,\n",
       "                      -6.4900e-01, -2.3538e-01,  1.3957e-01,  4.7469e-01,\n",
       "                      -3.5375e-02, -1.4234e-01,  1.4805e-03,  1.1006e+00,\n",
       "                      -2.0200e-01,  3.8516e-01, -5.6415e-01, -5.2610e-01,\n",
       "                      -9.7001e-01, -2.5380e-01,  1.7476e-01,  1.1126e+00,\n",
       "                       6.4607e-01,  4.5955e-01, -2.2361e-01,  2.4568e-01,\n",
       "                       3.7702e-01,  1.9531e-01, -3.7367e-01, -2.2358e-02,\n",
       "                       4.1600e-01,  1.0893e+00, -5.2731e-01,  6.4502e-01,\n",
       "                      -6.3708e-03, -4.0442e-01, -1.9929e-01,  9.4492e-01,\n",
       "                       3.6171e-01, -6.8195e-03, -2.5810e-01,  5.1797e-01,\n",
       "                       5.4185e-01,  2.3432e-01, -2.9541e-01, -8.7217e-01,\n",
       "                       2.2166e-01,  4.7412e-01, -5.7721e-01,  7.5042e-01,\n",
       "                      -4.0989e-01,  4.6868e-01, -7.4565e-01, -7.3576e-01,\n",
       "                       8.7351e-01,  3.5199e-01, -7.9219e-01, -1.7894e-01,\n",
       "                       4.1433e-01,  6.9201e-01, -5.5189e-01, -6.2204e-01,\n",
       "                       2.8137e-01,  1.3054e-01, -7.3905e-01, -3.1531e-01,\n",
       "                      -6.7533e-01, -1.0553e-01, -1.2692e-02,  5.9336e-01,\n",
       "                       3.5761e-01,  7.3665e-01, -6.3286e-01,  2.3433e-01,\n",
       "                       1.8226e-02,  4.4652e-01, -4.0346e-01, -7.9669e-02,\n",
       "                       5.5917e-01,  5.1373e-01, -7.9364e-01,  2.9678e-01,\n",
       "                      -8.5908e-01,  2.2090e-01, -2.5196e-01,  4.3437e-02,\n",
       "                      -4.7620e-01, -6.7991e-02,  1.6685e-02,  7.1588e-01,\n",
       "                       5.0372e-01, -4.6581e-01, -3.1786e-01,  5.1958e-01,\n",
       "                       3.0281e-01,  6.2237e-01, -2.3842e-01,  7.2842e-01,\n",
       "                       1.3588e-01,  7.4488e-01, -3.0694e-01, -1.4013e-01,\n",
       "                       7.4490e-01,  1.6519e-01, -4.7577e-01, -3.8646e-01,\n",
       "                       3.4212e-01,  2.9683e-02, -3.5324e-01,  4.3411e-01,\n",
       "                      -1.1182e-01, -4.8618e-02, -5.2879e-01,  1.0700e+00,\n",
       "                      -8.2926e-02, -5.1462e-02,  7.8759e-02,  5.7799e-01,\n",
       "                       8.6694e-02,  8.0867e-02, -5.5226e-01, -2.2186e-01,\n",
       "                       5.5632e-01,  1.1073e+00, -7.7618e-01,  3.9236e-01,\n",
       "                      -7.7353e-02, -2.1469e-01, -4.3399e-01,  1.0403e+00,\n",
       "                       2.0535e-01,  6.9045e-01, -7.7817e-01, -9.1260e-01,\n",
       "                      -2.0045e-01,  7.4100e-01,  6.1723e-01,  2.6658e-01,\n",
       "                       1.1030e+00, -8.2387e-02, -2.6801e-01, -1.8813e+00,\n",
       "                       7.2484e-02, -1.9836e-01, -3.8817e-01, -7.2667e-01,\n",
       "                       2.3726e-01,  2.8475e-01,  3.8692e-01, -4.6444e-01,\n",
       "                      -1.4979e-01, -5.4831e-01, -3.4183e-01,  1.0462e+00,\n",
       "                       1.3346e-01,  1.0502e-01,  2.6186e-02,  6.6635e-01,\n",
       "                       1.2327e+00,  1.1168e-01, -5.3292e-01, -1.1460e+00,\n",
       "                      -1.1465e-01, -2.1948e-01, -4.2961e-01, -1.3102e+00,\n",
       "                       2.5664e-01, -3.7793e-03, -3.4642e-01, -2.6148e-01,\n",
       "                       5.5773e-01,  6.5461e-01, -5.4114e-01,  3.0840e-01,\n",
       "                       3.6518e-01,  8.2181e-01, -9.1708e-01,  1.0469e-01,\n",
       "                       3.7621e-01,  3.9171e-01, -1.5356e-01,  5.9027e-01,\n",
       "                      -6.7688e-01, -8.7370e-03, -7.8315e-02,  6.6073e-01,\n",
       "                       2.8241e-01, -3.1075e-02, -2.3208e-01, -6.0173e-01,\n",
       "                       1.1045e-01,  6.7715e-01, -1.8975e-01,  6.8122e-01,\n",
       "                       1.8560e-01,  8.7634e-01, -1.8990e-01,  1.8643e-01,\n",
       "                       1.4559e-01, -6.2266e-02, -1.1953e-01,  8.0627e-01,\n",
       "                       8.7773e-01, -1.9120e-01, -6.2979e-01, -4.9362e-01,\n",
       "                       5.9293e-01, -7.4667e-02, -4.4337e-01,  2.4978e-01,\n",
       "                       6.5777e-02,  1.7752e-01,  7.3024e-02,  5.4462e-01,\n",
       "                      -3.7006e-01,  2.7592e-01, -9.4581e-01, -7.1095e-01,\n",
       "                       2.0122e-01,  5.6941e-01, -7.1822e-01,  1.4587e-01,\n",
       "                       1.8453e-01,  5.5543e-01, -8.4529e-01, -1.4666e-01,\n",
       "                      -3.3956e-01,  8.6453e-02, -6.0531e-02,  4.3914e-01,\n",
       "                       1.5681e+00, -6.9240e-01, -5.8420e-01, -7.8522e-01,\n",
       "                       5.2006e-01,  2.7269e-01, -4.6764e-01,  3.0203e-01,\n",
       "                       5.3195e-01,  1.5495e-01, -4.0777e-01,  2.7128e-01,\n",
       "                       8.7795e-02,  5.6230e-01, -7.2455e-01, -9.0332e-01,\n",
       "                       1.6862e-01,  7.4415e-01,  6.5049e-01, -5.0124e-01,\n",
       "                       3.9829e-01,  7.3730e-01, -6.5312e-01, -1.7723e-02,\n",
       "                       6.7643e-01,  7.2766e-01, -5.3540e-01,  3.2058e-01,\n",
       "                      -1.2179e-01, -1.8241e-01, -5.7919e-01, -3.5966e-01,\n",
       "                       4.3767e-01,  4.8360e-01, -4.8251e-01,  4.6583e-01,\n",
       "                       2.1984e-01,  8.1241e-02,  1.0709e-01,  8.1232e-01,\n",
       "                       4.3186e-01,  2.5258e-01, -1.6265e-01,  1.3251e-01,\n",
       "                       9.1717e-02, -2.6505e-01, -8.6338e-02,  8.5751e-01,\n",
       "                       4.0751e-01, -3.8193e-01, -3.3751e-01, -1.8641e+00,\n",
       "                      -1.3033e+00, -3.6911e-01,  2.2241e-02,  5.6826e-01,\n",
       "                       3.5231e-01,  4.6412e-01, -5.5437e-01,  4.3511e-01,\n",
       "                       1.4242e-01,  5.9011e-01, -7.1857e-01,  1.2425e-01,\n",
       "                       7.5109e-02,  2.9524e-01, -3.8849e-01,  6.6014e-01,\n",
       "                       1.8597e-01,  4.0273e-01,  4.9831e-01,  9.2487e-01,\n",
       "                      -2.4727e-02,  2.3847e-01,  3.6872e-01,  4.2823e-01,\n",
       "                       6.5923e-01,  2.1281e-01, -3.3404e-01, -3.9266e-01,\n",
       "                      -1.3592e-01,  2.1611e-01, -1.9836e-03,  8.9459e-01,\n",
       "                       2.6957e-01, -9.9720e-02, -3.0500e-01, -1.5279e+00,\n",
       "                       9.1045e-02, -2.9117e-02, -3.9335e-01, -2.2661e-01,\n",
       "                      -2.5184e-01, -1.3011e-01,  4.7672e-02,  1.8421e-01,\n",
       "                       4.5804e-01,  6.9014e-01, -5.6176e-01,  4.6771e-01,\n",
       "                       6.5150e-01, -7.2728e-01, -6.5199e-01,  3.3829e-01,\n",
       "                       3.2860e-01,  1.4305e-01,  4.3423e-02,  6.6125e-01,\n",
       "                       6.5295e-01, -8.4825e-02, -4.4876e-01, -1.2389e+00,\n",
       "                      -9.7804e-02,  7.2648e-01,  3.1610e-01,  9.5773e-01,\n",
       "                       3.0953e-01,  8.4253e-01, -6.0208e-01,  5.8207e-01,\n",
       "                      -3.2426e-02, -7.2707e-01, -4.7642e-01,  1.6936e-02]),\n",
       "       size=(14224, 396), nnz=656, layout=torch.sparse_coo,\n",
       "       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "639e5a41-a268-4e15-9027-ec4dbbcda71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db021f8-a311-4c37-ac60-c60392877d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fd9b6-b914-44b8-bdc4-c565e0906194",
   "metadata": {},
   "source": [
    "# Connect to R-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef175e6b-b693-4709-ba84-f3bf9761ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14224, 14224])\n",
      "torch.Size([14224, 396])\n"
     ]
    }
   ],
   "source": [
    "length = len(node_map.keys())\n",
    "\n",
    "indices = torch.zeros((1),device=device)\n",
    "indices = torch.arange(0, length, device=device).repeat(2, 1)\n",
    "\n",
    "values = torch.ones(length, device=device)\n",
    "identity = torch.sparse_coo_tensor(indices, values, (length, length), device=device)\n",
    "print(identity.size())\n",
    "print(feature_matrix.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19c9b30e-69aa-43ac-ad3b-e36dbc66ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14224, 14620]) Great success!device\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_cat = torch.cat((identity,feature_matrix),dim=1)\n",
    "print(feature_matrix_cat.size(), 'Great success!device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff547a-e563-4a62-a268-081364440c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab0a9217-e178-4ecc-ae9d-41270a55e860",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87512148-5532-423a-b7b4-999c6ede7bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15001"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from typing import defaultdict\n",
    "adjacency = defaultdict(torch.sparse_coo_tensor)\n",
    "# inv_edge_map\n",
    "indices_x = defaultdict(list)\n",
    "indices_y = defaultdict(list)\n",
    "\n",
    "values = defaultdict(list)\n",
    "i=0\n",
    "for s, p, o in graph:\n",
    "    edge_id = inv_edge_map[p.identifier]\n",
    "    # print(edge_id)\n",
    "    s_id = inv_node_map[s.identifier]\n",
    "    o_id = inv_node_map[o.identifier]\n",
    "    \n",
    "    indices_x[edge_id].append(s_id)\n",
    "    indices_y[edge_id].append(o_id)\n",
    "    values[edge_id].append(1)\n",
    "    i += 1\n",
    "\n",
    "graph_size = max(node_map.keys()) +1\n",
    "\n",
    "for edge_id in indices_x.keys():\n",
    "    ind_x, ind_y = torch.tensor(indices_x[edge_id]), torch.tensor(indices_y[edge_id])\n",
    "    ind = torch.stack((ind_x, ind_y))\n",
    "    val = values[edge_id]\n",
    "    adjacency[edge_id] = torch.sparse_coo_tensor(indices=ind,values=val,size=(graph_size,graph_size),device=device)\n",
    "    \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4d3af-c4fc-4645-8680-27c331b6bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e02d68b9-3be5-4562-8ef8-e0ae2e34ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great success! 0\n",
      "Great success! 1\n",
      "Great success! 2\n",
      "Great success! 3\n",
      "Great success! 4\n",
      "Great success! 5\n",
      "Great success! 6\n",
      "Great success! 7\n",
      "Great success! 8\n",
      "Great success! 9\n",
      "Great success! 10\n",
      "Great success! 11\n",
      "Great success! 12\n",
      "Great success! 13\n",
      "Great success! 14\n",
      "Great success! 15\n",
      "Great success! 16\n",
      "Great success! 17\n",
      "Great success! 18\n",
      "Great success! 19\n",
      "Great success! 20\n",
      "Great success! 21\n",
      "Great success! 22\n",
      "Great success! 23\n",
      "Great success! 24\n",
      "Great success! 25\n",
      "Great success! 26\n",
      "Great success! 27\n",
      "Great success! 28\n",
      "Great success! 29\n",
      "Great success! 30\n",
      "Great success! 31\n",
      "Great success! 32\n",
      "Great success! 33\n",
      "Great success! 34\n",
      "Great success! 35\n",
      "Great success! 36\n",
      "Great success! 37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     1,     2,  ..., 14221, 14222, 14223],\n",
       "                       [    0,     1,     2,  ..., 14221, 14222, 14223]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(14224, 14224), nnz=14225, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_adjacency(A, self=True): #gotta still add self-loops. See if it works without first.\n",
    "    #it still crashes :(\n",
    "    size = A.size()[0]\n",
    "    A = A.coalesce()\n",
    "    # A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "    eye_indices = torch.arange(0,size, device=device).repeat(2,1)\n",
    "    eye_values = torch.ones(size, device=device)\n",
    "    indices = torch.cat((A.indices(), eye_indices), dim=1)\n",
    "    values = torch.cat((A.values(), eye_values), dim=0)\n",
    "    A = torch.sparse_coo_tensor(indices, values, A.size(), device=device).coalesce()\n",
    "    # print('to_dense()?')\n",
    "    degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "    # print('no')\n",
    "    # print(degree.size())\n",
    "    d_inv_sqrt = degree.pow(-0.5)\n",
    "    D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    # A = A.to_dense()\n",
    "    normalized_values = values * d_inv_sqrt[indices[0]] * d_inv_sqrt[indices[1]]\n",
    "    \n",
    "    normalized_A = torch.sparse_coo_tensor(indices, normalized_values, A.size(), device=device).coalesce()\n",
    "    # normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "\n",
    "\n",
    "for i, key in enumerate(adjacency.keys()):\n",
    "    A = adjacency[key]\n",
    "    adjacency[key] = normalize_adjacency(A)\n",
    "    del A\n",
    "    print(f'Great success! {i}')\n",
    "    k = key\n",
    "adjacency[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eeb229-0bab-46de-980e-34b2f4a2ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "##save non-error version\n",
    "def block_diag (weights):\n",
    "    block_diag_matrix = torch.block_diag(*weights).to_sparse()\n",
    "    return block_diag_matrix\n",
    "    \n",
    "\n",
    "class R_GCNLayer(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, num_relations, block_split):\n",
    "        super(R_GCNLayer, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.num_relations = num_relations\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        max_block_size = 1\n",
    "        for i in range(1,11):\n",
    "            if x_dim % i == 0 and y_dim % i == 0:\n",
    "                max_block_size +=1 \n",
    "        print(max_block_size)\n",
    "        x_block_size = x_dim // block_split\n",
    "        y_block_size = y_dim // block_split\n",
    "        self.W = nn.ParameterList()\n",
    "        for _ in range(num_relations):\n",
    "            wr = nn.ParameterList()\n",
    "            # relation_weights = []\n",
    "            for i in range(block_split):\n",
    "                w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                # relation_weights.append(w)\n",
    "             #move blocks to params, not block diag\n",
    "            # print(block_diag_matrix)\n",
    "                wr.append(w)\n",
    "                # self.Wr.append(wr)\n",
    "            x_remainder = x_dim % block_split\n",
    "            y_remainder = y_dim % block_split\n",
    "            if  x_remainder > 0 and y_remainder > 0:\n",
    "                w = nn.Parameter(torch.randn(x_remainder, y_remainder)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                wr.append(w)\n",
    "            self.W.append(wr)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "        for r in range(self.num_relations):\n",
    "            X = X.to_dense()\n",
    "            if X.is_sparse:\n",
    "                diag = block_diag(self.W[r]).to_dense()\n",
    "                correct_indices = (indices >= 0).all(dim=0)\n",
    "                X = X.coalesce()\n",
    "                # X.indices = X.indices()[:, correct_indices]\n",
    "                # X.values = X.values()[correct_indices] \n",
    "                if diag.is_sparse: #this really defeats the point of using sparse matrices, but torch.sparse only supports sparse, dense mm\n",
    "                    # print(dir(X))\n",
    "                    diag = diag.to_dense()\n",
    "                    \n",
    "                weighted = torch.matmul(X, diag)\n",
    "            else:\n",
    "                weighted = torch.matmul(X, block_diag(self.W[r])) \n",
    "            if A[r].is_sparse:\n",
    "                transformed = torch.sparse.mm(A[r], weighted)\n",
    "            else:\n",
    "                transformed = torch.matmul(A[r], weighted)\n",
    "            # print(transformed.size(), aggregated.size())\n",
    "            aggregated += transformed\n",
    "        aggregated += self.bias\n",
    "        return aggregated\n",
    "\n",
    "# class R_GCNLayer(nn.Module): \n",
    "#     def __init__(self, x_dim, y_dim, num_relations, bases=5, block_split=2):\n",
    "#         super(R_GCNLayer, self).__init__()\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.num_relations = num_relations\n",
    "#         gain = nn.init.calculate_gain('relu')\n",
    "#         max_block_size = 1\n",
    "#         x_block_size = x_dim // block_split\n",
    "#         y_block_size = y_dim // block_split\n",
    "#         self.W = nn.ParameterList()\n",
    "#         self.relation_vectors = nn.ParameterList()\n",
    "#         for _ in range(num_relations):\n",
    "#             self.relation_vectors.append(nn.Parameter(torch.randn(1,x_dim)))\n",
    "#         wr = nn.ParameterList()\n",
    "#         for i in range(block_split):\n",
    "#             w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "#             nn.init.kaiming_uniform_(w, a=gain)\n",
    "#             # relation_weights.append(w)\n",
    "#          #move blocks to params, not block diag\n",
    "#         # print(block_diag_matrix)\n",
    "#             wr.append(w)\n",
    "#             # self.Wr.append(wr)\n",
    "#         x_remainder = x_dim % block_split\n",
    "#         y_remainder = y_dim % block_split\n",
    "#         if  x_remainder > 0 and y_remainder > 0:\n",
    "#             w = nn.Parameter(torch.randn(x_remainder, y_remainder)) \n",
    "#             nn.init.kaiming_uniform_(w, a=gain)\n",
    "#             wr.append(w)\n",
    "#         self.W = wr\n",
    "#         print(f\"W: {self.W[0].size()}\\nVr:{self.relation_vectors[0].size()}\")\n",
    "\n",
    "#         self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "#         nn.init.zeros_(self.bias)\n",
    "#     def __init__(self, x_dim, y_dim, num_relations, num_bases=30):\n",
    "#         super(R_GCNLayer, self).__init__()\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.num_relations = num_relations\n",
    "#         self.num_bases = num_bases\n",
    "\n",
    "#         gain = nn.init.calculate_gain('relu')\n",
    "#         self.W_b = nn.Parameter(torch.Tensor(self.num_bases, self.x_dim, self.y_dim))\n",
    "#         nn.init.kaiming_uniform_(self.W_b, a=gain)\n",
    "#         self.W_r = nn.Parameter(torch.Tensor(self.num_relations, self.num_bases))\n",
    "#         nn.init.kaiming_uniform_(self.W_r, a=gain)\n",
    "#         self.bias = nn.Parameter(torch.Tensor(self.y_dim))\n",
    "#         nn.init.zeros_(self.bias)\n",
    "\n",
    "#     def forward(self, A, X):\n",
    "#         aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "#         weights = torch.matmul(self.W_r, self.W_b.view(self.num_bases, -1))\n",
    "#         weights = weights.view(self.num_relations, self.x_dim, self.y_dim)        \n",
    "#         for r in range(self.num_relations):\n",
    "#             A_r = A[r]\n",
    "#             relation_weights = weights[r]\n",
    "#             weighted = torch.sparse.mm(A_r, X)\n",
    "#             print(weighted.size(), relation_weights.size())\n",
    "#             weighted = torch.matmul(weighted, relation_weights)\n",
    "#             print(weighted)\n",
    "#             aggregated += weighted\n",
    "        \n",
    "#         aggregated += self.bias\n",
    "#         return aggregated\n",
    "\n",
    "class R_GCN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, y_dim, max_k_hop, num_relations):\n",
    "        super(R_GCN, self).__init__()\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.num_relations = num_relations\n",
    "        self.x_dim = x_dim\n",
    "        self.block_split = 1\n",
    "        self.bases = 1\n",
    "        self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations, self.bases) for _ in range(max_k_hop)])\n",
    "        self.final_r_gcn = R_GCNLayer(h_dim, y_dim,num_relations, self.bases) \n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.norm = nn.BatchNorm1d(y_dim)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        # if X.size(0) % 2 !=0:\n",
    "        #     X = X[:-1,:]\n",
    "        # outer layers\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(self.max_k_hop):\n",
    "            H = self.gcns[i](A, X)\n",
    "            H = self.act(H)\n",
    "            H = self.drop(H)\n",
    "        Y = self.final_r_gcn(A, H)\n",
    "        out = self.act(Y)\n",
    "        out = self.norm(out)\n",
    "        return out#nn.LogSoftmax(Y)\n",
    "\n",
    "#feature_matrix.size(0)+1\n",
    "num_classes=5\n",
    "r_gcn = R_GCN(feature_matrix_cat.size(1), 32, num_classes, max_k_hop=2, num_relations=i+1).to(device)\n",
    "out = r_gcn([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], feature_matrix_cat)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc635fd6-d7b2-4ca0-9fa6-d59d3f29bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.coalesce().indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3ec9a-84e0-41dc-97a0-2b8cb5489fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c226473-516c-430b-9108-ef4c06d2d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "6338/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea53cc-d72e-48d2-a3b9-dba17fe6a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4efe3-34ad-48c7-b56e-4d040b9710c7",
   "metadata": {},
   "source": [
    "# Why are these zeros...?           ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c855e-463c-4006-afa6-ea8a1508684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_list = [normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3316a6c-75ac-4126-9291-f84f19333d76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MR_GCN(nn.Module):\n",
    "    def __init__(self, A_list, eye, num_nodes, rgcn_inp_dim, embed_size=396, num_classes=5):\n",
    "        super(MR_GCN, self).__init__()\n",
    "        \n",
    "        self.A_list = A_list\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding_matrix = torch.zeros((num_nodes, embed_size), dtype=torch.float32, device=device).to_sparse()\n",
    "        self.MME= MultiModalEncoder(feature_matrix=self.embedding_matrix, encoders_config=encoders_config).to(device)\n",
    "        self.graph_dim = A_list[0].size(0)\n",
    "        self.R_GCN = R_GCN(rgcn_inp_dim, 128, num_classes, max_k_hop=2, num_relations=len(A_list)).to(device)\n",
    "        self.eye = eye\n",
    "        self.feature_matrix = []\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        X = self.MME()\n",
    "        X = torch.cat((self.eye,X), dim=1)\n",
    "        X = self.R_GCN(self.A_list, X)\n",
    "        return X\n",
    "mr_gcn = MR_GCN([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], identity, num_nodes, feature_matrix_cat.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a992be4-5f05-41ef-8ae7-fdbd1537c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_gcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c272ea1-1625-4f5c-9c78-bd430ca27928",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_label_set|val_label_set|test_label_set)\n",
    "num_classes = len(labels)\n",
    "ohe = torch.eye(num_classes)\n",
    "label_to_ohe = {label:ohe[i] for i, label in enumerate(labels)}\n",
    "label_to_num = {label:i for i, label in enumerate(labels)}\n",
    "\n",
    "labels = torch.full((num_nodes,), -1, dtype=torch.long, device=device)\n",
    "\n",
    "train_labels = labels\n",
    "for node_id, label in train_node_label_map.items():\n",
    "    train_labels[node_id] = label_to_num[label]\n",
    "\n",
    "train_mask = labels>=0\n",
    "\n",
    "test_labels = labels\n",
    "for node_id, label in test_node_label_map.items():\n",
    "    test_labels[node_id] = label_to_num[label]\n",
    "\n",
    "test_mask = labels>=0\n",
    "\n",
    "val_labels = labels\n",
    "for node_id, label in val_node_label_map.items():\n",
    "    val_labels[node_id] = label_to_num[label]\n",
    "\n",
    "val_mask = labels>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984a686-acd2-4811-b7bf-bd0977cd330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b3f4f-db9d-4c26-8a58-31fe9853667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada288fa-c4f4-4b12-badb-9e2025f6ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[train_mask].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f2028-dfff-44fa-940a-f4161e873aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[train_mask].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e36100-8d42-4ce0-85c9-52586c5c64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(mr_gcn.parameters())\n",
    "optimizer = torch.optim.Adam(params,lr=0.01)\n",
    "\n",
    "num_epochs = 600\n",
    "mr_gcn = MR_GCN([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], identity, num_nodes, feature_matrix_cat.size(1), num_classes=num_classes)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mr_gcn.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = mr_gcn()\n",
    "    print(outputs.size())\n",
    "    loss = criterion(outputs[train_mask], labels[train_mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    mr_gcn.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = mr_gcn()\n",
    "        val_loss = criterion(val_outputs[val_mask], labels[val_mask])\n",
    "        \n",
    "        _, predicted = torch.max(val_outputs[val_mask], dim=1)\n",
    "        correct = (predicted == labels[val_mask]).sum().item()\n",
    "        tot = val_mask.sum().item()\n",
    "        if tot:\n",
    "            \n",
    "            val_acc = correct / tot\n",
    "        if epoch % 5 == 0 and tot:\n",
    "            print(f\"epoch: {epoch}\\navg loss: {loss}, val_loss: {val_loss} val accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f28a9-1f12-4f1f-a7fb-f65757b31eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in mr_gcn.parameters():\n",
    "    if param.is_sparse:\n",
    "        print(f\"Sparse tensor found: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd1621-8248-4369-b861-91ae5e9dee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1648e2-3f0a-42f6-82be-2afcbb8d05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask[335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83015b-7c0a-419f-98aa-21a15d2f0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_node_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94939049-3943-4c41-a3fe-d82101c73017",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c43b9a-e892-4816-9772-e97bc79383fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23826ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
