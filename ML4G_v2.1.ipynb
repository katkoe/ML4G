{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9a3cc9-f2e3-473a-8286-d9377071d310",
   "metadata": {},
   "source": [
    "# Version 2 (sparse matrices...)\n",
    "### I created the last notebook naively and completely missed that the discusses method operates on sparse matrices, so I'm starting again from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c292f-2131-4f8e-a58b-03aff54d508a",
   "metadata": {},
   "source": [
    "# Generating a sparse matrix representation of a graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4159d-e344-4900-9c07-e02cd1556604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508eddc7-743a-4d1e-a43c-aad72c579ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_nodes = 100\n",
    "average_out_degree = 9\n",
    "total_edge_classes = 36\n",
    "embedding_size = 128\n",
    "output_dim = 4\n",
    "\n",
    "\n",
    "\n",
    "# First we generate some arbitrary graph\n",
    "def generate_graph(num_nodes, average_out_degree, total_edge_classes):\n",
    "    num_edges = average_out_degree*num_nodes\n",
    "    indices = torch.randint(0, num_nodes, (2, num_edges))\n",
    "    print(indices.size())\n",
    "    edges = torch.ones(num_edges)\n",
    "    print(edges.size())\n",
    "    A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes))\n",
    "    return A\n",
    "# A = generate_graph(15,3,5).to_dense()\n",
    "\n",
    "# Now maybe it's useful to ensure it's connected...\n",
    "def generate_connected_graph(num_nodes, average_out_degree, total_edge_classes, embedding_size):\n",
    "    edges_set ,\n",
    "    num_edges = average_out_degree * num_nodes\n",
    "\n",
    "    def sample_random_edge_class():\n",
    "        return torch.randint(0, total_edge_classes, (1,)).item()\n",
    "    \n",
    "    def generate_random_edge():\n",
    "        u, v = torch.randint(0, num_nodes, (2,)).tolist()\n",
    "        #no self loops\n",
    "        while u == v:\n",
    "            u, v = torch.randint(0, num_nodes, (2,)).tolist()\n",
    "        return (u, v)\n",
    "    \n",
    "    def generate_random_node():\n",
    "        return torch.randint(0, num_nodes, (1,)).item()\n",
    "    \n",
    "    def sample_random_node(A):\n",
    "        connected_nodes = A._indices()[1].tolist() #select from incoming to...\n",
    "        if not connected_nodes:\n",
    "            return generate_random_node()\n",
    "        return connected_nodes[torch.randint(0, len(connected_nodes), (1,)).item()]#torch.nonzero(A)\n",
    "    \n",
    "    initial_edge = generate_random_edge()\n",
    "    edges_set ,.add(initial_edge)\n",
    "    indices = torch.tensor([[initial_edge[0]], [initial_edge[1]]], dtype=torch.long, device=device)\n",
    "    edges = torch.ones(1, device=device)\n",
    "    A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes), device=device)\n",
    "    A_list = []\n",
    "    for edge_class in range(total_edge_classes):\n",
    "        for _ in range(num_edges - 1):\n",
    "            u = sample_random_node(A)\n",
    "            v = generate_random_node()\n",
    "            while u == v:\n",
    "                v = generate_random_node()\n",
    "            new_edge = (u, v)#torch.cat([u, v], dim=0).unsqueeze(1)\n",
    "            if new_edge in edges_set ,:\n",
    "                continue\n",
    "            edges_set ,.add(new_edge)\n",
    "            edge_tensor = torch.tensor([[u], [v]], dtype=torch.long, device=device)\n",
    "            indices = torch.cat([indices, edge_tensor], dim=1)\n",
    "            edges = torch.ones(indices.size(1), device=device)\n",
    "            A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes), device=device)\n",
    "            # indices = torch.stack((indices,torch.cat([u,v]).unsqueeze(1)),dim=0)\n",
    "        # print(edges_set ,)\n",
    "        A_list.append(A)\n",
    "    random_labels = torch.rand((num_nodes, embedding_size), device=device)\n",
    "    return A_list, random_labels \n",
    "\n",
    "A, X = generate_connected_graph(num_nodes,average_out_degree,total_edge_classes, embedding_size)\n",
    "\n",
    "# Now per edge type? or randomly assign edges...? maybe multinomial_sample(1/k)^k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687621c-5ccc-4eba-b2e1-9006d1bcd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c52b3f-618e-4ab2-ac37-251e542ace6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cb952-daaf-4992-8e60-22b83795f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175c831-5866-40bd-8f7c-3ce5e4d1587a",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db5cdd-6881-47d5-92e8-b3742e910537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## stopped working after some edits to the graph generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300d86c-3c4b-49a5-b447-6f48bce0629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.functional as f\n",
    "\n",
    "# k_hop = 3\n",
    "\n",
    "# def normalize_adjacency(A, self=True): # Self-loop doesn't work with R-GCN\n",
    "#     size = A.size()[0]\n",
    "#     A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "#     degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "#     # print(degree.size())\n",
    "#     d_inv_sqrt = degree.pow(-0.5)\n",
    "#     D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "#     A = A.to_dense()\n",
    "#     normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "#     return normalized_A\n",
    "# A_prime = normalize_adjacency(A)\n",
    "\n",
    "\n",
    "# class GCNLayer(nn.Module):\n",
    "#     def __init__(self, x_dim, y_dim):\n",
    "#         super(GCNLayer, self).__init__()\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         # self.A_norm = A_norm\n",
    "\n",
    "#         self.lin = nn.Linear(x_dim, y_dim)\n",
    "\n",
    "#     def forward(self, A_norm, X):\n",
    "#         device = next(self.parameters()).device\n",
    "#         # print(A_norm, X)\n",
    "#         transformed = self.lin(X)\n",
    "#         aggregated = torch.matmul(A_norm, transformed)\n",
    "#         return aggregated\n",
    "\n",
    "\n",
    "# class GCN(nn.Module):\n",
    "#     def __init__(self, x_dim, h_dim, y_dim, max_k_hop):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.max_k_hop =max_k_hop\n",
    "#         self.x_dim = x_dim\n",
    "#         self.gcns = nn.ModuleList([GCNLayer(x_dim, h_dim) for _ in range(max_k_hop)])\n",
    "#         self.final_gcn = GCNLayer(x_dim, y_dim) \n",
    "#         self.act = nn.ReLU()\n",
    "#         self.drop = nn.Dropout(p=0.2)\n",
    "#         # self.norm = nn.LayerNorm(y_dim)\n",
    "    \n",
    "#     def forward(self, A, X):\n",
    "#         # outer layers\n",
    "#         device = next(self.parameters()).device\n",
    "#         for i in range(self.max_k_hop):\n",
    "#             H = self.gcns[i](A, X)\n",
    "#             H = self.act(H)\n",
    "#             H = self.drop(H)\n",
    "#         Y = self.final_gcn(A, H)\n",
    "#         out = self.act(Y)\n",
    "#         return out#nn.LogSoftmax(Y)\n",
    "# model = GCN(X.size(1), embedding_size, output_dim, k_hop).to(device)\n",
    "# out = model(A_prime, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870f0f8-f8b3-4003-ac61-f7d6c47a1e46",
   "metadata": {},
   "source": [
    "# R-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b77995-b03b-409e-ade0-c2e40cbc34a9",
   "metadata": {},
   "source": [
    "### Block diagonal weight matrix (one is held in memory for each relational weight per layer, so block diagonal sparse matrices save some memory at the cost of some layer-level information flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e4257-f40f-44c9-8322-22e4cf8fc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = embedding_size//4 #some partition... ensure it's a round number\n",
    "relation_weights = [torch.randn(block_size, block_size), torch.randn(block_size, block_size)]\n",
    "\n",
    "block_diag_matrix = torch.block_diag(*relation_weights).to_sparse()\n",
    "block_diag_matrix.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d2a2a-7cb7-4133-9c54-dc0d6022b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( embedding_size//4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c8b48-1db0-4837-95f6-cb2978bc6949",
   "metadata": {},
   "source": [
    "# R-GCN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b2e37-fd0f-4e3d-88a5-ec99fe15d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A, self=True):\n",
    "    size = A.size()[0]\n",
    "    A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "    degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "    d_inv_sqrt = degree.pow(-0.5)\n",
    "    D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    A = A.to_dense()\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "\n",
    "A_list = [normalize_adjacency(a, False) for a in A]\n",
    "\n",
    "\n",
    "##save non-error version\n",
    "def block_diag (weights):\n",
    "    block_diag_matrix = torch.block_diag(*weights).to_sparse()\n",
    "    return block_diag_matrix\n",
    "    \n",
    "\n",
    "class R_GCNLayer(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, num_relations, block_split):\n",
    "        super(R_GCNLayer, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.num_relations = num_relations\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        max_block_size = 1\n",
    "        for i in range(1,11):\n",
    "            if x_dim % i == 0 and y_dim % i == 0:\n",
    "                max_block_size +=1 \n",
    "        x_block_size = x_dim // block_split\n",
    "        y_block_size = y_dim // block_split\n",
    "        self.W = nn.ParameterList()\n",
    "        for _ in range(num_relations):\n",
    "            wr = nn.ParameterList()\n",
    "            for i in range(block_split):\n",
    "                w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                wr.append(w)\n",
    "            self.W.append(wr)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "        for r in range(self.num_relations):\n",
    "            weighted = torch.matmul(X, block_diag(self.W[r]))\n",
    "            transformed = torch.sparse.mm(A[r], weighted)\n",
    "            aggregated += transformed\n",
    "        aggregated += self.bias\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class R_GCN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, y_dim, max_k_hop, num_relations):\n",
    "        super(R_GCN, self).__init__()\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.num_relations = num_relations\n",
    "        self.x_dim = x_dim\n",
    "        self.block_split = 2\n",
    "        self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations, self.block_split) for _ in range(max_k_hop)])\n",
    "        self.final_r_gcn = R_GCNLayer(x_dim, y_dim,num_relations, self.block_split) \n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.norm = nn.BatchNorm1d(y_dim)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(self.max_k_hop):\n",
    "            H = self.gcns[i](A, X)\n",
    "            H = self.act(H)\n",
    "            H = self.drop(H)\n",
    "        Y = self.final_r_gcn(A, H)\n",
    "        out = self.act(Y)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "model = R_GCN(X.size(1), embedding_size, output_dim, k_hop, total_edge_classes).to(device)\n",
    "out = model(A_list, X)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738743cd-c50b-4103-8130-e28d8746ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7295ba-260f-4eb1-86c4-6c1c2dd67825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f738fa3-694a-46dc-bdf6-9438be741943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc738f59-b939-4037-8859-d4e70f0b9150",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c0aef-fcd4-4f35-bb0a-23bb5d85e2a2",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "from tqdm import tqdm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(model.parameters())\n",
    "optimizer = torch.optim.Adam(params,lr=0.01)\n",
    "\n",
    "num_epochs = 600\n",
    "\n",
    "true_labels = torch.randint(0, output_dim, (num_nodes,), dtype=torch.long).to(device)\n",
    "\n",
    "def save_gradient_hook(grad):\n",
    "    gradients.append(grad)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "\n",
    "    predicted_labels = model(A_list, X)\n",
    "    \n",
    "    loss = criterion(predicted_labels, true_labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    i += 1\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    _, predicted = torch.max(predicted_labels.data, 1)\n",
    "    total += true_labels.size(0)\n",
    "    correct += (predicted == true_labels).sum().item()\n",
    "    avg_loss = epoch_loss\n",
    "    accuracy = 100 * correct / total\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch: {epoch}\\navg loss: {avg_loss} accuracy: {accuracy}\")\n",
    "        # make_dot(predicted_labels, params=dict(list(model.named_parameters()))).render(\"r_gcn_torchviz\", format=\"png\")\n",
    "\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8288b8-687d-4645-b942-75ab8bebe744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c268095-1f9a-401d-aa33-b27ec4da3408",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92ff5184-21dd-40ee-ad5d-10a0e7632c6c",
   "metadata": {},
   "source": [
    "# I'm confused about the rdf stuff. I don't see any multimodal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6513d3-0373-42a9-b114-907ea0e5fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d0e07-a704-4346-b9b4-92d49d45becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef63b0-c9f1-44b7-ae9e-09210b4395c9",
   "metadata": {},
   "source": [
    "# Create graph from n-triple file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5b7f1d5-1f5a-44d8-b1f2-764a729e737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph\n",
    "import logging\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "\n",
    "data_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped.nt'\n",
    "mini_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped_mini.nt'\n",
    "folder = './Downloads/ml4g'\n",
    "subsample_chance = 1\n",
    "def create_mini_nt(infile, outfile):\n",
    "    with open(infile, 'r', encoding='utf-8') as inp, open(outfile, 'w', encoding='utf-8') as out:\n",
    "        for i, line in enumerate(inp):\n",
    "            if random.random() <= subsample_chance:\n",
    "                out.write(line)\n",
    "\n",
    "\n",
    "\n",
    "def create_new_graph(path, batch_size = 1000, test=True):\n",
    "    logging.basicConfig(\n",
    "    filename='rdf_parsing_errors.log',\n",
    "    filemode='w')\n",
    "    \n",
    "    graph = Graph()\n",
    "    batch_num = 0\n",
    "    i = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        while True:\n",
    "            batch = []\n",
    "            try:\n",
    "                \n",
    "                [batch.append(next(f)) for j in range(batch_size)]\n",
    "                i += j\n",
    "            except:\n",
    "                pass\n",
    "            if not batch:\n",
    "                break\n",
    "            batch_num += 1\n",
    "            nt_string = ''.join(batch)\n",
    "            try:\n",
    "                graph.parse(data=nt_string, format='nt')\n",
    "                if test:\n",
    "                    graph = Graph()\n",
    "            except ParseError as e:\n",
    "                logging.error(f\"in batch: {batch_num}:\\npproblematic data:\\n\\n{batch}\\n\\n\")\n",
    "                check(batch, batch_num, test=test)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def check(batch, batch_num, test = True):\n",
    "    graph = Graph()\n",
    "    for i, line in enumerate(batch):\n",
    "        try:\n",
    "            graph.parse(line)\n",
    "        except Exception as e:\n",
    "            logging.error(f'in line: {i}:\\n{line}\\n{e}')\n",
    "graph=create_new_graph(mini_loc, test=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "892957c9-1e49-4af8-92aa-1c1d62bb5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38851 6459\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "import random\n",
    "def to_connected_small(g,subsample_chance=0.5):\n",
    "    obj_set = set()\n",
    "    mini_graph = Graph()\n",
    "    i=0\n",
    "    j=0\n",
    "    for s,p,o in g:\n",
    "        i+=1\n",
    "        if random.random() >= subsample_chance and (i<50 or s in obj_set or o in obj_set):\n",
    "            obj_set.add(s)\n",
    "            obj_set.add(o)\n",
    "            j+=1\n",
    "            mini_graph.add(triple=(s,p,o))\n",
    "    print(i,j)\n",
    "    return mini_graph\n",
    "graph = to_connected_small(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db1d1db7-ff84-4dc5-bda5-e74a315a5d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdflib.graph.Graph"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dbd6b40-16da-40e5-8763-4444b9e31865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4645d0-128d-406b-9fff-711c039c4b6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pickle_file_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrdf_graph.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pickle_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(graph, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file_path = os.path.join(folder, 'rdf_graph_smallest.pkl')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(graph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80590000-b2f4-4a06-80fc-d70ae965d536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: day is out of range for month\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: year 0 is out of range\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: Invalid isoformat string: '-01-01'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: day is out of range for month\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: year 0 is out of range\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: day is out of range for month\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: year 0 is out of range\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: month must be in 1..12\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: day is out of range for month\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_xsd_date at 0x7c4cb175ed40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/term.py\", line 2183, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/royal-cookings/miniconda3/lib/python3.12/site-packages/rdflib/xsd_datetime.py\", line 593, in parse_xsd_date\n",
      "    return parse_date(date_string if not minus else (\"-\" + date_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: day is out of range for month\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import pickle\n",
    "import os\n",
    "import rdflib\n",
    "import logging\n",
    "folder = './Downloads/ml4g'\n",
    "\n",
    "pickle_file_path = os.path.join(folder, 'rdf_graph.pkl')\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1a7a1df-aa03-4867-b493-41b5e47c30d3",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6459"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from typing import defaultdict\n",
    "node_set = set()\n",
    "http_set_s = set()\n",
    "http_set_m = set()\n",
    "http_set_l = set()\n",
    "\n",
    "edge_set = set()\n",
    "string_set_s = set()\n",
    "string_set_m = set()\n",
    "string_set_l = set()\n",
    "\n",
    "image_set = set()\n",
    "num_set = set()\n",
    "poly_set = set()\n",
    "datetime_set = set()\n",
    "date_set = set()\n",
    "year_set = set()\n",
    "point_set = set()\n",
    "\n",
    "text_edge_set_s = set()\n",
    "text_edge_set_m = set()\n",
    "text_edge_set_l = set()\n",
    "\n",
    "image_edge_set = set()\n",
    "num_edge_set = set()\n",
    "spatial_edge_set = set()\n",
    "temporal_edge_set = set()\n",
    "encoder_map = defaultdict(list)\n",
    "\n",
    "i = 0\n",
    "\n",
    "dtype_set = set()\n",
    "\n",
    "def is_date(date_string):\n",
    "    try:\n",
    "        rdflib.term.parse_datetime(date_string)\n",
    "        return 'datetime'\n",
    "    except ValueError:\n",
    "        try:\n",
    "            rdflib.term.parse_xsd_gyear(date_string)\n",
    "            return 'year'\n",
    "        except:\n",
    "            try:\n",
    "                rdflib.term.parse_xsd_date(date_string)\n",
    "                return 'date'\n",
    "            except:\n",
    "                return False\n",
    "\n",
    "def add_str_to_set(string,edge):\n",
    "    if len(string) < 20:\n",
    "        string_set_s.add(string)\n",
    "        text_edge_set_s.add(edge)\n",
    "    elif len(string) < 50:\n",
    "        string_set_m.add(string)\n",
    "        text_edge_set_m.add(edge)\n",
    "    else:\n",
    "        string_set_l.add(string)\n",
    "        text_edge_set_l.add(edge)\n",
    "\n",
    "for s,p,o in graph:\n",
    "    i+=1\n",
    "    pi = p.identifier\n",
    "    for node in [s,o]:\n",
    "        ni = node.identifier\n",
    "        node_set.add(ni)\n",
    "        try:\n",
    "            dtype = node.datatype.identifier\n",
    "            dtype_set.add(dtype)\n",
    "        except AttributeError:\n",
    "            dtype = ''\n",
    "        if 'http' in ni[:200]: #200, because images sometimes have kgbench url attached\n",
    "            if len(ni) < 20:\n",
    "                http_set_s.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "            elif len(ni) < 50:\n",
    "                http_set_m.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "            else:\n",
    "                http_set_l.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "\n",
    "        else:\n",
    "            if is_date(ni) and ('Year' in dtype or 'date' in dtype):\n",
    "                date_type = is_date(ni)\n",
    "                if date_type == 'datetime':\n",
    "                    datetime_set.add(ni)\n",
    "                elif date_type == 'year':\n",
    "                    year_set.add(ni)\n",
    "                elif date_type == 'date':\n",
    "                    date_set.add(ni)\n",
    "                    \n",
    "            elif node.isalnum():\n",
    "                if node.isnumeric():\n",
    "                    if node.isdigit():\n",
    "                        num_set.add(int(node.identifier))\n",
    "                        num_edge_set.add(pi)\n",
    "                    else:\n",
    "                        num_set.add(float(node.identifier))\n",
    "            elif ni.startswith('POINT') or ni.startswith('Point'): #didn't see any points, but according to the paper they can be included.\n",
    "                point_set.add(ni)\n",
    "                spatial_edge_set.add(pi)\n",
    "            elif node.isalpha(): #maybe elif maybe not dunno if it filters out strings with numbers\n",
    "                add_str_to_set(ni.pi)\n",
    "            elif ni.startswith('_9j_'):\n",
    "                image_set.add(ni) #might want to load this to hard drive if memory becomes an issue.\n",
    "                image_edge_set.add(pi)\n",
    "            elif ni.startswith('POLYGON') or ni.startswith('Polygon'):\n",
    "                poly_set.add(ni)\n",
    "                spatial_edge_set.add(pi)                \n",
    "                temporal_edge_set.add(pi)\n",
    "            elif ni.lower().startswith('multipolygon'):\n",
    "                poly_set.add(ni)\n",
    "            elif ni.isascii():\n",
    "                add_str_to_set(ni,pi)\n",
    "            elif ni.isprintable():\n",
    "                add_str_to_set(ascii(ni),pi) #don't know if it's necessary, but it probably can't hurt\n",
    "            else:\n",
    "                add_str_to_set(ascii(ni),pi)\n",
    "\n",
    "                \n",
    "\n",
    "    edge_set.add(pi)\n",
    "\n",
    "    \n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59648f-28b8-4c11-ad6b-9d8b360b53d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c1f2dbe-df3b-421e-a165-16b8e26b118c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as trv\n",
    "\n",
    "\n",
    "transform_temp = trv.Compose([\n",
    "    trv.Resize((224, 224)),\n",
    "    trv.ToTensor(), trv.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5], \n",
    "        std=[0.3, 0.3, 0.3])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04ddc8a4-7f94-4e7d-91be-63fb572b0052",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.115552986850069, 51.8905737268378, 1, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/royal-cookings/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from string import printable\n",
    "import pickle\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# node_map = {i:node for i,node in enumerate(node_set)}\n",
    "# inv_node_map = {node:i for i,node in enumerate(node_set)}\n",
    "\n",
    "edge_map = {i:edge for i,edge in enumerate(edge_set)}\n",
    "inv_edge_map = {edge:i for i,edge in enumerate(edge_set)}\n",
    "\n",
    "\n",
    "def tokenize_string(s, character_map, max_len=5000):\n",
    "    s = s[:max_len]\n",
    "    tokens = [character_map[char] for char in s]\n",
    "    return tokens #\n",
    "\n",
    "def encode_image(encoded_str, transform):\n",
    "    #preprocess for encoder[2](2 layer cnn -->embedding_dim)\n",
    "    image, img_bytes = decode_base64_image(encoded_str)\n",
    "    return transform(image), img_bytes\n",
    "\n",
    "def encode_num(n, max_num, log=False):\n",
    "    v = n/max_num\n",
    "    v = math.log(v) if log else v\n",
    "    return torch.tensor((v),dtype=torch.float32, device=device)\n",
    "\n",
    "def encode_polygon(poly, global_mean_x, global_mean_y, x_max, y_max):\n",
    "    poly_tensor_x = (global_mean_x-torch.tensor([ point[0] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/x_max\n",
    "    poly_tensor_y = (global_mean_y-torch.tensor([ point[1] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/y_max\n",
    "    return torch.stack((poly_tensor_x,poly_tensor_y),dim=0)\n",
    "\n",
    "def encode_point(point, max_x, max_y):\n",
    "    \n",
    "    point = torch.tensor(point,dtype=torch.float32, device=device)\n",
    "    div = torch.tensor((max_x,max_y),dtype=torch.float32, device=device)\n",
    "    return point/div\n",
    "\n",
    "def encode_date(date):\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "        \n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    split_str = date.split('-')\n",
    "    years_str = split_str[0]\n",
    "    month_str = split_str[1]\n",
    "    day_str = split_str[2]\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    months = cyclical(int(month_str), 12)\n",
    "    days = cyclical(int(day_str), 31)\n",
    "    return torch.cat((centuries, decades, years, months, days), dim=0)\n",
    "\n",
    "def encode_year(year):\n",
    "    #preprocess for temporal encoder[4](? layer ffnn -->embedding_dim)\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "        \n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    years_str = year\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    return torch.cat((centuries, decades, years), dim=0)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, strings, character_map, max_length=5000):\n",
    "        # self.strings = strings\n",
    "        self.character_map = character_map\n",
    "        self.max_length = max_length\n",
    "        self.strings = strings\n",
    "        self.ids = []\n",
    "        self.pad = False\n",
    "        ids = []\n",
    "        for string in strings:\n",
    "            try:\n",
    "                self.ids.append(inv_node_map[string])\n",
    "            except KeyError:\n",
    "                self.ids.append(-1) #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strings)\n",
    "\n",
    "    def __getitem__(self, i, pad=True):\n",
    "        s = self.strings[i]\n",
    "        tokens = tokenize_string(s, self.character_map, max_len=self.max_length)\n",
    "\n",
    "        return torch.tensor([self.ids[i]]+tokens, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, encoded_images, transform):\n",
    "        self.encoded_images = encoded_images\n",
    "        self.transform = transform\n",
    "        self.ids = [inv_node_map[encoded_image] for encoded_image in encoded_images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        encoded_str = self.encoded_images[i]\n",
    "        image, _ = decode_base64_image(encoded_str, log_note=f\"inside generator, image nr: {i}\")\n",
    "        image = self.transform(image)\n",
    "        return self.ids[i], image\n",
    "\n",
    "class NumericalDataset(Dataset):\n",
    "    def __init__(self, numbers, log_scale=False):\n",
    "        self.numbers = numbers\n",
    "        self.log_scale = log_scale\n",
    "        self.max_num = max(numbers)\n",
    "        self.ids = [inv_node_map[str(number)] for number in numbers]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        n = self.numbers[i]\n",
    "        norm = n / self.max_num\n",
    "        norm = math.log(norm + 1e-8) if self.log_scale else norm\n",
    "        return self.ids[i], torch.tensor([norm], dtype=torch.float32, device=device)\n",
    "\n",
    "class SpatialDataset(Dataset):\n",
    "    def __init__(self, spatial_data, global_mean_x, global_mean_y, x_max, y_max, ids):\n",
    "        self.spatial_data = spatial_data\n",
    "        self.global_mean_x = global_mean_x\n",
    "        self.global_mean_y = global_mean_y\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "        self.max_len = max([len(data) for data in spatial_data])\n",
    "        self.ids = ids#[inv_node_map[spatial_datum] for spatial_datum in spatial_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spatial_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        spatial = self.spatial_data[i]\n",
    "        spatial_tensors = []\n",
    "        if isinstance(spatial, list):\n",
    "            for x,y,a,b,c in spatial:\n",
    "                \n",
    "                x = (x - self.global_mean_x) / self.x_max\n",
    "                y = (y - self.global_mean_y) / self.y_max\n",
    "                coord_tensor = torch.tensor([x,y,a,b,c], device=device)\n",
    "                spatial_tensors.append(coord_tensor)\n",
    "            spatial_tensor = torch.stack(spatial_tensors,dim=0)\n",
    "        elif isinstance(spatial, tuple):\n",
    "            x, y,_,_,_ = spatial\n",
    "            spatial_tensor = torch.tensor([(x / self.x_max, y / self.y_max,0,0,0)], dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            spatial_tensor = torch.zeros((1, 2), dtype=torch.float32, device=device)\n",
    "        return self.ids[i], spatial_tensor\n",
    "\n",
    "\n",
    "class DateTimeDataset(Dataset):\n",
    "    # todo: check if there are other temporal feature types in the data. i.e. just years etc.\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates\n",
    "        self.ids = [inv_node_map[date] for date in dates]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_date(date)\n",
    "        return self.ids[i], date_feature\n",
    "\n",
    "class YearDataset(Dataset):\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates\n",
    "        self.ids = [inv_node_map[date] for date in dates]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_year(date)\n",
    "        return self.ids[i], date_feature\n",
    "\n",
    "def process_point(point_str, highest_x, highest_y):\n",
    "    if 'POINT' in point_str:\n",
    "        point_str = point_str.split('POINT(')[1].split('))')[0]\n",
    "    elif 'Point' in point_str:\n",
    "        point_str = point_str.split('Point(')[1].split('))')[0]\n",
    "    point_list = point_str.strip(')').strip('(').split()\n",
    "    point = [float(coord) for coord in point_list]\n",
    "    point.extend([0,0,0])\n",
    "    point = tuple(point)\n",
    "    \n",
    "    point_x,point_y,_,_,_ = point\n",
    "    highest_x = point_x if point_x > highest_x else highest_x\n",
    "    highest_y = point_y if point_y > highest_y else highest_y\n",
    "    return point, highest_x, highest_y\n",
    "    \n",
    "\n",
    "def get_num_data(poly_str, max_x, y_max):\n",
    "    poly_str = poly_str.split('(')[-1].split(')')[0]\n",
    "    poly_combi_str_list = [poly for poly in poly_str.split(',')]\n",
    "    try:\n",
    "        poly_tupled = [(float(poly.split()[0]),float(poly.split()[1]),1,0,0) if i < 1 else (float(poly.split()[0]),float(poly.split()[1]),0,0,1) if i == len(poly_combi_str_list)-1 else (float(poly.split()[0]),float(poly.split()[1]),0,1,0)\n",
    "                       for i,poly in enumerate(poly_combi_str_list)]\n",
    "    except ValueError:\n",
    "        poly_tupled = [(float(poly.strip(')').strip('(').split()[0]),float(poly.strip(')').strip('(').split()[1]),1,0,0) if i < 1 else (float(poly.split()[0]),float(poly.split()[1]),0,0,1) if i == len(poly_combi_str_list)-1 else (float(poly.split()[0]),float(poly.split()[1]),0,0,1)\n",
    "                       for i,poly in enumerate(poly_combi_str_list)]\n",
    "    x_max, y_max = max([x for x,y,_,_,_ in poly_tupled]), max([y for x, y,_,_,_ in poly_tupled])\n",
    "\n",
    "    x_max =  x_max if x_max > max_x else max_x\n",
    "    y_max =  y_max if y_max > max_y else max_y\n",
    "    x_mean = sum([tup[0] for tup in poly_tupled])/len(poly_tupled)\n",
    "    y_mean = sum([tup[1] for tup in poly_tupled])/len(poly_tupled)\n",
    "    return poly_tupled, x_mean, y_mean, x_max, y_max\n",
    "\n",
    "global_mean_x = 0\n",
    "global_mean_y = 0\n",
    "\n",
    "max_x = 0\n",
    "max_y = 0\n",
    "\n",
    "node_map = {i: node for i, node in enumerate(node_set)}\n",
    "inv_node_map = {node: i for i, node in enumerate(node_set)}\n",
    "\n",
    "points_tupled = []\n",
    "point_ids = []\n",
    "for i, point in enumerate(point_set):\n",
    "    point_tupled,max_x,max_y = process_point(point,max_x,max_y)\n",
    "    points_tupled.append(point_tupled)\n",
    "    point_ids.append(inv_node_map[point])\n",
    "\n",
    "\n",
    "polys_tupled = []\n",
    "poly_ids = []\n",
    "x_max, y_max = 0,0\n",
    "for i, poly in enumerate(poly_set):\n",
    "    poly_tupled, x_mean, y_mean, x_max, y_max = get_num_data(poly, \n",
    "                                                             x_max, y_max)\n",
    "    global_mean_x += 1\n",
    "    global_mean_y += 1\n",
    "    polys_tupled.append(poly_tupled)\n",
    "    poly_ids.append(inv_node_map[poly])\n",
    "i += 1\n",
    "print(poly_tupled[0])\n",
    "global_mean_x, global_mean_y = global_mean_x/i, global_mean_y/i\n",
    "\n",
    "\n",
    "character_map = {char:i for i,char in enumerate(printable)}\n",
    "character_map['\\x7f'] = 101\n",
    "\n",
    "# node_set.update(string_set_s)\n",
    "# node_set.update(string_set_m)\n",
    "# node_set.update(string_set_l)\n",
    "\n",
    "text_dataset_s = TextDataset(list(string_set_s), character_map)\n",
    "text_dataset_m = TextDataset(list(string_set_m), character_map)\n",
    "text_dataset_l = TextDataset(list(string_set_l), character_map)\n",
    "\n",
    "image_dataset = ImageDataset(list(image_set), transform_temp)\n",
    "\n",
    "numerical_dataset = NumericalDataset(list(num_set), log_scale=True)\n",
    "\n",
    "spatial_dataset = SpatialDataset(polys_tupled + points_tupled, global_mean_x, global_mean_y, x_max, y_max, ids=point_ids + poly_ids)\n",
    "\n",
    "datetime_dataset = DateTimeDataset(list(datetime_set)) #if possible combine\n",
    "year_dataset = YearDataset(list(year_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78dfac-61da-4cd3-82d7-39253078eeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09d10c-29a6-46c9-905b-5363731c689e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09675e30-8e0a-484b-8949-c2ca0f0521f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "byte_str = \"\"\"_9j_4AAQSkZJRgABAQAAAQABAAD_2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL_2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL_wAARCADIAMYDASIAAhEBAxEB_8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL_8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4-Tl5ufo6erx8vP09fb3-Pn6_8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL_8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3-Pn6_9oADAMBAAIRAxEAPwCprVoYUDmLGeoYjIA9hWBHbLOwCRlSGBJPrXea6kQtGYtiQ8Llep5zgGuXdLxpIwwGwbQ2ByR_n-dAHuelqPsUYZgW2jJ7nAq2QAQTwSKpacFFlGTkcA8_SrDNkjJyBmgCpdOoYnOSR2NY05XyJGOSCMDPrV-4fbkBSQQeO5rJuZWNm25SCG5HoKAPPvF00kFhIyShCXA4GSfb2HSuStNd1C3Kxm5MqZIYE5yD1Fdb4vgFzp7omWYygqAOSP6VwLWDxZAjJweSD0oA7Ky1xL6QCX5cgABiARgngH0res9TidnCbgFIBU8ZA64_GvKBJNbSBgoGCRhhkHn-VdBpniWBGVZ0EJGASASPrxzQB61azpLaI0irJGOAc4I9jj0p1xZJMQNxKgcIQRg8etcrpXiSyRZJSzJGynCA7txJx2zg8-oq_wD8JLbIzBTK4BG1wMjp69aAJLiyltc4BZTgYx07ZH481C4LEA8kgEEGtq31KKeMAYdmAyufu57ntVTUEBuA6qFUAnBA4PtigCmHBBCtgAgDIOcYwMfrUlvKDqAXAII2hSCSMY5yDUTqyKDgHcwOPTNOtCZZyxKoRnGBjOBg9-en60Aa0wICFAMAEnH-fapLZmcgkAYAHOe57flUaxBII95IYgkHcDk5B59OaW2YNcADKgEnk-h60AOmJNsGOWJzk9-uetcdrj7JyMndnccDggdK7PG5QoAA5GB0_wA81yevRIt0wODwcd8UAGm3oRyobI8ssQTnJ_zj8qivEMkiYYH5QcgAc4GR_n0rKtWaEs4LbwflIIBAJx_WtX5nRmAYkLgcYPAwcfiD-dAE-naY76hbEszRZ3tkcE4P5V1kduiXKFUUHBIwQNvXqa57RbgGOMHJG3kknAORgc9_8K1Uu5FuRIylQRgnA4OB3H4UAdCoZgUds4OT06_5NFZ0d-H3bNxxjIooAzL-3kn8lQqyEsCy4Bz659O3NcncQONRk5JjVwAFOCef5cela_8AwkFoQWt5ltiyhSuAcnjJPt7VWs7yB7gKLhZVZsvgAsR2Oeg7UAer2ZBtVXp8ox-VTFCTkjGPf2pLJM24YDGQB9OKlQKM7iSTnmgDLuBtBLcnnGKxZ2L20gALAHJFbl8QvBxxnk1izyKljIWcBC2Dgc0AcNrYLxNgEAMBgEA_XmuUFtOGkxMucZIYZGOeP89q63UozcbgoPUYOcAjOP51jX2nsqloywjZcYAxzwPrgmgDmJ8u7EwxkcE8cD6f_XrPn2rlRCpkJyACQAPfNdNbwJEHViTIAcjsD2P5fzqr_ZrvJ5rKWDHO4nt1HXmgDHtoNRQh4Qy5BJUcD8a7HQor29CG6XYgx0AJPbNX9O0xBEiqoGBnGOhPbPX8637OzEAUbQQDk57DigC5bW8VtDtyST6HI6UStvUDIyTkA8ggdKmyHIXJKjoM85qKQA7ARgAkD86AKhGXJfHyYOMc9-1Fkn-kKpcE4J4OM8_T3qaQHLYOGxgEZOf85NNjQxXEbksSCSBwSBQBo7chVA4UnJI6f_XqRIsThy4y3I54HbH54pzozRgHAUHJA5yfT-tMHDAZJAwcnvzQBHI5QMSCcA4zxk1wuoXbvdNwc9eDxjH-NdxOn3hwck8Z6Vw2rWrrcbzjBzyB6mgBkE0krBlKhVbGfUn1_KtyyjJt7gKATjjJwDyelYWnKoLAkABgQCePX866nSlIt5CVAzntwOucdaAOft9WFuk0E2NhIAOf0_L-VbI1RbqERKoIGQcnkDoffNZlzpyT3JthuxI4w6kgryDjHfua2G0qOwmjRGIBTcWducnqMeg5NAE1o-IF-c7sc7TtI-vBopthG8YO4qAckHkcZ_H_ACKKAPLriweQhrS9iJCgnLgZP0NWbeW6tLu2AuFk3SqCAMHJI7irN14ahQhopmjJzwRjAziorLRNShvrdo5VeMTLnAzxkelAH0fZA_YQAcEgEflSsCGGBnA6VPbbBahRxwD-lNc4J2gE0AZU4ad2DAAEYGayLu3AtGBGRuzjGefpW1dy-VC77SccgDrWPJKstmxUkFjkjuPagDidUPkAnIBzxkkj6_8A1vWs4XaSRRqZA5IJIzgD_wCv9PWpPGtwYLcusY2BwMEnuPauStNWuVDMkWRjHCk9e59DyeaAOgjgtGuCxMjEEq2D1HYZ9aswWk8sgiYkorDGeSBwM-9Q6FCb2RNwdCSNzEYU8dgev1roruJrNS6qpjYAZBGRxnnvQBHDETIqIxIB6AEYx-H1rSm2IwYtsA55_wA9DVe2urRCzLMuABg8c-9aA1XS1jBleDkDO4D8snqOOg6UAMidHUMpBBOMgfnTZcBeSc5yABVK78R6NEpAmAkxkBFIB9sHj16d6gTxLpM8YZ5wpBGBtJIx1J_L3oAunCyKWHGM5zzj0qzbIjT5JAAHUnr16EcfyrBfxJYvIxaUshBw4UYB9wKINatyGf7UUyAOAcMO-ff8aAOtX94MA4Ge3YAdB2obbwrEqCQRz29KwV8TafF5cTSFVB3b8_eH8s8frVhNZ0y6mVVvFQE5JckD_P6UAajRCQkEdSc_zrE1TTRKEKKFAJJweuMf4Vt27xyoxilVweFZSMNzjP6U6RGdSuSMjBxQBw6WgEgVVJDNk7TyQSAfrXQ2YP2aRQAQHIx0yeMfTtUMtuttIrlc4JA-mc_jVuBBBb5YnJYsOSOgx0oAoFFGtWyPlQ7EjAyc44wPwH4Vf1uymub6NAFCqocYGSQc5HH41Lp0cLX7zsrb1UnLjhSDjIz35rUkNtLKJDPGqBSFIccj1x6f40AYltMqWy-ZIFUcBgaK6SCOzWBX8xNrc7lIBJ_PpRQB5VqEzSlsNkkAZPYDpUOnorXtv5mQ3mKARxxkVau1XzioAJPGe461Lp8KfaoAACxkXPHTkdKAPa1GYwqnAAA-tWpAI4wAAKhjQJGCMYwOnrSPMWG1h0PBoAzZwSzK2cAE1z8rgI4AIBY4OO1dHclRLIpODt4Getc1cYS2dt2FBJOfSgDifEkDX6GCJwp8wEE9wO3esCDw46c3V6ozwQpyRn8a7DUY1eFGiBVg3XHXOMACkGnLcG2ntUMwEoaVGYKAARwc9e3NAFeDwJaqimeeQqD124BGO-DmnHwJYOw8u-ZI2OAGGST6A556V03203NvMFAQo-0FwMHHfmqxhUoAQTj5sEcAjg4_P9KAMaLwLYzBdt6mSeSYjg_gDxXFap4A8Tf8JC9vBqKG1yGV1kICqTgHaT146Z5PevXLWCRIgzgjJwATz-P4VBMxGvi3U4Row7HHoCcfSgDkLH4XQxkNqGt3N2cYBWMKB-BJz-NaD_C_w9LIHW5vwx4-SRVH5BcV2JQRgEZzjgdv8iobncYSqAgjGUI9PTvQByjfCrRCcLc3wToQsqjJz3G3rU5-GWhNOHie7j2oAUMoZTjjJBGcn2NdGZ1ghBLgA4GSDnJ4xjFSoMxl5M4PAwefxoA5-P4daa4wLmUDoVxkfzqMfDCyG4rfTD0BX_69dZbzqkUjg7huIwDz_npUH2p9rEsSxPHPSgDC0zwuNOMkUWo3AIblSAFyeM9xUkr6vbTlZ4Z5yACro-VYZPAAAAPscVrQvvkk3Pk5HfHGBV6aQCNMHr69DQBw95PrETq4sBKgbJjBKsD9STn8qq3Or-Jb0quk-GCrIAGmuJQFUkc4wRmvQTtBQ4BBOMEdsdRVC9uGhuAUyAy5II47frQByg8MeJtSEY1PxDHCCc-XFESFPcA5HPH6VcT4eCQBLnxJfsDztQAD8yTWnHqEZwC-9xIMKR1GOxJ9-aspeIhZi6tjg44yMDjAzjigCrB8N9IjT97eX0oPTdKAf0FFbUV-HiDJgg_3T_nvmigDy-WTzDuAGCeuK19JjUGKYgMWdQMgYGCOayo1IlZwgKrnI64Jqzp-rG3vhbvEDG0q4GeQSQOD_SgD2UuyxgYyCB0qKUuHO0fU1dRR5CMQAcDpUEhIZjkdOM0AY-oiVpiFIAx1z1rn5gXt5Y2GcHJz39s1vX0reYflBBBHT2rGlRZLVwFIyCRkfrQBgy4EpgdwiPgnknAGD2pNLuUUtbAqxWQggk8qec4P1pFHnapBEMqCwBIGTkketaP9mwW801wmOUCjB6kHv-A_SgBt5P5csazkBpQAY15x6HPvj-VWYYgzqwbCAYCEDj2PtWPqSQ31xHHvKoWUA9SOfXj863Ft_L2YOQDjr79eaALsmSEA7Hg1HDaB_FqSFSwMPzDB44AAz78_lVTxB4gsvCmjm-uwJpHGIYRwWP8AQV5HFr_jrxffzXOlzPaQ_dLRHy0AHQZOSSM__qoA9-lEMShWBJYbcAZ59M-tY98EjkVULgHklxk88YyK8ug1_wCIXht0k1J11azBy8bkMwHcggZBx3Oa9JtNVsPFHhyHU9McgD5JEcYZGGMqR2I-voe9AFc3RRmLIWC5wPX3_Ko7fWXM7RtCTAYwxQnlST9fcH2p0iskTyl8KoJJGMgen1zx-Nc5bKZL5353bCRg9QD0OPb-VAHZhgbePa2QcnGeuKbzk5GCCcdwKhH7uCIAEfLkgY_z61Ij5GCeoIBI6GgB1q5F04JwBjGRzjFXZ2KwgnHHfHNZdo-bpwAcAAdfatCVQYgCuR0x2xigB7znfGuVwzYyTz0zxWdrhEXlkE52gEg4xxTbmWQavYqFzGWAJHqRj-WTTPE52CNcgZAPPIPAoA560lVZi2CXVjh-R6Dj6D3q7e3bKCY5C5B4wACQexwfX-tVFiURs6AEnABBzg9Tmob1VWUMpYBlDKD1PIyD7jHb696ANqyuQjFWkwhX5doyRz3_ADorMtL3zIy7oEQ9CACepooAx21CWJCAgVGO_O3rx6n05_GqVoxfVLeRzwJVPTknIqzNPcyr5TsPLjXy1BAHHfHrz361UtgF1W2GcKZVJyOnIoA-hoXzaq2OwFRyYJOcY9KISGt-CMAA8VG7bcsBnAoAxtQAFyoySMnHoOKyZUcIz-dgg8Dtit6coSwYAEngmsS7jABI5OeAelAHNzvJBd-erhGJJOD1wRjp0rUjRRapKcgMMlWJ4J579uaxbl2efaACxBBA_qO4raeNliigLEBQMkYO7Hrn8aAKy2SGTc4HlnG1gDkY5AH9a6GzgErjYjMccDGM-9Z6OGZIVPAOSB2_Lt2q_YzeVcKpY89R2x6f59KAPJfinLLe-JIrTcfLiAjUHpkkD-ddpPZDw74WhNlbKQpMQHphWJJx1JCk_U5rJ8c6E11fSzRj94p3KRzwec_gea67wtqemeItF-w6gSk8ZBkQEho3Bzkd8Hkg4xgkHjqAc54duLx9cTTbpo51lI3FgDgEEjBAHcDg9Rnpjm3ounf2B8TL3SLZglvfW4uRGR8odSAePcN-g9K6210HQtIuf7TN79oaMloxhAqnBGTtABIBIyemTjFYmiRzav4yu9cmUiMx-TBkYO3IyfXB4A9QM96ANHXtHBicAMDISSVOcEcmua0-xeO4mTBXC9xkkD-XfvXb65LNFFK0eCVBwDnGeAOPbn86wrI4uPML7iRg44z60ARynBQAfwgZJwP88VJuGAeMd8GmXQAkTAHHJx25NSgLtB4CkYxQBDagfbSoBwGAOexwK13AIQLxwAfrzWRagC-K9Bv6gcHArXcAAgLjB_HvQBXESvPCWAGGJHA5IBx_OsnxakjtGMcAAjnrwK2clWjxwCSSTnGMGsPxVcmO9RGzhkGMDuABQBjwGQsWwQpYqDg4PAB6e1WtTslMQlDEKcYTGeP8emahsHWchSWBUnaM-uOfb_8AVW1d7TZEAZAJGc9T3z-tAHOW8RMRUOAQRnBI7fSipLC4WElQOSMsQepBI_xooAp3SIl0zQ7SgO4HHGM8fhxWed0uqI8vJMq8gY5JHarD7y5VlK7TyCc5IJpQmbqAngh1JGehzkZ9-KAPcICFtyMcEDBprbQ5Gc8cg0tuS1vk4JAH8qSYguMYzigDKvEY3AAUYDZ_Ssm9z5DErjkjmtuUk78kbh6Vzc4mfzlZiCSSvpigDmzg3Dy7gpXgBh1raQYjjV1AZUHAJJzgf_X_ADrFtrSe51YW2ThuWJPQZ7fhmulMAWVizKrAZAI569PwoAdYvFIuWARyOFyCce_61oJBGzld-w5GDtz2HSoEhiYq0eARjIBHH19KeAxnywYAjG7jGKAE1G0hlljUMWcDGD9O5_pWFd6Tp5u8snkzjhXTIb8CMED8a6SQK8m_aSFAwR7cislw82thMjYIy-Mck4wDQBBb6PLPIoupZnRSCN7EgD6Enn_Oa6nTESIiJRsVMEYI5Oe4x7Vn2xkCCQ5Jxzk9B9Kv6W58-QNg4GARwefb6UAalzGs4KlQT1yRwR6D8656a0SCVkVBz0JGD_P2rpRgq4wxIGAD0z64rGuFMmCSFIGTnJJPegDnr_jZgAAg8-hyT_WmW77jtA6HIB607VHCRpgA4bBHsf8A9VVLVyZDweBnI_D_AOvQBPA_-luTgEuSQR9OtbGcwgtyTjPGaw7cj7XLnA-bufYGt7JKxkYycAemT1NAEUxYKhUjKg8Z7YJrmfFrYuoSSNwjUknr0HH6V10sQ2gHAG04xxxgiuH8dSCPU4wBgKqjB6YwOaAMW2d3vgq5G7gEHpXTXl25sU5JB5OT1PTp-HSuQhn2SKwIBwOhzn1rXmuDNp5UMRkAggAj_PNAFMTyJIRtzwc4A65oqJXyhLEE8DBbGOvHQ0UAWnX5s55zj1z35pYQXuYycZLA8jBzxnH5VMyBQWBDAHJA9cf4VWtnH2uFg27LjcAehyRj6UAe2wACyDg8lQMH6VBO-CCOoB6VNAVFkMnOAP5VDOSq7woCkdRQBkMSZmkZiF5BB6A1kXTgMSXHzEkYPJArauCRHIpiIJ5VwM596569EZkO4t8nTA65oAztHM8uqzrE4RihwzDOMEehrdjsJWcG5vWZhk_IgAPtznA9-tYmkoBrkxVTtCYJB6ZNdGAThiAB2Gc5oAcYGQHynAU9HJyBgdcAe2MUrl1iYAxM4GRgEfiTz2poQiAgMwUAggcCo8Mcgc5wMYyfQUAPheZwN8CKmc5WTJ-uCB-VV7idLfWYgTgSxMqnryCM8-wq0nmCQKAy57YwK5Dx2jNc6SsMzROWkBIYgtkAY46igDrUJLIsYYBhknOf0q7o5jieYzykMOVDdCCfp-lVLfMcaErkkc-_FSAruQsuTxg9fb_P1oA1LW5K3bxlNoK5Uk5PXOcAfX8qhvJR5hCgg55BGR-FMYkSRys2ABgFQASc9-vHNSXUYIEoOVYc4GDk96AOZ1VCQwweDkkcDt_jVS0jIYE_ebAyTWlqSErnB7jHSqdsBhQMg5x1oAbbD_TpRkj5hkD6CtxcFYgRwrZI9cHNZFuP9KlbGMsMAjnPTNa0IBYAEAY4A6mgCad9zjgcAjr2xXBePoib5SQfmjB_QV3km0FQc7iME_nXG-Oj5t8owAVjAx3HT_EUAcUHYeWCDhVxkdK0hLm32oVIwCCT1rNMXzkkE7R0PQ-1Wo0cxAAZK98dOPT04oAmjiVnYM2G7lcjP5fU0U6CNmkK-aVOMkD5fSigCxM0sSBIvmLMcnpxgA5pbNSJIQUBHmAcd-eue_Jz-FWJY0L_ALzcGBJBXg4xjp3qIPCtxCCGG2QZA4HA_nmgD2uEAxbTzwP5VBeHYqKoBGe_Sp4zmJMAAlRx-FQ3MeWG7nHIAoAxWjeaVpDLtHKlc8dK5e9IgecB1YHqTzmuqzGu87dxBOQfpXLXEKSpO7AqQx474oAbozql5J8wIMeCcdDng1rM4RgY5CW6H0ye4rnNJB-3zAE4CEAE9eRWnI4QIoJJBz6_h9KANe0lRVCkkt0JxgGrGGLMMkDIIJOTWXASdhOBk5I61sI6GY_KR8o9hmgBlpKsVwzOGYkEcnt-Ncr4wSWbVNNmRFJjdjjgDAIODXUbQ84AAIBxisXxLKkUungKGaS7EPPoRg_XoKAN0wKIo0YcgAjHXpSOpxt3M3OASelXXjTgOSSBweuR9KhbETEsQT1475oACSIAwwcgEjPJFWY2D2xXqRyP8KqB9y5yCccKeePpmpreQGMrgLuGcY6Y7CgDJvo2JJY8AZwf6VSt0YKBzwSCR2rSvFUsrMpBAIHJz6-tUIgY3Y88HIyOvagBFYC5myQQGA4-mauQNsuY-Tg8ZxWci_vbgHgkg57cir6DLBsc8HpjGf8A9VAF-Rlfk8kZI_OuI8Zyg6gM8BlAPqOB_gK69nwSSTkdOn-e1cR4vJfVgoOCoHGOvGKAMD5CgUgZGSW5PpWlaRo0bBhnnAI6Gs_DNgE5ABBA4AHFa1ghMXABGAAfx45oAYYRnc77AMqOOvSitKWzZyGABJ5JooAydQlMd0CgySODjgA9s_nzTYVaQRSkciTIB6gcf1FToFeaQSncCOCQO2QP6frTo5RFIUCbiWAUk9OefxxQB7NAR9nRhwSgP44qNpQ5IxyOuaLUjyExk5QZJPtTZAr5U8E9cdaAMe4TEsjEggkYGOlc5dwStJMArNkZ4HQCuouUwrFmHtjvXO3LFJHbeQdpGAcD3FAGNpoQao42ksUIAz06VfYg8EqR39RWbpjH-3CCDjBAJ6cVpMMTMAc5OR1NAFqE4UAckDJ46-hrVspiYyhySQec81jRsRIrrkZAGen6VesmxICp55yT_OgDSjQPONwIUn5iQcjPcetYHjGREXSdsQZv7Rj5JwQCDzx6YHFdBnC5bI78d_wrnfGBfzNKRl-YXiHB9CD_AI0AdKjkRAMA2CRknJHNVp3KPhjuRgcEnOMdv0qxLF5WCDjjkjgZ71RdGAcL82CGXJ69jigCxEVOC3I6A44PH_66toFRCQcEcHPU1Tt8sRwQR68DHcGryKBEVPIxx6DtxQBl3ODGGIOAc5B_D-tVgOCSMEEE49P_ANdWpwDGQRjjn8OapuxABX7p7DpntQBHGm6-uABwApIPuBVxwEZgTgYAB_z06Vn6bIZNUuxnjC9T7f8A1qtzk7mBGCAPw6_40AWR84U5wD69-lcV40eNdVDJnBUZ-oArsUIKovOAuQSe3AriPGcmdUG5eABwPp_n86AMmOMtliGK8ngdPrWxpyM0YBO07gMcDjk4_X_OKw4JQIgNwBbIXJP-eK6HSc7kYuGAAGR0P50AbccYX5im4Hsxxg0VoW0e9dwxzzyfpRQByJhBt8gkkN1z04659v61TkRjeQoo4JyAByTyR_OtOLatoyBcsVBbkcYxge_X9KiEZgmt3GDtAc4HI6nj_PagD1WzBW2jDdQg_lTLhgGB3YIPNELs1rGxySYwSR34qKcEorEjk80AZN5cogJVizE845ArDAe4lkDKDgHknqTW3JGyysAi7MHIA5rI2souZApXaSc-g7frQBjWEZTVlRlO4FiDjqO_1rWnQlt2cEHr1rMsZ2l1mAjaWUMCN2DjFa1ywMhCqQcYxn2oAZEWwCSenPH9K2baJY4POdtxI6AdDWKSI488nJ6Z_CtGzuGnjCMc4GQQOvYZoA1YmV2BIJJBGMVzXihGN7pxUqQLkMQRk9COD6D-tdJDyuQSADkA9qxdfhL6vYKW5UEgcDOc9vagDoZAskRZlySAeQB9c1mzqABg4PYir28tEA2CRnPNVLrBgZtqgggnHUD1oAIHJUAkFhxx1I9frV23dslWyc5OSOlZyASmM8IwGeD1561owsdo3DJ6AgYNAFKcEF-gIzx7YrOBYRDCjIGRz6Vr3SBmkzj8Py_pWaqBFk5zgnnrnnrQBm6a5TXLsE9cdenQVozEgyMDgEA5-hHb6GsaFwmuXDA4BwAcHqFFakpxbswIIPHPXtk0ALbu_nxoSBlSST0xkcY_GuR8cFjq7ZIAUKMYwDkZrrrIJJcRluXVcjmuS8bqTrMuQRhEOc5zwBmgDAiVS6bipAY8Z5HGa6bSAGwAASCDjBPArmrcB2Vs4IPI9hjNdZpEStIhj3AAAMCfb_8AUMfWgDq7ZC0IG5RjHXiim2cohJy-PlA5OP8APSigDkZFMfMYP3CrAdzzz-ff2FSQJC88bOFCAqm0ZzjIyT-v50y3AMSkZACEHjp0Ax-RocBGSQAEA44OckduOnSgD1LYEKquAoXAFMkRgoJAx24pLN_Ns42Ay5QHk-1WhIRGA4BGOBigDBeNjcyYO3PHWuduWl-13ETMSCcnPANddOIDcFkBB7gniuS1AxDUXwWJI5A7HtQBiaaBH4ijCkHBYEEYHT9a6GdC0mQADjPWub04s_iVDllAZie-cCuokwXBPBJxwaAKDucELgBegx0_xrR05HMRcgBCCCc8_wCf8KoTc8KxOTkZFaOnkhNgAwBycHNAGvakhlU5AOMg85rJ1nP_AAkVjhTtAILE8DOcAe5wfyrat0AXKgk46kflWB4hZ01zSo0Bbe7HOem0d_wJoA2GJVAABg8jFRYbaXJ5x1Izn8KlcNgNjAwRnqPWow-FAPcccc-tAFcCKNwzkD_ZI4z7f5xV2MkxkRt3yAc8Zqu6biCuNwPGen-etLkxgsOOMjA7elAFmUZU46lckjpmsmciK4PAIPJxxkf5NalvKJY1KgYwRjr3_wD11m3ykOc8HHB6ZoA5l5MaxMpIBDEc8EgYH8hWmtyDYjdjJPp_n0rFu2KavIRk85HAOeAP8amWUG1KAgMoBweuCcn_AD70Aa2lSGSfJPCqeMc54zWF4yKNqcyk4HlKASehwa0tFlczOq5OUbAP1_xH61i-L2YavKgA-ZQRkY59c0AYFlKMgE5IBwQOT_nFdZo04VkGWwBk5xjJ_wD1CuOgRVjVsZCk5Oeo_wA5rf0kSKwdWG3oAQCD6YoA7SMlnATagC-nvRT9LhMkQJ2jg5785ooA5-MRSQF1XaMBeDx0yT9elVgpVgRkFSMn1yRRRQB6fpYzp8JOT-7Bz36U922IpJIzxRRQBm3ETPcBgxAPp1rn71YY744Rmkwdx7Z7GiigDA0_MevxsCDvLblHGODW3dTlGChhgDJBHXOKKKAIC5bZjjGOM9a1dMYZZTjJIyOgxRRQB0cO3aBtyAQcg859q53xLIqapo8-duZHUYzznA_XI_OiigDTYBosYPQdO1VnQh1-Y8DnB7UUUAPTJbBBOPTvU7LuRjjIAOOcGiigBli-xguCMEg5A5qrqkREm4kAEgAHnPrRRQBxGpuPtbsMZDkEk_jgn8aapYBSqkgxDg5IABzx-RH50UUAbOgBmm3YGAh-YHrzWX4xjaTWX2jBWJWGe4AB_wDrUUUAc_bxJuIJKgA844Pt7cGui0y0CSAFF2smAMHGcg5FFFAHV2rFYwoC9MjKnpRRRQB__9k=\n",
    "https://data.labs.pdok.nl/rce/id/image/20324688 http://dbpedia.org/ontology/thumbnail _9j_4AAQSkZJRgABAQAAAQABAAD_2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL_2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL_wAARCADIAIIDASIAAhEBAxEB_8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL_8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4-Tl5ufo6erx8vP09fb3-Pn6_8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL_8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3-Pn6_9oADAMBAAIRAxEAPwD3FEwAD1pWjXHQVI4A5HWoyw6UAIqD0GRUyog5CgH1xUasM0_eMUAI6K4wyg_Wo1tYgPu5J9TT9-TTwc9KAK7QoGIAwDxTwFUBQOKkwST6U3ABoAjliVkwOD2qt5DknI4q8Oa5_wAT-IY9FtCiENdyDCJ6D1NAGL4u8QjTYGs7Vh9rcYJBzsB7_Wue8Eau1pfvYTufLuDuQt2f_wCuP1ArDmeS5neWZmeRzliTyc96fcSmWWGVVCvGoGV4JIyc_wAvyoA9cxTWqloOprq2lxzEgzKAsgHqO_49a0CtAEBFAXkE-tSFRnmpIkBOT0FADfLPoaKtbB6UUAX24UkmoAfmqVs4NNGDyRzQAxiAeKaXOetDkbqYTQAomHmlOdwUHOOMHI_pU6ydKy0nRtUnhAG-OKNmPf5i3f8A4DVwE0AWi9NDA1EDWNq3iKDSr6O3lzl03AgZA5xyaALmta1Do1kZXIaVuI0zyx_wryu-up768a5uD5kjnJOeB7D0q3qt_Lq2pPNI4ZQSEA6AVTRCJCGOADjpxQARPaxSFrmFnQA5VDg59RyOlS3UWlyRb7OabzMBvKlXsevPTjND20bxusrkIc4IGSPwqJ7aJFBjuUdVXBUghhn2P0oA0PDOo_2XqYVyRBOQrDPQ9j7V6QVyMgjB6Yrx58odygk9RXovhXVv7T0sRuf9IgG1gT1GODQBrkYpyvjtSsOTj1phGDQBLv8AeiouaKANQuCKj3EA4qEOxPFLlsZwcUAOYkmmcnik3c0uc96AOe06Yv411hQcqYYlA_3Qc_qxroxXH6BL5njDUW4xIJCPoGUD9K6q7uorG2aeZgqqMnPf2oAqa9qjaRpE11EqPOoHlRscBjkZHUdj_L1rzZr-bWZZL67eOOVl3LEWwxGOcDvUPiTXbrWdQUIMxB-ASCFUEEk-5yQPrmn2EELxQy3N15aALHwMlVCjnH-etADbdITDMZCQ-P3Z56_hUYDYwGIJA61PcW0UUzC2uVnjGMOBjI-lNiVsgEkL1JI7UASqG8vDEHtwKR4cZLKcEdaeXJbZEQoPcjJ_-tUU0k4IaZ_MQALuAwVwMD-VAEThcH0HtVrRNSbStWjmBJjYhXUdwf69D-FVZyQBggg9D61FII1t0K5EwYk8nHsaAPXg6SruRgwPORSEc1zfgiWafTrgzPI4DgrvJIAI7A9PwrpiDQA3FFPxRQAJOiziIn5ypYDHYED-oqwTkVyXizVrrRjBPabN5YKwdcgggk_qBWRB491EACW0t3HfGQf5mgD0Bl5z60ydjHbSuOCqE5-gJrkofHyEATaey-6Sg_oR_Wp5_G-nyWUyrDOJWQhVcDaSRjkjNAHN-FL9YvEcEjzjbIsvmEg9ME55HqB-VR-LPEc-q3H2W0BWEZwewHcn-grCjgFsW8rYbplAJycRqARk_mc9M_TObCWCz6cEtXkllZjuCqckg4P54NAEAt4YtOR4ZtzswJIPJ-Yd_ersSg2kZIOc9cegAqeLSEiso4bmZY9oAKINzcHp1wPz_CnSLFH5aQoyRjgZbJPueAM_QCgCvEuCxI4B6A9atZJUKOQeT_hUcIGCQAQDzxVhMCTAAwBnp7_zoAswQQOphLDfIvBzgA9h9c1XvLKeBSJ4yMA4AI5x1P0qSQw7WJA8zggY55681HJK11eqbp2CGPCkHABA6d-o_lQBQaIrHsyMHleenqKi2B5ER3CAsAXPQcjk1cn25jZCSucYPWqt2MxkgDBBHFAHpHhW3SDRUVZUlO4guhyCB0rZIwa5zwEAdClAGMTnjrjgV05Q0AR4op-2igDhPHLrJKsYIyrIcZ68NzXLrEDgj0rfvydTu577a6RqmcZAIUE8EEHnn8KxpLhIzkmEKThTIuCT2GaAGpbtKxWMAkEAD1JIGP1qaTS7yAgtAV5wC5AA9STSRkySoAEUlgFMRyWOQQAPTuT2HuQDYnS5KmBy4JwSHyDx7HmgCCOGytoJYipuJJANzZwM4OeepHOKkS4cRiKMLFHyAiDAI9-5_GqrQujEOxHepUQlgQSQKAJEQliSc47USg7o-gAJwKsJEAec5NR3AUGMA5O7GOlAC2ZwrEAHJPapG2-btUAEg4z9aZageUSOMseD9acQxugQMkqTx9aAElRngBXBkGcgHAxUAiLMglB2EjkAcHtmrCA5YFSMdhTST5ZOTgd8UAQXCAuEGQAfTt7VXuogISQQQKtS8mPJyeuc1FdAeQ5yOBQBueFdeXSAba5X_R5WDBx1UkAcjuOK9BjkjniWSJwyMMgg5BryLZmBDgnAGDWpouvXOkTbTuktiRujJ6e49DQB6ViistPFOklFPnNyPSigDymWdbmeTyw6BmBwGOADgkficn8aW6gD28YLkBW3YzyfYZ_nVK0LYG_gkAA9zg8Z-oIrWsoIk3TXBLuHICZyR6D2GcUAO0OwL3k0-zywWByuRjAB4PX0-pPNdNeWkA05btLtmugACPMyRzzg9ax4ZQ8QUgALkADjk5Ofrzj8BVOaV8RqowC2Rxzg0AaAub4glb6YqTxuCtx-INU77X20-VIp5Ecv3Nuhx6E8D9KtqcADPt7Vxnid2bX4QArAKvXHHNAHaLczlQfJs2BAPEJGR-DUzZJe3cNskEMchJwVY4PtyeKW3lP2ePGMbQP0qG4mZZ4njYqwYEEDkHNAE72ktjK0EwG8HJAORzz1qFiROpGeFIqxHcyzM7zyM7gkAtyeOlTJA9xMXjhdwFwSoyAaAKSSMC5GQDURci3Knvk1p_Y5gz7reQD12Gq5tmEDFoXBGeqmgClKcmIAdgDx7VFdoDA_GMDNW7hEiSNnBUDGSeAOKpX2oWsUDGOVHccgD5gSOe30oAtQQSvaoVQsAB0pkltcA5ETj8OlQ3uvaZcukqwvA4UBgo4JHcdKz5NegXlFuDj_AGiP60Aaeyb-63_fJorK_wCElH924_7-_wD1qKALUTxNexrtdcnchxgEYXBxj2PFW7VTJLMSSSZSSSeetZ1mf9NgYEABccdhhsfyq_Yjd5jYyTKRn8TQBYUqpKhg2CMbSCDwOQR1HHX3ps5AEBAxk8nHtT8AM6joHxj04qObpbjoQeKALSgEDBxnnr_9aqF_4XsdTvY72XU2hkVQCn2csOD6jrV5QOCMk1IBnqKADykhUIkgZFGA54BHrg1XlAaeJQRjI5z71l-KgRpqEMwO44wcdqzfCe5YZCzMx80HJJOOlAHSX88tjp9zcQpE8ykBBICVySOwIJ4z3qPw9qd5qcE5nWGJ1YbTBuXjnrknOeKh1tydLuF3hQXHJ6DkVW8Ms0UE4DhuRkg8d_6UAdSlyImKSXVwXUZYCQjA9etC6qvmrEtxc7nXcoM3UHoep9DXnfi-9uIL-MwyuhZQGw2Mj0NVVv4gHLGTdAoBOTnBOKAPQtW1kPpc5MskkZPlkFwec46Y_rXGtt-YADB6c4rmo7yU6oI1kbyXkyUPQ5710BcYOKAEYgKQDk-hHFViAepPPapGPOM_Wm5HX0oAbsHoKKdn_OKKANRbu7tLnIVJArZZSM8EnH8zW5pU9lPCC0jW7mTJyMqSSeB3FVobOe7BmW2IflSOhOMjn_Peon0S_WRJoIWR85IB4JoA35dNuI2kcJ5gMhAMZDAcDGcHIz71XktLl2hAtpjgnkRmq1omt2kUiokis7hmIYjP5Vau7zVnAjEtwgJAJLkdj05oAuJYXeM_Zpsf7hrkPEniWfRdQNsQE46OMEdPX61ufbdTRAGnmyc4XzjzjqaydT0q31W4W4vLOO4uCoyzuenvyKAOZu_Fg1KPyblwFJ42gZB9qbZa-mmLIiOMltwDDnHarOu6FY2ot5La0jgKgk7CTnPTqTTtF0aC9jE01rDN8wUmTqBj0_GgAbxKdVtJ7ZwpBAbI4III_wAa2_DJBtZSvcjnpk85_lVu10iHTj5lpY6coZQH3oCDg55BFXC8s2UtxaRqMcLGFye54HI5oA4vxmMalbjGQQKrGJGa7KgjcF_ABq6jWvCjamy3L30SOmAABkE1S1Cym0u0aUSQozZAZSGJODwAfYGgDjI3U6spA6ODz6YrolkycHtXIWyTTXjTyMW2tkkd-a6K3kZzkUAX-uDjikJwOhxTcsCOh9qUnjBzQA7cPX9KKblaKAO08L6sl5JNG42nIbGeeQB_MH866XCEYU5P1rza1uzaTRyQoqELgHkjtV2XxLqRbAuVUdchQf6UAd0yEkYOMg9Cev8AjWHq8WpowktWDquSQV56Vzra7qL4BvXyQORjAz7iom1e-wVa7kbkcgmgCGXUtSTAZ1V4ycAjoD2zVX7bqLoDvYk-g7Z6VbkuZXJYyMxOSSTn_PSkMvUKWPXJzigCrdtdT6ezXBYkHCk-lS6XpuoXdorWYcruOSGwAfrT7gySadMXlLKpJUHGFyBUNtaPfeXAuvPp6FQBCqg7jzk59f8ACgBmopq-nEi6L7CQAQ-QfyNZj6lcnK-c3UYOSMV1UVloWkwFLzXmnGcv5jA7j7gg1m3eqeBwSkazTSN0MOcg-2TigDEN3cOcNMxGPU0rTzOoDElSOh5qzNpccy-bYOzKRlUZgXH1Axn8KoeTdgMpjfC8HKkAfmKAAJEDnaAc9hjFIysDlWIx0xxTRIBkEnOfwp5KnBDcfWgB_wBquo8fMGwO4zT01GUYMkSse2DjFMY4YHaME_pTvlIJ6Ee1AE_9pr_zwP50VW2j1ooA2WJyGBwo4Hv-FRliELA4JPA9fwpuGUgoCCQCCACSP_retMZyCEAZieSO_vQBIJcggnAJOAO5wO341ds9Lvr6LfDGCnQknBrPwowQoBxznrxzz-dRRavfaVcma3nJUkgo4OGH07UAdInhvUT_AHE4xndSy-Hr6BQ4US8HIQ5Irlbnx5rIkKhmJIBOFOASAcDGOnSnaJ4l1nVNat4Z3mELP83UZA59aAN6ezuINLKso3MckdCB_jVP7LE8CSsAcHHB5B7c9utdD4gu4_KSFWUZALEn8hXMpcxI0kRlXaQTnPGf_rGgCCXTLCSRpHtQzdckZP61raXB4eEgikiWOboAQADj3xVFXdjgAs-B90Zz78VIdKmuUH-iSkg5BKnGPb079-9AG9LqnhzTfvPbKV7AAkfnWXefErSIQyQo056EAYFYU2gWxnP2m0IcHkSHt-PrUlrpOmiZVMSQ57sMj60ASSalaayvmNaRWhY5VkXOc-oH-FVJdGuFRnhAmB6GPnH4da6-28OWIVZC5kGMApgA_lWpbWVpbEGGBVI4DYyfzoA8ya3uo4g_lSFBnJZDgUxLklTlScDHB6V6vLLFEhMzRIncuQB-tctq8vhicmMgNMxwPs4wSf5GgDlvPH-TRVz-yIn-ZZE2nkZnjzj86KALDSsFZcgFfUdcd_T0qe202-njRxbOVkyc44x685qh5-7AALMDk4xz_hXY-Gb8XenrE4AkhJU_TjFAGOnhy_clmVQc5ALgAfgPqelTL4UncgvLApHbk5966onBIIwDQTkYNAHB6po39nygyhWBBKsBgEcf1q14aEUmvRAKuWJJ6-nT-tdRfW6Xdq0UyBlJGPUe4rndLK6HqxSdFHmMTFNj3ORQBF4js2GvzrvHl7VGO54rKgtogC7IGYHnJznFbmuzGfWZZdhwVXHIAyAB1JrKkkjiLM8kaqThhuyceowKAN3TvFdhcIkTMInHBBGPzH-FbSyiRQQxIPQg5FeTX9qnmu0Eu8KchhwRnpxWjoniC9sJkgnkEkfA3g8D2PvzQB6VJDDcx7JolkX0YZx9KxNR8LQ3KH7JII2AOEc8E5z1HNc_c_EEJuWJU4JAwCScfU1lzePb2WMGN2XJIwAByAP8aANQprWhOGUsY-CQSSpHsaq3_ifxFcsVt0WFT0xwT-XP5EVTi1me7UtLLI2AMgsSBVwSjaoUsRnGe2KAMKew1i7k3Xt84J9M5_U5qay0IW0nmmV3YnIJ9vp_nmt3JkUMSAAcMOo59KFAI3qxCk4GBx7nmgCn9k_2n_76P-FFWPtco43t-YooAikOxiyFQAcABeQB0z2yef8AJrQ8NX7Wmrlp2YJKh3E5I45zz7f4VUWeMRFCgZyQSNxHBHTHtWXKzITIoIYkkAZ9wKAPQZvFenLIQFkcAdVXGTn3xWbceO7OEnbCM9i8oU_kM157qBuJ5iFRgMKMcjOBjOPfGapf2VNKSz5GfbpQB3U_xBdyPKEIAP8ACpY_mSBWfPr9zqseDKSAd2NoXGT7ZrnItEmwmX4JwR71p2tmEDDcVx3J9-aALJLOS5ZndjnJPT-XXHpTzbkwlmAAOASSOOBx04_CrdvEpj-4pIIGM8nn_wCvUgjEjuu5SC2cdR0HrQBUSBZFjBViDzkDOfoe9R3Fovm8FVLKeoIBIHsME5B4AFXreBwdyHDR5XAOMjp_I0k6ANuUgkE5BPQd_wCdAGLJosAIf7xORtBIIx1GMU6DTVUiN4ioUkAquckjPOfpW-0Kzq4TLELkBAc5_Ciz0i7mYSLAFUvglztAIAzkn3zQBmQ2QQgsAoIyo6dO5q6AsSghxnHBwCCRnoe3_wBarcumWcU6G61K3jBTdiP5yORwPfmq9zdaRFGEt47q4lJ4LnCkZ6juD_KgBsaMY2TcGLAEk4BB6ke31GKuWelXckKPDauyAljlQFx1HzGov-EjngtyltaWtvhcfKm5umM5NZ1zqF5cqFnvJHwudhYhT2A44oA3ovD0vlJzb_dH_LYf40VymU_54v8An_8AXooAvqJTGQIYuCf4skA9xj0xj8KYEjSUFQrcYJOQPzP-BoooAJgouDgKobkjGVBz9OB3-tJsQxbAAzdz2B9MCiigBOpAIDKepORg446_54pYEXzHiYAAHJPcA9MD09_cUUUAaEdlJLEDFHJKSdo8pSeOxz0B-pqz_Yl6DMXEUaoASZnC8kHgY9gKKKAK9u-lW1w_2rUVKjokCknpjg8jt60y51fSYgVtNMuJyWDb5mxu9RgZoooAiHi3UVUxwrb2iAdI1BYAdOTk5rJuNTnnLuZ5JgWLFSxGSc5P1NFFAEUUpkAYxmMkZAB4x-FWnlbAYMAMYwTySaKKAIYovODBmcYySTgn271NJaKxwsoIGCAXzg_QgUUUAR7X_wCfmP8AMUUUUAf_2Q==\n",
    "https://data.labs.pdok.nl/rce/id/image/20636163 http://dbpedia.org/ontology/thumbnail _9j_4AAQSkZJRgABAQAAAQABAAD_2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL_2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL_wAARCACFAMgDASIAAhEBAxEB_8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL_8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4-Tl5ufo6erx8vP09fb3-Pn6_8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL_8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3-Pn6_9oADAMBAAIRAxEAPwD1LXtJiS2-0pMIZywH-zk9AT_Wsmx8U3ViJLG72pPkFZZclc_7XsfUVxE_xCuNU0trK-G-QkbpVYgSBTkYHABz3796uX-ox6hp1hcxRkER4k5yUPZSfXr-dD0sNam9NIYbyXUruaJvlCxiLAGB2GOvOfzrmDGHkaRsEkk_SqQlLSccAVaEoVCxOABkmjd3C1h0pAQADLscKPU0kNv5KbSdxJyzepot1LMZnGGIwoP8K_4mrsaGRgoBNAFdisaF26Ac1XW4dnVxlFHbNa2t6DfWCQvcJiNgORyFb-6fQ1jbccDtUsD0yLU5NT0K3iEIeO5XypJFYDyyCN2QffkYqjoU872jWvntG9pNtli4IcZyOvTkEcVieGtVFqJbSVsI7Bkz2boR-I_lT59S-weIpZ0HyTBQ49Scc_p_OkBpWKw2-v39lIild3nwEj7ufT8D-lY_ioBta84f8tUBP1HB_kKh1TUGOqpcwSDKqB8v6iqd_ere3CyBSpA2896B2C0iE95DF2dwD9M810_iCZxDJIk8m24ZUEIPynA649cAD8a5a2lEVwkmSNp61fmvzdahb5-aOJgV9-mf5UnuFjqHv7jSNOltkiXyYoABKW_jKgEY9cng_WodCg0z-z2iv1RriQCUKT8wTBwR37ZNZOqX4uzDZBsBpA0h_l_M1e1a9SGwZIVHnTgQpjqF4yPyAFAalfTNFfWftMsEqxRI-2Pfzu__AFDk1kXsRsrxraV13p1weK6G3WPSrcztNJ5UMIDQhsLJJzgn8Sfwx6Vx9xI9xM8sjbnZizE9yeTQgLJPHWhR371SjLRFsAkHtV8KwVGZGXcAwyMZFMLHP6pZfZpfNjH7pz_3yfSqIkdSMOfpmurmjSaJo5BlGGDXK3Vs9pcNE_I6qcdR60xj1uZB1IPrkVPFOZGCFcE-lUlPvVqyGZ846A0AXwMCipQmeaKQHnUd432eSIyrtByEI579DXQW81za6faXdqA5EY3qDnIyeGHf2NcjFL-7lUSAZOdmOT15zWotxJClu8Mm1xbp368HqKqSvoSnbU723mt7yKG5UFlmAfg_MfUfnxn2qPbvmGT8iHJX1Nc5pmrzPeR2yRgecEO1CQCzHsOxJ9Dj1rfd4-VAZJVJ3K4Kt07jtg5ya5W5035G6UZI0o2LYHrXUaZpGowWserW8asyOCkbDJb6Dv8A5xWJ4a01tbvDbhxC4QuCw4I9K6U6ze2cQ0jUpRbrkLHcqnGFbpnt0xnt3FdMZJq6MWrGhb6wNdnlDxw-QYfLlt3OWJyefoP84rj9d0kabdZibdBITsz95fY-v1ro9b0_zW_tG0fyr1Bu3x9JfwHc-veucvNbmvbRop41Lk9QOAPp60CRkBijbgcEcinPLJMxaRizepppHzZOaBSKA-uaTvTsZFJ26UAO_GnAkU0dadQA5ZSrh-rDnmpTcM9ykrtjbjHtiq54ooAu31-9wgiDfuwc_U4qkBk0nO7mrtg0SXCySNtCcijYDV0_SRaWhv72BpXGPKt1GSefvMPQdcdT-la_iS8srxIbWCNnvEOMqMbB6Edz_Ks0apd6gwttOUx5-_M3BUf0_nWrpUEWiXJkMO6FI9z3TkD5s-h6DHQDJ657UhXsck8bRsVcFWBwQRgiql_ZrewbeBIvKH39K6HV5Bq13PeWVpIsKAGR8Egn1PpWP04NMZyG0pIVYEMDgj0NX9OQmRz7AVb1Wy3j7TGPmH3wO49abpKZjckdWxQBdCYHSipioVSxIAAzk0UCPF4n_cSL5nf7m32POf6e9bQ8qSG2R4wSYUw6HDA7fyNYMD_6PIDI3f5NvHQ85_z1rajliSG3LpJvWFTlSCD8vGQen505CEtDINYjQJJNho1CR_fIA3Y_nXSJeLM5WN2lAPMU5xKn41yYmMGpGUsyhWU7lYgj93jORyK0vtXnqGZxOM5_e8kfSReR-NCV9RtnY6VrdxptyJbSRg6MGZG4f_6_Wunt_Fmgzae9vqBuWkmdpJFaNpCGP8QYDg_lXmC3bKgDMSv8InP_AKDIOPzp9xNHNEfOUlgpG2UgFh3AbofwOanlt8I733O9s9cCtImnXck9nFIU2yIVwcZxgjIqvJI0sjO5yzHJNcJHqB0m7BgJKEbtrfxgdQ3qcdG68YOa7WN1kjSRCSjqHU46g9KE7q4WtoSZowfWgc8UZxTAPpRSZpryRxrukcIOmTnr2HFAEg6U7GajZ1jRndtqryWPanBwRwfccUDHY5o-lIG9BThjHWgACgDNHQ4oyTzSHnmgDVsdTjsIfkiJcn5ucfrWnb21zq7LcX8mIRzHbocA_X0_n9K5mN9sikqCB2Per7a1dBWjhPlg8Ar94D60ibHVjV49D0v7HeLA8hDfuYznOScEjGF49Sfxri2lWVyyrtUk4X09qgYMzEsTk8n1NSQD5iKYyYAsvQVWtYUiaZFGAsh_kKugcdKghGJ7j_rp_wCyigDM8S3RtNFl2th5iIgfr1_TNFZHjOcvdWlqCflUufqTgfoDRUvcpbHnMT5tnXe_BJ2bfl-ufWtOZkEcY6P5KYJ7fIO9ZUD5tHG9zjPy7fl-ufWt1ZpQkMJkJj8lRtIBGPLz36VbdmZvYm0eQR66CAW2QpuC4J-6fXj860Ne0uFLZLuFFhmWQqfKj2FgecfUD-dZVgAdfkB2YadlYPnaR5Z4OOa27-8gOmT7HVkjAbCswwAwCnaevBOSK55NqomvI1STiznZZbq1mYJJ5sZAZXxy6kcH_wDXmmm_Cff4XG4xAfKfQkdPyAq2lndSJvSDMZz8qkHrzkDPQ_l9DWfe6XK5UFJIyDkFkPy-v1Fb7qzM9BTPLqd8iRLl3-VF9Se9enQR-TbwxAcRxqmc9SBivP8ASb5NDWUiO3nlY_60gggYHFdVoevRaqHV_LjmDfLGCcsuOvNGmyA289qXOenQCoycDjNVtQuY7TTLmeX7ixHIzjPHT8aCij4j18aRbCOAq105BEZ5AUHknuB_OuK1PV9R1GSGK9lliil2uFCFUCHo2BywrNVzdNP5807zttKYG4uSeQxPPTp74rrtJ0a3s2tLq7aO5sZ8RONxKwSfwhvX0OehPSqtYnc5cRyTTGGa5KwjdtkIdg-MlQF6jPH0zWpYa_rMFs4geSWGPCt5qbhHngAN19cD2rdFhYN_ZkdzDCsVmLh7klAMiM7Bu7nkg1i63pMNhFFdhzDJcsXWyJLMi9VJ78d89CeOlG4zutM1CLUrKOZJC7YCyBsbg3oavqe1eV2OpRwa5byuZPJXYJw-Bk_xdOoB5H0r1Mc47j1FIY_oOlGMmlAp1IBm0Egmpo4WkYKoyTTAMGrVrM8UihE3-3rQBbg0G7mG4tBGvUl5OlS3ugTaZax3LyJIkpwpRWA6Z7jmpjHd3sYX-ynYKQwO_uDkHpyKua1e6jdWcIvLWKKMNlChyWOPqaEIwAOKrxDE9x_vj_0EVaFQRD_SZx6uP_QRQI4LxLNu1-5kOMQjA5_ur_jRVDXZd76pLjO53HT_AGsUVFrmmxycB_0RwHkwOSpHydu_rW4MGSL2hU_-QqxIGxYuoeTGOhHydR0PrWzGQXQE9IP_AGlWkjIfBhdbmwRlblyAR1IQ8VaIxHeo8-5TA2Fbt90g8D8PSq9sGfxFKikgm5bkf7lXLyxng06-YOpLR7RhWwcFQTnAA6fzrCT1t6Gq2IdFuDstl86zjzC6lpl6KGGAeepzwcZ61sLdsnS4sOfSVlrFs7q0tYVTcWKp5YHlA9DksPc-_b0qdtZgjwRE57D5VXP6VsjO5cuZopELtJZ7x3FwOfrleRWBcSCCaG6tiI2I3jYcgMO4PpVu71WQ5QRMo7gOMn24FR6Xplxrt0rlfLtUO1m7Af3R74pbu6Dod_DKZoYpCNpdA2PTIzWD40hEmlwuXkG2ZUwG4IOe31rocAbVAwAABj0rI8T6dJqOkP5buGhHmiMdHIz198ZqijjfD8clzqlutqbWCWEkmSQFhJtywLc9sAcY6CtrxDqMltZysGsvtM37uV7SYsHHfehGPoc5Fc9ps0wnikaEXdtZAERuFxtZvu475JPrziuunt31jTLdJvKQ3Zxa2lvwkfq7kfeKjtwAcDrTYjhIdQu4ZhcLOzuWDHc27JBzyD15wea7-wnjlRwl1pkE064me4laWdsjndnb69OlYVp4VRp42SQTK5mMKMMCQxsBtPoGGenStm_voIdE8_EN7ayKyRw3QBmgcD7uT94KeoPI9TQwOGuHEZ8jZEzbw_nj75GMAZz07_WvVdCjEWi2YUsd0KMSWJySB615oILmQpp-0BZZEkjiyGwWAAOR3xj9a9R0-zFhZRWoleRY1CgueeBjj2pMC-KXHrTRTu9IYDrVy0uPJkA8vf8ATrVIHmr1ncpAw3qSM9aGI05LmG4jVHtrtcMG-VRg4OcH2q7rGr_2lawxfZ5IvLbPzE-mMAdqrPqVtPB5cd_NbvkchG7HpwfwrQ1bVbK70aKCI_vVkBICYGMHocA96LCOc4FQRD_Sp_8AeX_0EVOOtV4uLuf_AHl_9BFAHnOt2cYs7-UO_wB8nHb79FQa00uzUoyxyrtxn0ailco5OM_6ERmX6EfJ_D0PrWxcXUyx-WZpfLWJRt3fLjaOKyVBFkcmTGehHy9unvWxLbkqWMsI-QEJncx-Udu3TvVO3Uh3sWNLiM2r3ahWeR4wy7W2ney9j2rZ1hiumEgbg8ijmQsTjoPTOePauZtrm5ttVma2ZPMLlAXGQFC__Xq5c29zcGOTUbs5GduQE6-mRuP4LWPs25qRpzWVihMyRSsGILISp29M98ew6e9U7qOa5I2htuMk_wB729hXQR2FtEoYwkHs052BvoDlj-AFPmtTFEbgqke3ndINit_wHlm_HFa6ogzdBvYLGf7Pf2kLws3zM65aPtn3HqK9Ftra3tlK28SRqxyQgwCfWvONTVWuV2JjKEke3-cV6LYq8dlbpJw6xqG-oAzSTurhazsWiARimkFgVORkYpw5FKR3FMo838QeGDpSm5iLSWxP-sP3o29D7e9V9P1W406eKS3aSNpYxG7SkFSxPJB7DOD1zxXpc0EVzC0U0YdDjKsOODWJqnhWyu4HNpH9nmPPyHCn6jp-VO4jnLfX7qKO00-BrbzbJnKTO3ythWzyTgg5OPU4rO89LqSW4vVmjM7q29SMMf4sjHfjpwMd66d_AkIhbZeOXAO390Ov51s6f4b0-0jDSQiecj55Jfn59uwFDAydB8OF2i1KYmBlwbeNR0Hq2fUf412AHoKYkYjRVQYUDAGamUcc0tgDFGcUv1pOlAADyMVes5oUdfMAznrjpVIDLUuCDQB0-2K8hC297ZxyZH3yBn25FaGtwWCaPFJBDDHNvAOzk9D3BINcSD0qaFiW_CgRa6moE5up_wDgP_oNTA8VAn_H1P8A8B_lQB574gtyNX1KED_WFiPxGaK0PF8Ai1uKY5CzRA591OP5YoqG7Gi1R58ATZYzJwTwfu_w9Pety5t3R5JHUJGIzguQN3yY4HU1klQLPGXzzwR8o6dD6-v4VoX277RMFHzcAevQVo1czZHYgjWmbZIw84hhFJsc7lwMGuhFn9mkO4LbOeqQ_vJT9Xasmzt86qHmin8oPEwMY-bKrxj3zW4qTMzAKYEzyBy5Puf_ANdZOajuVyt7DUEVsxKx7ZG998jfUmi40XXdRtxPZWD-Q2cNlclh35OT_Kus8GaBZandyCdyiQKZZHYgBl-p78g1sjUJpUGn6aDczktmcqFCLnjjp06ngegoUnLXoDXLoec6N4dliuHn1KJlnjcbVYg7iB944J6HOB0FdIBgc8Vt3GjLaQtLPNvbGWPbPrWMwFWhCjnAp4P1qIH3pwJJz-FMB-KMHoKB70uaABetSbc0xTUgPtQAoAxQGIozRn3pAO6cik6nim556VLGjO2EGTTAcgwA5XKg_ga37TT7S_tt5GCeGKn5lNRaJNA8n2W5CKTjZuGMn8e9aNzpItbjzdOnSG4IOYHYASAdQM_yotclsxr_AEK5so_OUCa27Sp0H19KoRIVY_kK6_StS0_T7KdJ7Zo7sFmZSDlyT0z1H49q5uVxJM74A3HOFGAKBkYJ9KgQ_wCmT_8AAP5VNJIkMTSOcIoyazbK7NxcTuRjJGB6CgCh4wtfO02K5ABaCTBPs3H88UVsXkCXtnNbP0kQqf6H86KTQ00ePOh-yoP3n8XUfL26f59K3L7ZDdOtvF5k7vgE9j6f559cdKWXR7uOOD7TCyNIwCQlSGwf4sdcV0S6fFYys8mZ7kjDk8DHovpTmC1KtrZC1iwELTAAyOOXZh6-g9PTFTKr3JwzfIv8_pU0cm8ZXGTnPGTyMU0IbaXzSSYzgSA849GrBUW3eRbqaWRdg4TyxwmMYFd61notr4btLi3upYVLK8jK_wA7kdVK9D3AHvn3riFQcFa1dG-xHUrcairPahuVBwPx9umfatzMusk-uyB2U21gpyiZ-Z_c-v16elZuqW8FmViRSG65rpddudN0S-uJrVEkupVUJGv3EA_iI6d-B6DsK4uaaSeZ5pXLuxyWNKwIibg8U5eFPapLeF7h9qAnHX2qR4Skhj-81AyIE4ozinmMqMEEEdjTMHNADxThwOlIik4qdIiSAByfWgCMUuwmrCwMsyxsDk46e9Wbmya12SEYjJAz6GmBnlMdantZzbzKx5Tv7ite-0v_AER54xlowGOO696xCPejcR1dxbWF1pf2mR9sajPnJ1X_AD6VLpt6bC-g_tJDIqqUjudp3bDg4OR2wO2R0rm7DUHsZGXAeGUbZYn-64_x9DXRaxr0V_ptvbQIRj5nLDkH0H9aBWKmv6gmo6g0kSqI1-VSBgsPU1kj65pc1lavfC3i8iIgSuOcfwj_AOvTAp6rffaJfJjP7pD1Hc-tM02TFww9VrMBJq3ZttukP1FBR0Aeiq6uKKRJv-MtPgHiXT4Npx5zzlsncxIAwT6DHFc_FaC-nYlgpYkdM4AoopRfur-urB7sz7i0FnfCNW3BuemPSp9g2kYoopjI7JirSwdVj27fYEdPwq6ODkUUUCILvO4Pnr2qDdyB60UUhnZ6JZRDSopOrTMxJ-h2ioNPtYrq_u5ZBzG-FHp1H9KKKlbkvqVLm2V9W8o_dJHGKp30SQ3JVFxgZooqikOsY993EueCf6VrXdrHDe2rDnzGww_ED-tFFIZa1a3SOySdeHjfj8j_AIVeuoUudOuUZcYh80H0IAP9aKKaJZV0W4nkmjDsjW6osLxFeX3EjOc8Y2-lYN7AtrfTwKcrHIyAnuAaKKbEiKJBIzE9FPQVaXAAoopFMSd_JheQDJVc4rjZXeaRpZG3OxyTRRTBAo_SpYOJkI67hRRQBrA9aKKKAP_Z\n",
    "\"\"\"\n",
    "\n",
    "def decode_base64_image(encoded_str,log_note='pass values to decode_base_64_jpg'): # - to +, _ to /\n",
    "    \"\"\"\n",
    "    encoded_str: url safe base 64 jpg string --> image bytes string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_bytes = base64.urlsafe_b64decode(encoded_str)\n",
    "        image_obj = Image.open(BytesIO(image_bytes))\n",
    "        image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "        return image, image_obj\n",
    "    except Exception as e:\n",
    "        logging.error(f\"{e} error encoding image at {log_note}\")\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "def save_bytes_to_jpg(image_bytes, item_num=0,folder='Downloads/ml4g/image_data/',name='Fred'):\n",
    "    filename = f'{folder}{name}_{item_num}.jpg'\n",
    "    with open(filename, 'wb') as img_file:\n",
    "        img_file.write(image_bytes)\n",
    "\n",
    "\n",
    "image, image_bytes = encode_image(byte_str, transform_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab0a94-1c17-4fb4-9e9f-ef3a746b1e5f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09cd4067-852c-4807-bc0b-417be80a827f",
   "metadata": {},
   "source": [
    "# Convert raw values to consistent feature vectors for the encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34660fba-188d-4dfd-902e-b9c2173915e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_spatial(batch):\n",
    "    nodes_ids, batch = zip(*batch)\n",
    "    padded = nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "    padded = padded.permute(0, 2, 1)\n",
    "    return nodes_ids, padded\n",
    "\n",
    "def collate_text(batch, min_size=20):\n",
    "    if batch[0].size(0) < min_size:\n",
    "        pad_num = min_size - batch[0].size(0)\n",
    "        pads = torch.zeros(pad_num, dtype=torch.long, device=device)\n",
    "        batch[0] = torch.cat([batch[0], pads], dim=0)\n",
    "    return nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "spatial_dataloader = DataLoader(spatial_dataset, batch_size=batch_size, collate_fn=collate_spatial)\n",
    "text_dataloader_s = DataLoader(text_dataset_s, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_m = DataLoader(text_dataset_m, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_l = DataLoader(text_dataset_l, batch_size=1, collate_fn=collate_text) #it crashes every time it starts :'(\n",
    "image_dataloader = DataLoader(image_dataset, batch_size=1)\n",
    "datetime_dataloader = DataLoader(datetime_dataset, batch_size=batch_size)\n",
    "year_dataloader = DataLoader(year_dataset, batch_size=batch_size)\n",
    "numerical_dataloader = DataLoader(numerical_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fa37dd0-337b-45ea-8ad0-50b9be965bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del polys_tupled, points_tupled,node_set, http_set, edge_set , string_set, string_set, string_set,image_set ,num_set ,poly_set ,datetime_set ,date_set ,year_set ,point_set ,text_edge_set ,text_edge_set ,text_edge_set ,image_edge_set ,num_edge_set ,spatial_edge_set ,temporal_edge_set ,encoder_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e1e85-5dd0-491e-b949-60ca053ce7f1",
   "metadata": {},
   "source": [
    "# Multimodal Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7198e43b-a426-4ee6-8d74-6ea1b2652130",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import torch.functional as f\n",
    "import torch.nn as nn\n",
    "\n",
    "# ]1] temporal conv\n",
    "# Layer Filters Kernel Padding Pool\n",
    "# 1 64 7 3 max(2/2)\n",
    "# 2 64 7 3 max(2/2)\n",
    "# 3 64 7 3 -\n",
    "# 4 64 7 2 max()\n",
    "# Layer Dimensions\n",
    "# 5 512\n",
    "# 6 128\n",
    "# 7 128\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=102, dropout=0.4, size_type='medium'): #filters = 64\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.size_type = size_type.lower()\n",
    "        self.mlp_hidden_dim = 1024 if self.size_type == 'large' else 512 if self.size_type == 'medium' else 256\n",
    "        self.tcnn_hidden_dim = 128 if self.size_type == 'large' else 64 if self.size_type == 'medium' else 32\n",
    "        self.dilation_vals = (1,1,1) if self.size_type == 'small' else (2,4,8)\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) #used embedding dict instead of ohe to save space\n",
    "                                                            #sparse should probably work too\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            embed_dim, self.tcnn_hidden_dim , kernel_size=7, padding=3, dilation=1)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.pool1 = nn.MaxPool1d(2,2)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.pool2 = nn.MaxPool1d(2,2)\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm3 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        self.conv4 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=2)#, dilation=self.dilation_vals[2])\n",
    "        \n",
    "        self.norm4 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop4 = nn.Dropout(dropout)\n",
    "        self.pool4 = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.lin1 = nn.Linear(self.tcnn_hidden_dim, self.mlp_hidden_dim)\n",
    "        self.lin2 = nn.Linear(self.mlp_hidden_dim, embed_dim)\n",
    "        self.lin3 = nn.Linear(embed_dim , embed_dim)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        residual = embedded\n",
    "        #conv\n",
    "        \n",
    "        x = self.conv1(embedded)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop4(x)\n",
    "        # if residual.size(1) != x.size(1):\n",
    "        #   residual = nn.utils.rnn.pad_sequence([residual, x], batch_first=True)\n",
    "        x = self.pool4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #ffnn\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        # x += residual\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "# [2] MobileNet \n",
    "# Type / Stride Filter Shape Input Size\n",
    "# Conv / s2 3  3  3  32 224  224  3\n",
    "# Conv dw / s1 3  3  32 dw 112  112  32\n",
    "# Conv / s1 1  1  32  64 112  112  32\n",
    "# Conv dw / s2 3  3  64 dw 112  112  64\n",
    "# Conv / s1 1  1  64  128 56  56  64\n",
    "# Conv dw / s1 3  3  128 dw 56  56  128\n",
    "# Conv / s1 1  1  128  128 56  56  128\n",
    "# Conv dw / s2 3  3  128 dw 56  56  128\n",
    "# Conv / s1 1  1  128  256 28  28  128\n",
    "# Conv dw / s1 3  3  256 dw 28  28  256\n",
    "# Conv / s1 1  1  256  256 28  28  256\n",
    "# Conv dw / s2 3  3  256 dw 28  28  256\n",
    "# Conv / s1 1  1  256  512 14  14  256\n",
    "# 5\n",
    "# Conv dw / s1 3  3  512 dw 14  14  512\n",
    "# Conv / s1 1  1  512  512 14  14  512\n",
    "# Conv dw / s2 3  3  512 dw 14  14  512\n",
    "# Conv / s1 1  1  512  1024 7  7  512\n",
    "# Conv dw / s2 3  3  1024 dw 7  7  1024\n",
    "# Conv / s1 1  1  1024  1024 7  7  1024\n",
    "# Avg Pool / s1 Pool 7  7 7  7  1024\n",
    "# FC / s1 1024  1000 1  1  1024\n",
    "# Softmax / s1 Classifier 1  1  1000\n",
    "# Table 2. Resource Per Layer Type\n",
    "# Type Mult-Adds Parameters\n",
    "# Conv 1  1 94.86% 74.59%\n",
    "# Conv DW 3  3 3.06% 1.06%\n",
    "# Conv 3  3 1.19% 0.02%\n",
    "# Fully Connected 0.18% 24.33%\n",
    "\n",
    "#idea: pass identity vector with batches to encoders (don't process, just pass back), use that to map embeddings to nodes later\n",
    "class ImageEncoder(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 256, stride=2, alpha=0.5)\n",
    "        self.image6 = MobileBlock(256, 512, stride=1, alpha=0.5)\n",
    "        self.image7 = MobileBlock(512, 512, stride=2, alpha=0.5)\n",
    "        self.image8 = MobileBlock(512, 1024, stride=1, alpha=0.5)\n",
    "        self.middle_conv = nn.Sequential(*[MobileBlock(\n",
    "                1024, 1024, stride=1, alpha=0.5) for i in range(5)])\n",
    "        \n",
    "        self.image9 = MobileBlock(1024, 1024, stride=1, alpha=0.5)\n",
    "        \n",
    "        self.image10 = MobileBlock(1024, 2048, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(1024,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        # middle_conv = [MobileBlock() for i in range(5)]\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.image(batch)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.image6(x)\n",
    "        x = self.image7(x)\n",
    "        x = self.image8(x)\n",
    "        x = self.middle_conv(x)\n",
    "        x = self.image9(x)\n",
    "        x = self.image10(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class MiniImgEnc(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(MiniImgEnc, self).__init__()\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 512, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(256,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image(x)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        # x = self.lin2(x)\n",
    "        # x = self.norm(x)\n",
    "        return self.act(x)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class MobileBlock(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_size, stride=1, normal_conv=False, alpha=0.5, dilation=1):\n",
    "        super(MobileBlock, self).__init__()\n",
    "        old_in_channels = in_channels\n",
    "        in_channels = int(alpha * in_channels)\n",
    "        embedding_size = int(alpha * embedding_size)\n",
    "        # depthwise conv\n",
    "        self.normal_conv = normal_conv\n",
    "        if self.normal_conv:\n",
    "            # in_channels = in_channels//2\n",
    "            pass\n",
    "        # 3x3x3x32\n",
    "        self.conv = nn.Conv2d(old_in_channels, in_channels, 3, padding=1, stride=2, dilation=1)\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, 3, stride=stride,\n",
    "                                   padding=1, groups=in_channels)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        # pointwise conv\n",
    "        self.pointwise = nn.Conv2d(in_channels, embedding_size, 1, stride=1, padding=0)\n",
    "        self.norm2 = nn.BatchNorm2d(embedding_size)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(self.conv.kernel_size)\n",
    "        if self.normal_conv:\n",
    "            x = self.conv(x)\n",
    "        # print(x.size())\n",
    "        x = self.depthwise(x)\n",
    "        # print(x.size())\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.pointwise(x)\n",
    "        # print(x.size())\n",
    "        x = self.norm2(x)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [3] temporal conv\n",
    "# layer filters kernel padding pool\n",
    "# 1 16 5 2 max(3/3)\n",
    "# 2 32 5 2 -\n",
    "# 3 64 5 2 avg()\n",
    "# layer dimensions\n",
    "# 4 512\n",
    "# 5 128\n",
    "# 6 128\n",
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128, dropout=0.2):\n",
    "        super(SpatialEncoder, self).__init__()\n",
    "        \n",
    "        # temp cnn\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=3, ceil_mode=True)\n",
    "        self.norm1 = nn.BatchNorm1d(16)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm1d(32)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        \n",
    "        # dense\n",
    "        self.lin1 = nn.Linear(64, 512)\n",
    "        self.lin2 = nn.Linear(512, 128)\n",
    "        self.lin3 = nn.Linear(128, embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, spatial):\n",
    "        # x = self.pad(spatial, batch_first=True)\n",
    "        x = self.conv1(spatial)\n",
    "        x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.drop1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #flatten\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# [4] mlp h == in dim 1st col, out 2nd col\n",
    "# XSD:gYear 6 2\n",
    "# XSD:date 10 4\n",
    "# XSD:dateTime 14 6\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super(TemporalEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = input_dim\n",
    "        self.out_dim = 2 if input_dim <= 6 else 4 if input_dim <= 9 else 6 #I messed something up but its very specific...\n",
    "                                                                        #norm_cent returns a digit so idk if that should be 2?\n",
    "                                                                        #there's no sin/cos, so it seems weird to encode it digitwise\n",
    "        num_layers = 2\n",
    "        self.mlp = nn.Sequential(*[nn.Linear(input_dim, input_dim), \n",
    "                                   nn.ReLU(),nn.BatchNorm1d(input_dim)]*num_layers)\n",
    "        self.out = nn.Linear(input_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "\n",
    "#[5] one to one encoding for numerical\n",
    "class NumericalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NumericalEncoder, self).__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8c0f3-7864-4598-a4e8-ecc714a2e6ae",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46ff724b-ac91-443b-8c30-fa9f606d0359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder: text_s\n",
      "encoder 'text_s': processed 1\n",
      "encoder: text_m\n",
      "encoder 'text_m': processed 0\n",
      "encoder: text_l\n",
      "encoder 'text_l': processed 11\n",
      "encoder: image\n",
      "encoder 'image': processed 11\n",
      "encoder: numerical\n",
      "encoder 'numerical': processed 0\n",
      "magic has occurred stay spiffy\n",
      " Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 15 in the list.\n",
      "encoder: spatial\n",
      "encoder 'spatial': processed 0\n",
      "magic has occurred stay spiffy\n",
      " Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 15 in the list.\n",
      "encoder: datetime\n",
      "encoder 'datetime': processed 1\n",
      "magic has occurred stay spiffy\n",
      " Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 15 in the list.\n",
      "encoder: year\n",
      "encoder 'year': processed 0\n",
      "magic has occurred stay spiffy\n",
      " Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 15 in the list.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1512344/2935355595.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    }
   ],
   "source": [
    "text_encoder_s, text_encoder_m, text_encoder_l = TextEncoder(128, size_type='s'), TextEncoder(128, size_type='m'), TextEncoder(128, size_type='m')\n",
    "datetime_encoder = TemporalEncoder(9)\n",
    "spatial_encoder = SpatialEncoder(5)\n",
    "year_encoder = TemporalEncoder(5)\n",
    "image_encoder = ImageEncoder(128)\n",
    "numerical_encoder = NumericalEncoder()\n",
    "\n",
    "class MultiModalEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_matrix,\n",
    "                 encoders_config):\n",
    "        super(MultiModalEncoder, self).__init__()\n",
    "\n",
    "        # self.dataloaders = [config['dataloader'] for config in encoders_config]\n",
    "        self.encoders_config = encoders_config\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.column_map = {'text':(0,128),\n",
    "                           'image':(129,257),\n",
    "                           'numerical':(258,259),\n",
    "                           'spatial':(260, 388),\n",
    "                           'datetime':(389,393),\n",
    "                           'year':(394,396)}\n",
    "        self.x_indices = []#torch.zeros(8, device=device)\n",
    "        self.y_indices = []\n",
    "        self.values = []#torch.zeros((8,128), device=device)\n",
    "\n",
    "        self.num_nodes = feature_matrix.size()[0]\n",
    "\n",
    "    def forward(self):\n",
    "        for config in self.encoders_config:\n",
    "            name = config['name']\n",
    "            encoder = config['encoder'].to(device)\n",
    "            dataloader = config['dataloader']\n",
    "            print(f\"encoder: {name}\")\n",
    "            if 'image' not in name:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    if 'text' in name:\n",
    "                        cols = self.column_map['text']\n",
    "                        node_ids = batch[:, 0]\n",
    "                        features = batch[:, 1:]\n",
    "                    else:\n",
    "                        cols = self.column_map[name]\n",
    "                        node_ids, features = batch\n",
    "                        node_ids = torch.tensor(node_ids,device=device)\n",
    "                    # I confused myself. passing the node_ids through the forward pass was completely unnecessary.\n",
    "                    # change to sparse and use automatic column assignment base on thingy\n",
    "                    try:\n",
    "                        embeddings = encoder(features)\n",
    "                    except ValueError: #if batch size is 0 or 1 when it's supposed to be more.\n",
    "                        x,y = self.column_map[name]\n",
    "                        output_dim = y-x\n",
    "                        embeddings = torch.zeros(y-x, dtype=torch.float32,device=device).unsqueeze(0)\n",
    "                    # embeddings = embeddings.to(device)\n",
    "                    # self.feature_matrix[node_ids, cols[0]:cols[1]] = embeddings\n",
    "                    # sparse assignment\n",
    "\n",
    "                    # if embeddings.size(0) <= 1:\n",
    "                    #     embeddings = embeddings.unsqueeze(0)\n",
    "                    #     print('unsqueezed', embeddings.size())\n",
    "                    batch_size = node_ids.size(0)\n",
    "                    y_cols = torch.arange(cols[0], cols[1], device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "                    # print(y_cols.size())\n",
    "                    embedding_size = embeddings.size(1)\n",
    "                    x_rows = node_ids.unsqueeze(1).repeat(1, embedding_size)\n",
    "                    self.x_indices.append(x_rows)\n",
    "                    self.y_indices.append(y_cols)\n",
    "                    self.values.append(embeddings.view(-1))\n",
    "\n",
    "                    \n",
    "            print(f\"encoder '{name}': processed {i}\")\n",
    "\n",
    "            # del dataset, dataloader\n",
    "            try:\n",
    "                \n",
    "                x = torch.cat(self.x_indices).reshape(-1)\n",
    "    \n",
    "                # print(x.size())\n",
    "                y = torch.cat(self.y_indices).reshape(-1)\n",
    "                # print(y.size())\n",
    "                indices = torch.stack([x, y], dim=0)\n",
    "                # print(indices.size())\n",
    "                values = torch.cat(self.values)\n",
    "                # print(values.size())\n",
    "                self.feature_matrix = torch.sparse_coo_tensor(\n",
    "                    indices=indices,\n",
    "                    values=values,\n",
    "                    size=(self.num_nodes,396)\n",
    "                    , device='cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print('magic has occurred stay spiffy\\n', e)\n",
    "\n",
    "        return self.feature_matrix\n",
    "\n",
    "encoders_config = [{'name': 'text_s', 'encoder': text_encoder_s, 'dataloader': text_dataloader_s},\n",
    "                   {'name': 'text_m', 'encoder': text_encoder_m, 'dataloader': text_dataloader_m},\n",
    "                   {'name': 'text_l', 'encoder': text_encoder_l, 'dataloader': text_dataloader_l},\n",
    "                   {'name': 'image', 'encoder': image_encoder, 'dataloader': image_dataloader},\n",
    "                   {'name': 'numerical', 'encoder': numerical_encoder, 'dataloader': numerical_dataloader},\n",
    "                   {'name': 'spatial', 'encoder': spatial_encoder, 'dataloader': spatial_dataloader},\n",
    "                   {'name': 'datetime', 'encoder': datetime_encoder, 'dataloader': datetime_dataloader},\n",
    "                   {'name': 'year', 'encoder': year_encoder, 'dataloader': year_dataloader},]\n",
    "\n",
    "\n",
    "num_nodes = len(node_map)\n",
    "\n",
    "embed_size = 128 + 128 + 1 + 128 + 4 + 2\n",
    "\n",
    "feature_matrix = torch.zeros((num_nodes, embed_size), dtype=torch.float32, device=device).to_sparse()\n",
    "\n",
    "\n",
    "\n",
    "multi_modal_encoder = MultiModalEncoder(\n",
    "    feature_matrix=feature_matrix,\n",
    "    encoders_config=encoders_config)\n",
    "\n",
    "feature_matrix = multi_modal_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fd9b6-b914-44b8-bdc4-c565e0906194",
   "metadata": {},
   "source": [
    "# Connect to R-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef175e6b-b693-4709-ba84-f3bf9761ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6337, 6337])\n",
      "torch.Size([6337, 396])\n"
     ]
    }
   ],
   "source": [
    "length = len(node_map.keys())\n",
    "\n",
    "indices = torch.zeros((1),device=device)\n",
    "indices = torch.arange(0, length, device=device).repeat(2, 1)\n",
    "\n",
    "values = torch.ones(length, device=device)\n",
    "identity = torch.sparse_coo_tensor(indices, values, (length, length), device=device)\n",
    "print(identity.size())\n",
    "print(feature_matrix.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19c9b30e-69aa-43ac-ad3b-e36dbc66ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6337, 6733]) Great success!\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_cat = torch.cat((identity,feature_matrix),dim=1)\n",
    "print(feature_matrix_cat.size(), 'Great success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acb8ed-bac7-4450-84a5-dedbc8aef71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a9217-e178-4ecc-ae9d-41270a55e860",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87512148-5532-423a-b7b4-999c6ede7bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6459"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from typing import defaultdict\n",
    "adjacency = defaultdict(torch.sparse_coo_tensor)\n",
    "# inv_edge_map\n",
    "indices_x = defaultdict(list)\n",
    "indices_y = defaultdict(list)\n",
    "\n",
    "values = defaultdict(list)\n",
    "i=0\n",
    "for s, p, o in graph:\n",
    "    edge_id = inv_edge_map[p.identifier]\n",
    "    # print(edge_id)\n",
    "    s_id = inv_node_map[s.identifier]\n",
    "    o_id = inv_node_map[o.identifier]\n",
    "    \n",
    "    indices_x[edge_id].append(s_id)\n",
    "    indices_y[edge_id].append(o_id)\n",
    "    values[edge_id].append(1)\n",
    "    i += 1\n",
    "\n",
    "graph_size = max(node_map.keys()) +1\n",
    "\n",
    "for edge_id in indices_x.keys():\n",
    "    ind_x, ind_y = torch.tensor(indices_x[edge_id]), torch.tensor(indices_y[edge_id])\n",
    "    ind = torch.stack((ind_x, ind_y))\n",
    "    val = values[edge_id]\n",
    "    adjacency[edge_id] = torch.sparse_coo_tensor(indices=ind,values=val,size=(graph_size,graph_size),device=device)\n",
    "    \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61b4d3af-c4fc-4645-8680-27c331b6bab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6337"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e02d68b9-3be5-4562-8ef8-e0ae2e34ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great success! 0\n",
      "Great success! 1\n",
      "Great success! 2\n",
      "Great success! 3\n",
      "Great success! 4\n",
      "Great success! 5\n",
      "Great success! 6\n",
      "Great success! 7\n",
      "Great success! 8\n",
      "Great success! 9\n",
      "Great success! 10\n",
      "Great success! 11\n",
      "Great success! 12\n",
      "Great success! 13\n",
      "Great success! 14\n",
      "Great success! 15\n",
      "Great success! 16\n",
      "Great success! 17\n",
      "Great success! 18\n",
      "Great success! 19\n",
      "Great success! 20\n",
      "Great success! 21\n",
      "Great success! 22\n",
      "Great success! 23\n",
      "Great success! 24\n",
      "Great success! 25\n",
      "Great success! 26\n",
      "Great success! 27\n",
      "Great success! 28\n",
      "Great success! 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    1,    2,  ..., 6334, 6335, 6336],\n",
       "                       [   0,    1,    2,  ..., 6334, 6335, 6336]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(6337, 6337), nnz=6338, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_adjacency(A, self=True): #gotta still add self-loops. See if it works without first.\n",
    "    #it still crashes :(\n",
    "    size = A.size()[0]\n",
    "    A = A.coalesce()\n",
    "    # A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "    eye_indices = torch.arange(0,size, device=device).repeat(2,1)\n",
    "    eye_values = torch.ones(size, device=device)\n",
    "    indices = torch.cat((A.indices(), eye_indices), dim=1)\n",
    "    values = torch.cat((A.values(), eye_values), dim=0)\n",
    "    A = torch.sparse_coo_tensor(indices, values, A.size(), device=device).coalesce()\n",
    "    # print('to_dense()?')\n",
    "    degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "    # print('no')\n",
    "    # print(degree.size())\n",
    "    d_inv_sqrt = degree.pow(-0.5)\n",
    "    D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    # A = A.to_dense()\n",
    "    normalized_values = values * d_inv_sqrt[indices[0]] * d_inv_sqrt[indices[1]]\n",
    "    \n",
    "    normalized_A = torch.sparse_coo_tensor(indices, normalized_values, A.size(), device=device).coalesce()\n",
    "    # normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "\n",
    "\n",
    "for i, key in enumerate(adjacency.keys()):\n",
    "    A = adjacency[key]\n",
    "    adjacency[key] = normalize_adjacency(A)\n",
    "    del A\n",
    "    print(f'Great success! {i}')\n",
    "    k = key\n",
    "adjacency[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05eeb229-0bab-46de-980e-34b2f4a2ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6733 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1284, -0.5227, -0.1827, -0.4193],\n",
       "        [-0.6243, -0.8263, -0.6202, -0.4193],\n",
       "        [ 0.2527,  1.7183, -0.6202, -0.4193],\n",
       "        ...,\n",
       "        [-0.2853, -0.8263, -0.4588, -0.4193],\n",
       "        [-0.6243,  0.1299,  2.0749, -0.4193],\n",
       "        [ 0.8495, -0.8263, -0.6202,  0.3866]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##save non-error version\n",
    "def block_diag (weights):\n",
    "    block_diag_matrix = torch.block_diag(*weights).to_sparse()\n",
    "    return block_diag_matrix\n",
    "    \n",
    "\n",
    "class R_GCNLayer(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, num_relations, block_split):\n",
    "        super(R_GCNLayer, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.num_relations = num_relations\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        max_block_size = 1\n",
    "        for i in range(1,11):\n",
    "            if x_dim % i == 0 and y_dim % i == 0:\n",
    "                max_block_size +=1 \n",
    "        x_block_size = x_dim // block_split\n",
    "        y_block_size = y_dim // block_split\n",
    "        self.W = nn.ParameterList()\n",
    "        for _ in range(num_relations):\n",
    "            wr = nn.ParameterList()\n",
    "            # relation_weights = []\n",
    "            for i in range(block_split):\n",
    "                w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                # relation_weights.append(w)\n",
    "             #move blocks to params, not block diag\n",
    "            # print(block_diag_matrix)\n",
    "                wr.append(w)\n",
    "                # self.Wr.append(wr)\n",
    "            self.W.append(wr)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "        for r in range(self.num_relations):\n",
    "            weighted = torch.matmul(X, block_diag(self.W[r])) \n",
    "            transformed = torch.sparse.mm(A[r], weighted)\n",
    "            aggregated += transformed\n",
    "        aggregated += self.bias\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class R_GCN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, y_dim, max_k_hop, num_relations):\n",
    "        super(R_GCN, self).__init__()\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.num_relations = num_relations\n",
    "        self.x_dim = x_dim\n",
    "        self.block_split = 1#max([x for x in range(1,10) if x_dim % x == 0])\n",
    "        print(x_dim,self.block_split)\n",
    "        self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations, self.block_split) for _ in range(max_k_hop)])\n",
    "        self.final_r_gcn = R_GCNLayer(h_dim, y_dim,num_relations, self.block_split) \n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.norm = nn.BatchNorm1d(y_dim)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        # if X.size(0) % 2 !=0:\n",
    "        #     X = X[:-1,:]\n",
    "        # outer layers\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(self.max_k_hop):\n",
    "            H = self.gcns[i](A, X)\n",
    "            H = self.act(H)\n",
    "            H = self.drop(H)\n",
    "        Y = self.final_r_gcn(A, H)\n",
    "        out = self.act(Y)\n",
    "        out = self.norm(out)\n",
    "        return out#nn.LogSoftmax(Y)\n",
    "\n",
    "#feature_matrix.size(0)+1\n",
    "r_gcn = R_GCN(feature_matrix_cat.size(1), 128, 4, max_k_hop=2, num_relations=i+1).to(device)\n",
    "out = r_gcn([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], feature_matrix_cat)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "60c3ec9a-84e0-41dc-97a0-2b8cb5489fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[6310, 6310, 6310,  ..., 6294, 6294, 6294],\n",
       "                       [   0,    1,    2,  ...,  125,  126,  127]]),\n",
       "       values=tensor([-0.1165, -0.2938, -0.1102,  ...,  1.0345, -0.2601,\n",
       "                      -0.3010]),\n",
       "       size=(6337, 396), nnz=9728, layout=torch.sparse_coo,\n",
       "       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6c226473-516c-430b-9108-ef4c06d2d120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6338/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea53cc-d72e-48d2-a3b9-dba17fe6a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4efe3-34ad-48c7-b56e-4d040b9710c7",
   "metadata": {},
   "source": [
    "# Why are these zeros...?           ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "186c855e-463c-4006-afa6-ea8a1508684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_list = [normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3316a6c-75ac-4126-9291-f84f19333d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6733 1\n"
     ]
    }
   ],
   "source": [
    "class MR_GCN(nn.Module):\n",
    "    def __init__(self, A_list, eye, num_nodes, rgcn_inp_dim, embed_size=396, num_classes=4):\n",
    "        super(MR_GCN, self).__init__()\n",
    "        \n",
    "        self.A_list = A_list\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding_matrix = torch.zeros((num_nodes, embed_size), dtype=torch.float32, device=device).to_sparse()\n",
    "        self.MME= MultiModalEncoder(feature_matrix=self.embedding_matrix, encoders_config=encoders_config).to(device)\n",
    "        self.graph_dim = A_list[0].size(0)\n",
    "        self.R_GCN = R_GCN(rgcn_inp_dim, 128, num_classes, max_k_hop=2, num_relations=len(A_list)).to(device)\n",
    "        self.eye = eye\n",
    "        self.feature_matrix = []\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        X = self.MME()\n",
    "        X = torch.cat((self.eye,X), dim=1)\n",
    "        X = self.R_GCN(self.A_list, X)\n",
    "        return X\n",
    "mr_gcn = MR_GCN([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], identity, num_nodes, feature_matrix_cat.size(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8a992be4-5f05-41ef-8ae7-fdbd1537c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder: text_s\n",
      "encoder 'text_s': processed 1\n",
      "encoder: text_m\n",
      "encoder 'text_m': processed 0\n",
      "encoder: text_l\n",
      "encoder 'text_l': processed 11\n",
      "encoder: image\n",
      "encoder 'image': processed 11\n",
      "encoder: numerical\n",
      "encoder 'numerical': processed 0\n",
      "magic has occurred stay spiffy\n",
      " Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 15 in the list.\n",
      "encoder: spatial\n",
      "encoder 'spatial': processed 0\n",
      "magic has occurred stay spiffy\n",
      " Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 15 in the list.\n",
      "encoder: datetime\n",
      "encoder 'datetime': processed 1\n",
      "magic has occurred stay spiffy\n",
      " Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 15 in the list.\n",
      "encoder: year\n",
      "encoder 'year': processed 0\n",
      "magic has occurred stay spiffy\n",
      " Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 15 in the list.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1512344/2935355595.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_ids = torch.tensor(node_ids,device=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6788, -0.7182, -0.6142, -0.3846],\n",
       "        [-0.6788, -0.9592,  1.8218, -0.6536],\n",
       "        [-0.1685, -0.9592,  0.6672,  0.5025],\n",
       "        ...,\n",
       "        [-0.6788, -0.8383, -0.6142, -0.6536],\n",
       "        [-0.6788, -0.0351,  0.2258, -0.6536],\n",
       "        [-0.6788, -0.9592,  0.0503, -0.6536]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_gcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c272ea1-1625-4f5c-9c78-bd430ca27928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
