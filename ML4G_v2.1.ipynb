{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9a3cc9-f2e3-473a-8286-d9377071d310",
   "metadata": {},
   "source": [
    "# Version 2 (sparse matrices...)\n",
    "### I created the last notebook naively and completely missed that the discusses method operates on sparse matrices, so I'm starting again from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c292f-2131-4f8e-a58b-03aff54d508a",
   "metadata": {},
   "source": [
    "# Generating a sparse matrix representation of a graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4159d-e344-4900-9c07-e02cd1556604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508eddc7-743a-4d1e-a43c-aad72c579ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_nodes = 100\n",
    "average_out_degree = 9\n",
    "total_edge_classes = 36\n",
    "embedding_size = 128\n",
    "output_dim = 4\n",
    "\n",
    "\n",
    "\n",
    "# First we generate some arbitrary graph\n",
    "def generate_graph(num_nodes, average_out_degree, total_edge_classes):\n",
    "    num_edges = average_out_degree*num_nodes\n",
    "    indices = torch.randint(0, num_nodes, (2, num_edges))\n",
    "    print(indices.size())\n",
    "    edges = torch.ones(num_edges)\n",
    "    print(edges.size())\n",
    "    A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes))\n",
    "    return A\n",
    "# A = generate_graph(15,3,5).to_dense()\n",
    "\n",
    "# Now maybe it's useful to ensure it's connected...\n",
    "def generate_connected_graph(num_nodes, average_out_degree, total_edge_classes, embedding_size):\n",
    "    edges_set = set()\n",
    "    num_edges = average_out_degree * num_nodes\n",
    "\n",
    "    def sample_random_edge_class():\n",
    "        return torch.randint(0, total_edge_classes, (1,)).item()\n",
    "    \n",
    "    def generate_random_edge():\n",
    "        u, v = torch.randint(0, num_nodes, (2,)).tolist()\n",
    "        #no self loops\n",
    "        while u == v:\n",
    "            u, v = torch.randint(0, num_nodes, (2,)).tolist()\n",
    "        return (u, v)\n",
    "    \n",
    "    def generate_random_node():\n",
    "        return torch.randint(0, num_nodes, (1,)).item()\n",
    "    \n",
    "    def sample_random_node(A):\n",
    "        connected_nodes = A._indices()[1].tolist() #select from incoming to...\n",
    "        if not connected_nodes:\n",
    "            return generate_random_node()\n",
    "        return connected_nodes[torch.randint(0, len(connected_nodes), (1,)).item()]#torch.nonzero(A)\n",
    "    \n",
    "    initial_edge = generate_random_edge()\n",
    "    edges_set.add(initial_edge)\n",
    "    indices = torch.tensor([[initial_edge[0]], [initial_edge[1]]], dtype=torch.long, device=device)\n",
    "    edges = torch.ones(1, device=device)\n",
    "    A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes), device=device)\n",
    "    A_list = []\n",
    "    for edge_class in range(total_edge_classes):\n",
    "        for _ in range(num_edges - 1):\n",
    "            u = sample_random_node(A)\n",
    "            v = generate_random_node()\n",
    "            while u == v:\n",
    "                v = generate_random_node()\n",
    "            new_edge = (u, v)#torch.cat([u, v], dim=0).unsqueeze(1)\n",
    "            if new_edge in edges_set:\n",
    "                continue\n",
    "            edges_set.add(new_edge)\n",
    "            edge_tensor = torch.tensor([[u], [v]], dtype=torch.long, device=device)\n",
    "            indices = torch.cat([indices, edge_tensor], dim=1)\n",
    "            edges = torch.ones(indices.size(1), device=device)\n",
    "            A = torch.sparse_coo_tensor(indices, edges, (num_nodes, num_nodes), device=device)\n",
    "            # indices = torch.stack((indices,torch.cat([u,v]).unsqueeze(1)),dim=0)\n",
    "        # print(edges_set)\n",
    "        A_list.append(A)\n",
    "    random_labels = torch.rand((num_nodes, embedding_size), device=device)\n",
    "    return A_list, random_labels \n",
    "\n",
    "A, X = generate_connected_graph(num_nodes,average_out_degree,total_edge_classes, embedding_size)\n",
    "\n",
    "# Now per edge type? or randomly assign edges...? maybe multinomial_sample(1/k)^k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8687621c-5ccc-4eba-b2e1-9006d1bcd136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c52b3f-618e-4ab2-ac37-251e542ace6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cb952-daaf-4992-8e60-22b83795f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175c831-5866-40bd-8f7c-3ce5e4d1587a",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db5cdd-6881-47d5-92e8-b3742e910537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6300d86c-3c4b-49a5-b447-6f48bce0629d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     normalized_A \u001b[38;5;241m=\u001b[39m D_inv_sqrt \u001b[38;5;241m@\u001b[39m A \u001b[38;5;241m@\u001b[39m D_inv_sqrt\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalized_A\n\u001b[0;32m---> 16\u001b[0m A_prime \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_adjacency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGCNLayer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dim, y_dim):\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mnormalize_adjacency\u001b[0;34m(A, self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_adjacency\u001b[39m(A, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m): \u001b[38;5;66;03m# Self-loop doesn't work with R-GCN\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m     A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(torch\u001b[38;5;241m.\u001b[39meye(size,device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto_sparse(), A)\n\u001b[1;32m      9\u001b[0m     degree \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39msum(A, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dense()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as f\n",
    "\n",
    "k_hop = 3\n",
    "\n",
    "def normalize_adjacency(A, self=True): # Self-loop doesn't work with R-GCN\n",
    "    size = A.size()[0]\n",
    "    A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "    degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "    # print(degree.size())\n",
    "    d_inv_sqrt = degree.pow(-0.5)\n",
    "    D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    A = A.to_dense()\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "A_prime = normalize_adjacency(A)\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        # self.A_norm = A_norm\n",
    "\n",
    "        self.lin = nn.Linear(x_dim, y_dim)\n",
    "\n",
    "    def forward(self, A_norm, X):\n",
    "        device = next(self.parameters()).device\n",
    "        # print(A_norm, X)\n",
    "        transformed = self.lin(X)\n",
    "        aggregated = torch.matmul(A_norm, transformed)\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, y_dim, max_k_hop):\n",
    "        super(GCN, self).__init__()\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.x_dim = x_dim\n",
    "        self.gcns = nn.ModuleList([GCNLayer(x_dim, h_dim) for _ in range(max_k_hop)])\n",
    "        self.final_gcn = GCNLayer(x_dim, y_dim) \n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        # self.norm = nn.LayerNorm(y_dim)\n",
    "    \n",
    "    def forward(self, A, X):\n",
    "        # outer layers\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(self.max_k_hop):\n",
    "            H = self.gcns[i](A, X)\n",
    "            H = self.act(H)\n",
    "            H = self.drop(H)\n",
    "        Y = self.final_gcn(A, H)\n",
    "        out = self.act(Y)\n",
    "        return out#nn.LogSoftmax(Y)\n",
    "model = GCN(X.size(1), embedding_size, output_dim, k_hop).to(device)\n",
    "out = model(A_prime, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870f0f8-f8b3-4003-ac61-f7d6c47a1e46",
   "metadata": {},
   "source": [
    "# R-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b77995-b03b-409e-ade0-c2e40cbc34a9",
   "metadata": {},
   "source": [
    "### Block diagonal weight matrix (one is held in memory for each relational weight per layer, so block diagonal sparse matrices save some memory at the cost of some layer-level information flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72e4257-f40f-44c9-8322-22e4cf8fc3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5196, -1.4885,  1.5626,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.3460,  0.2630,  0.4484,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.5315, -0.1715, -2.4562,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0429,  1.1042,  0.8317],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4913,  0.2087,  1.9447],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0329,  1.9868,  0.5162]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = embedding_size//4 #some partition... ensure it's a round number\n",
    "relation_weights = [torch.randn(block_size, block_size), torch.randn(block_size, block_size)]\n",
    "\n",
    "block_diag_matrix = torch.block_diag(*relation_weights).to_sparse()\n",
    "block_diag_matrix.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10d2a2a-7cb7-4133-9c54-dc0d6022b3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print( embedding_size//4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c8b48-1db0-4837-95f6-cb2978bc6949",
   "metadata": {},
   "source": [
    "# R-GCN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603b2e37-fd0f-4e3d-88a5-ec99fe15d455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5920,  1.1350,  0.0000, -1.1087],\n",
       "        [ 0.6620, -0.4539,  0.0000, -0.0728],\n",
       "        [ 0.0807,  1.4868,  0.0000,  1.1986],\n",
       "        [-0.6910, -0.4539,  0.0000, -1.5395],\n",
       "        [ 0.5642, -0.4539,  0.0000, -0.0079],\n",
       "        [ 1.0080, -0.0392,  0.0000, -0.8843],\n",
       "        [ 0.8802, -0.4539,  0.0000, -0.5808],\n",
       "        [-0.3409,  2.0548,  0.0000,  0.5090],\n",
       "        [ 0.3340, -0.4431,  0.0000,  1.3803],\n",
       "        [-1.4383, -0.4539,  0.0000, -0.4917],\n",
       "        [ 0.7711,  4.2626,  0.0000,  1.4262],\n",
       "        [ 1.4438,  0.7113,  0.0000,  1.2710],\n",
       "        [-0.6211, -0.4539,  0.0000,  0.3680],\n",
       "        [ 0.8349, -0.4539,  0.0000,  0.3027],\n",
       "        [-0.5995,  1.3687,  0.0000, -1.4483],\n",
       "        [ 1.0016,  0.2350,  0.0000, -0.5727],\n",
       "        [-0.1088,  0.0843,  0.0000,  0.6895],\n",
       "        [ 0.5223, -0.1109,  0.0000, -0.4441],\n",
       "        [ 1.5128, -0.4539,  0.0000,  2.0340],\n",
       "        [-1.3691, -0.4539,  0.0000, -0.5100],\n",
       "        [-1.4959,  1.6615,  0.0000, -0.4773],\n",
       "        [-0.0174, -0.4539,  0.0000, -0.3675],\n",
       "        [ 0.8665, -0.4539,  0.0000,  1.1034],\n",
       "        [ 0.7851, -0.4539,  0.0000,  1.7903],\n",
       "        [ 0.4914, -0.3171,  0.0000,  0.1846],\n",
       "        [-0.4312, -0.4539,  0.0000,  0.2317],\n",
       "        [-1.0157, -0.4539,  0.0000, -2.6253],\n",
       "        [-0.5435,  2.4093,  0.0000, -1.5354],\n",
       "        [ 0.2372, -0.4539,  0.0000,  0.1574],\n",
       "        [ 0.1452,  0.5861,  0.0000,  0.0986],\n",
       "        [-1.7352, -0.4539,  0.0000, -0.9218],\n",
       "        [ 1.3646, -0.4539,  0.0000,  0.4646],\n",
       "        [ 0.3722, -0.4539,  0.0000,  1.1255],\n",
       "        [-1.1559, -0.4539,  0.0000, -0.7016],\n",
       "        [ 0.6350, -0.1274,  0.0000,  0.5648],\n",
       "        [ 1.5342, -0.4539,  0.0000,  0.5529],\n",
       "        [-0.1004, -0.4539,  0.0000, -0.8412],\n",
       "        [ 0.2071,  0.5997,  0.0000, -0.7048],\n",
       "        [-0.4282, -0.4539,  0.0000,  0.1962],\n",
       "        [-1.2064, -0.1862,  0.0000, -0.7831],\n",
       "        [ 0.5803, -0.4539,  0.0000,  1.1444],\n",
       "        [-1.0262, -0.4539,  0.0000, -2.0303],\n",
       "        [ 1.5339,  1.7815,  0.0000,  2.7557],\n",
       "        [-0.4598,  0.6633,  0.0000,  0.8054],\n",
       "        [-0.4639, -0.4539,  0.0000, -0.5216],\n",
       "        [-1.4037, -0.4539,  0.0000,  0.0898],\n",
       "        [-0.2414,  0.0820,  0.0000, -0.7488],\n",
       "        [ 0.4882, -0.2662,  0.0000,  0.5901],\n",
       "        [ 0.2468, -0.4539,  0.0000,  0.2632],\n",
       "        [ 0.3512, -0.4539,  0.0000,  0.4217],\n",
       "        [ 2.6850, -0.4539,  0.0000,  0.9848],\n",
       "        [ 0.3992, -0.4539,  0.0000, -0.2605],\n",
       "        [ 0.1457, -0.4539,  0.0000, -0.8288],\n",
       "        [ 0.4903, -0.4539,  0.0000,  0.6990],\n",
       "        [-0.9231, -0.4539,  0.0000,  0.7440],\n",
       "        [-1.7970, -0.4539,  0.0000, -1.8145],\n",
       "        [ 0.7801, -0.4539,  0.0000,  0.5444],\n",
       "        [-0.3789, -0.4539,  0.0000, -1.0684],\n",
       "        [ 0.6134,  2.9140,  0.0000,  0.5642],\n",
       "        [-0.5291, -0.4539,  0.0000, -1.0151],\n",
       "        [-0.4082, -0.2556,  0.0000, -0.0663],\n",
       "        [-1.0890, -0.4539,  0.0000, -0.9278],\n",
       "        [-0.9856, -0.4539,  0.0000, -0.5313],\n",
       "        [ 0.8107, -0.4539,  0.0000,  0.5991],\n",
       "        [-0.5075, -0.4539,  0.0000,  0.9040],\n",
       "        [ 0.2463, -0.0397,  0.0000,  0.0752],\n",
       "        [ 0.7139, -0.4539,  0.0000,  1.6851],\n",
       "        [-0.2451, -0.4539,  0.0000, -0.5008],\n",
       "        [-2.0361, -0.4539,  0.0000,  0.2877],\n",
       "        [ 0.1538,  5.1560,  0.0000,  1.2941],\n",
       "        [ 1.1906, -0.0716,  0.0000,  1.2724],\n",
       "        [-2.5271,  1.8959,  0.0000,  0.0620],\n",
       "        [-1.5916, -0.4539,  0.0000, -0.2759],\n",
       "        [ 0.7491, -0.4539,  0.0000,  0.7515],\n",
       "        [ 0.5406, -0.4539,  0.0000,  0.2361],\n",
       "        [-1.2506, -0.4539,  0.0000, -1.2138],\n",
       "        [-0.4134,  0.7536,  0.0000, -0.5194],\n",
       "        [ 2.8455, -0.4539,  0.0000,  1.2405],\n",
       "        [-1.2196,  0.2280,  0.0000,  0.1067],\n",
       "        [-0.7921, -0.4539,  0.0000, -2.2206],\n",
       "        [-1.2631, -0.4539,  0.0000, -1.1329],\n",
       "        [ 0.8933, -0.4539,  0.0000,  0.3366],\n",
       "        [-1.5710, -0.4539,  0.0000, -0.9677],\n",
       "        [ 1.8808, -0.4539,  0.0000,  1.1966],\n",
       "        [ 0.5947, -0.2363,  0.0000, -1.1344],\n",
       "        [-0.8227, -0.4539,  0.0000, -0.3084],\n",
       "        [ 0.3597, -0.4539,  0.0000,  0.0606],\n",
       "        [-0.5454, -0.4539,  0.0000, -0.3974],\n",
       "        [ 1.6562,  2.2569,  0.0000,  1.3721],\n",
       "        [ 0.4027, -0.4539,  0.0000, -0.2346],\n",
       "        [-0.9937, -0.4539,  0.0000, -3.0189],\n",
       "        [-0.6976, -0.4539,  0.0000,  0.8038],\n",
       "        [ 0.6336, -0.4539,  0.0000,  0.0453],\n",
       "        [-0.2785, -0.4539,  0.0000, -0.4815],\n",
       "        [ 1.1669, -0.4539,  0.0000, -0.3934],\n",
       "        [-0.9054, -0.4539,  0.0000,  0.5963],\n",
       "        [ 0.6921, -0.4539,  0.0000,  0.0098],\n",
       "        [ 0.3143, -0.4539,  0.0000, -0.1241],\n",
       "        [ 0.0166,  0.1774,  0.0000,  0.0932],\n",
       "        [-0.3565, -0.4539,  0.0000,  1.0707]], device='cuda:0',\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_adjacency(A, self=True): # Self-loop doesn't work with R-GCN\n",
    "    size = A.size()[0]\n",
    "    A = torch.add(torch.eye(size,device=device).to_sparse(), A)\n",
    "    degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "    # print(degree.size())\n",
    "    d_inv_sqrt = degree.pow(-0.5)\n",
    "    D_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    A = A.to_dense()\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "# A_prime = normalize_adjacency(A)\n",
    "\n",
    "A_list = [normalize_adjacency(a, False) for a in A]\n",
    "\n",
    "\n",
    "##save non-error version\n",
    "def block_diag (weights):\n",
    "    block_diag_matrix = torch.block_diag(*weights).to_sparse()\n",
    "    return block_diag_matrix\n",
    "    \n",
    "\n",
    "class R_GCNLayer(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, num_relations, block_split):\n",
    "        super(R_GCNLayer, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.num_relations = num_relations\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        max_block_size = 1\n",
    "        for i in range(1,11):\n",
    "            if x_dim % i == 0 and y_dim % i == 0:\n",
    "                max_block_size +=1 \n",
    "        x_block_size = x_dim // block_split\n",
    "        y_block_size = y_dim // block_split\n",
    "        self.W = nn.ParameterList()\n",
    "        for _ in range(num_relations):\n",
    "            wr = nn.ParameterList()\n",
    "            # relation_weights = []\n",
    "            for i in range(block_split):\n",
    "                w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                # relation_weights.append(w)\n",
    "             #move blocks to params, not block diag\n",
    "            # print(block_diag_matrix)\n",
    "                wr.append(w)\n",
    "                # self.Wr.append(wr)\n",
    "            self.W.append(wr)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "#dense version change forward loop such that weigted is matmul, etc of blockdiag(w)\n",
    "# def block_diag (weights):\n",
    "#     block_diag_matrix = torch.block_diag(*weights)\n",
    "#     return block_diag_matrix\n",
    "    \n",
    "\n",
    "# class R_GCNLayer(nn.Module):\n",
    "#     def __init__(self, x_dim, y_dim, num_relations, block_split):\n",
    "#         super(R_GCNLayer, self).__init__()\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.num_relations = num_relations\n",
    "#         gain = nn.init.calculate_gain('relu')\n",
    "#         max_block_size = 1\n",
    "#         for i in range(1,11):\n",
    "#             if x_dim % i == 0 and y_dim % i == 0:\n",
    "#                 max_block_size +=1 \n",
    "#         x_block_size = x_dim // block_split\n",
    "#         y_block_size = y_dim // block_split\n",
    "#         self.W = nn.ParameterList()\n",
    "#         for _ in range(num_relations):\n",
    "#             # wr = nn.ParameterList()\n",
    "#             relation_weights = []\n",
    "#             for i in range(block_split):\n",
    "#                 w = torch.randn(x_block_size, y_block_size)\n",
    "#                 relation_weights.append(w)\n",
    "#                 nn.init.kaiming_uniform_(block_diag_dense, a=gain)\n",
    "#             block_diag_dense = block_diag(relation_weights)\n",
    "#             # print(block_diag_dense)\n",
    "#              #move blocks to params, not block diag\n",
    "#             # print(block_diag_matrix)\n",
    "#                 # wr.append(w)\n",
    "#             self.W.append(block_diag_dense)\n",
    "#             # self.W.append(wr)\n",
    "\n",
    "#         self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "#         nn.init.zeros_(self.bias)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "        # print(self.W[1], self.W[1].size())\n",
    "        for r in range(self.num_relations):\n",
    "            # print(block_diag(self.W[r]).to_dense())\n",
    "            # print(X.size(), self.Wr[r])\n",
    "            weighted = torch.matmul(X, block_diag(self.W[r]))  # (num_nodes, out_dim)\n",
    "            # print(weighted.size())\n",
    "            transformed = torch.sparse.mm(A[r], weighted)\n",
    "            # aggregated_r = torch.matmul(A_norm, transformed)\n",
    "            # print(aggregated.size(), transformed.size())\n",
    "            aggregated += transformed\n",
    "        aggregated += self.bias\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class R_GCN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, y_dim, max_k_hop, num_relations):\n",
    "        super(R_GCN, self).__init__()\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.num_relations = num_relations\n",
    "        self.x_dim = x_dim\n",
    "        self.block_split = 2\n",
    "        self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations, self.block_split) for _ in range(max_k_hop)])\n",
    "        self.final_r_gcn = R_GCNLayer(x_dim, y_dim,num_relations, self.block_split) \n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        # self.norm = nn.LayerNorm(y_dim)\n",
    "        self.norm = nn.BatchNorm1d(y_dim)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        # outer layers\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(self.max_k_hop):\n",
    "            H = self.gcns[i](A, X)\n",
    "            H = self.act(H)\n",
    "            H = self.drop(H)\n",
    "        Y = self.final_r_gcn(A, H)\n",
    "        out = self.act(Y)\n",
    "        out = self.norm(out)\n",
    "        return out#nn.LogSoftmax(Y)\n",
    "model = R_GCN(X.size(1), embedding_size, output_dim, k_hop, total_edge_classes).to(device)\n",
    "out = model(A_list, X)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738743cd-c50b-4103-8130-e28d8746ae56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7295ba-260f-4eb1-86c4-6c1c2dd67825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f738fa3-694a-46dc-bdf6-9438be741943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc738f59-b939-4037-8859-d4e70f0b9150",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea2c0aef-fcd4-4f35-bb0a-23bb5d85e2a2",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                           | 1/600 [00:00<01:34,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "avg loss: 1.5931912660598755 accuracy: 24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                          | 7/600 [00:01<02:08,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "avg loss: 1.3879225254058838 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                         | 12/600 [00:02<01:15,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "avg loss: 1.3931773900985718 accuracy: 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█                                         | 16/600 [00:02<01:09,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15\n",
      "avg loss: 1.3721710443496704 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▍                                        | 21/600 [00:04<02:22,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20\n",
      "avg loss: 1.381373405456543 accuracy: 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▊                                        | 26/600 [00:05<02:34,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25\n",
      "avg loss: 1.3520781993865967 accuracy: 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                       | 31/600 [00:06<02:32,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30\n",
      "avg loss: 1.3763445615768433 accuracy: 31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▌                                       | 36/600 [00:08<02:34,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35\n",
      "avg loss: 1.380155086517334 accuracy: 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                       | 41/600 [00:09<02:29,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40\n",
      "avg loss: 1.3595530986785889 accuracy: 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                      | 46/600 [00:10<02:28,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45\n",
      "avg loss: 1.3274924755096436 accuracy: 37.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▌                                      | 51/600 [00:12<02:30,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50\n",
      "avg loss: 1.360823631286621 accuracy: 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▉                                      | 56/600 [00:13<02:30,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55\n",
      "avg loss: 1.333591341972351 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                     | 61/600 [00:14<02:27,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60\n",
      "avg loss: 1.3560552597045898 accuracy: 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▌                                     | 66/600 [00:16<02:31,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 65\n",
      "avg loss: 1.3014674186706543 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▉                                     | 71/600 [00:17<02:22,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 70\n",
      "avg loss: 1.2964564561843872 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▍                                    | 77/600 [00:18<01:12,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 75\n",
      "avg loss: 1.3276289701461792 accuracy: 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▋                                    | 82/600 [00:19<00:59,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80\n",
      "avg loss: 1.3033359050750732 accuracy: 37.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████                                    | 86/600 [00:19<01:22,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 85\n",
      "avg loss: 1.281274437904358 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▍                                   | 92/600 [00:20<01:18,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90\n",
      "avg loss: 1.3038192987442017 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▊                                   | 97/600 [00:21<00:59,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 95\n",
      "avg loss: 1.2798079252243042 accuracy: 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▉                                  | 102/600 [00:21<00:55,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100\n",
      "avg loss: 1.2892463207244873 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▎                                 | 107/600 [00:22<00:55,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 105\n",
      "avg loss: 1.2962119579315186 accuracy: 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▋                                 | 112/600 [00:23<00:54,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 110\n",
      "avg loss: 1.2696088552474976 accuracy: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▉                                 | 117/600 [00:23<01:18,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 115\n",
      "avg loss: 1.2839549779891968 accuracy: 47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▎                                | 121/600 [00:24<01:43,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120\n",
      "avg loss: 1.2907065153121948 accuracy: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▋                                | 127/600 [00:26<01:27,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 125\n",
      "avg loss: 1.2852177619934082 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████                                | 132/600 [00:26<00:58,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 130\n",
      "avg loss: 1.2596861124038696 accuracy: 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████▎                               | 137/600 [00:27<00:51,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 135\n",
      "avg loss: 1.2619649171829224 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▋                               | 141/600 [00:28<01:26,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 140\n",
      "avg loss: 1.2644131183624268 accuracy: 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▉                               | 146/600 [00:29<02:01,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 145\n",
      "avg loss: 1.2360329627990723 accuracy: 53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████▎                              | 151/600 [00:30<02:03,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 150\n",
      "avg loss: 1.1861118078231812 accuracy: 47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▋                              | 156/600 [00:32<02:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 155\n",
      "avg loss: 1.2239594459533691 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████                              | 162/600 [00:33<01:30,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 160\n",
      "avg loss: 1.3009917736053467 accuracy: 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████▍                             | 167/600 [00:34<00:54,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 165\n",
      "avg loss: 1.28175687789917 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████▊                             | 172/600 [00:34<00:53,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 170\n",
      "avg loss: 1.311435580253601 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████                             | 177/600 [00:35<00:55,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 175\n",
      "avg loss: 1.2474011182785034 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████▍                            | 182/600 [00:35<00:48,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 180\n",
      "avg loss: 1.265802025794983 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████▋                            | 186/600 [00:36<00:48,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 185\n",
      "avg loss: 1.252300500869751 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████                            | 192/600 [00:37<00:59,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 190\n",
      "avg loss: 1.279752492904663 accuracy: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████▍                           | 197/600 [00:38<00:46,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 195\n",
      "avg loss: 1.2423733472824097 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████▊                           | 202/600 [00:38<00:44,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200\n",
      "avg loss: 1.2703264951705933 accuracy: 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████▏                          | 207/600 [00:39<00:43,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 205\n",
      "avg loss: 1.2157301902770996 accuracy: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████▍                          | 211/600 [00:39<01:03,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 210\n",
      "avg loss: 1.2505460977554321 accuracy: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████▊                          | 216/600 [00:41<01:37,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 215\n",
      "avg loss: 1.2124401330947876 accuracy: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████                          | 221/600 [00:42<01:29,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 220\n",
      "avg loss: 1.2335026264190674 accuracy: 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████▍                         | 226/600 [00:43<01:41,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 225\n",
      "avg loss: 1.205970048904419 accuracy: 47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████▊                         | 232/600 [00:44<00:59,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 230\n",
      "avg loss: 1.1566271781921387 accuracy: 53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████▏                        | 237/600 [00:45<00:53,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 235\n",
      "avg loss: 1.2285709381103516 accuracy: 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████▌                        | 242/600 [00:46<00:43,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 240\n",
      "avg loss: 1.2213845252990723 accuracy: 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████▉                        | 247/600 [00:46<00:40,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 245\n",
      "avg loss: 1.1644874811172485 accuracy: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████▏                       | 252/600 [00:47<00:41,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 250\n",
      "avg loss: 1.096948266029358 accuracy: 57.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████▌                       | 257/600 [00:47<00:38,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 255\n",
      "avg loss: 1.3200058937072754 accuracy: 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|█████████████████▉                       | 262/600 [00:48<00:37,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 260\n",
      "avg loss: 1.3156260251998901 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████▏                      | 267/600 [00:49<00:36,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 265\n",
      "avg loss: 1.2527694702148438 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████▌                      | 272/600 [00:49<00:35,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 270\n",
      "avg loss: 1.206191062927246 accuracy: 53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████▉                      | 277/600 [00:50<00:36,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 275\n",
      "avg loss: 1.2287808656692505 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████▎                     | 282/600 [00:50<00:35,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 280\n",
      "avg loss: 1.206937313079834 accuracy: 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████▌                     | 287/600 [00:51<00:34,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 285\n",
      "avg loss: 1.185381293296814 accuracy: 52.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████▉                     | 292/600 [00:51<00:33,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 290\n",
      "avg loss: 1.1275010108947754 accuracy: 52.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████▎                    | 297/600 [00:52<00:32,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 295\n",
      "avg loss: 1.303713321685791 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████▋                    | 302/600 [00:52<00:32,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 300\n",
      "avg loss: 1.2996249198913574 accuracy: 37.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████▉                    | 307/600 [00:53<00:32,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 305\n",
      "avg loss: 1.3181191682815552 accuracy: 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████▎                   | 312/600 [00:53<00:31,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 310\n",
      "avg loss: 1.358932614326477 accuracy: 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████▋                   | 317/600 [00:54<00:30,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 315\n",
      "avg loss: 1.3484885692596436 accuracy: 32.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████                   | 322/600 [00:55<00:30,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 320\n",
      "avg loss: 1.2953979969024658 accuracy: 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████▎                  | 327/600 [00:55<00:30,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 325\n",
      "avg loss: 1.327468752861023 accuracy: 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████▋                  | 332/600 [00:56<00:31,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 330\n",
      "avg loss: 1.2585242986679077 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████                  | 337/600 [00:56<00:28,  9.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 335\n",
      "avg loss: 1.2836192846298218 accuracy: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████▎                 | 342/600 [00:57<00:28,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 340\n",
      "avg loss: 1.3168702125549316 accuracy: 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████▋                 | 347/600 [00:57<00:31,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 345\n",
      "avg loss: 1.3157339096069336 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████                 | 352/600 [00:58<00:28,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 350\n",
      "avg loss: 1.28369140625 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████▍                | 357/600 [00:59<00:26,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 355\n",
      "avg loss: 1.2548044919967651 accuracy: 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████▋                | 362/600 [00:59<00:27,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 360\n",
      "avg loss: 1.2455413341522217 accuracy: 35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████                | 366/600 [01:00<00:27,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 365\n",
      "avg loss: 1.3257793188095093 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████▍               | 372/600 [01:01<00:49,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 370\n",
      "avg loss: 1.3691078424453735 accuracy: 35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████▊               | 377/600 [01:02<00:30,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 375\n",
      "avg loss: 1.3542070388793945 accuracy: 31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████               | 381/600 [01:02<00:38,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 380\n",
      "avg loss: 1.4102771282196045 accuracy: 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████▍              | 387/600 [01:04<00:35,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 385\n",
      "avg loss: 1.355613350868225 accuracy: 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████▋              | 391/600 [01:05<00:50,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 390\n",
      "avg loss: 1.3184797763824463 accuracy: 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████              | 396/600 [01:06<00:45,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 395\n",
      "avg loss: 1.3756871223449707 accuracy: 35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████▍             | 401/600 [01:07<00:44,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 400\n",
      "avg loss: 1.2785409688949585 accuracy: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████▋             | 406/600 [01:08<00:51,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 405\n",
      "avg loss: 1.2995092868804932 accuracy: 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████             | 411/600 [01:09<00:39,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 410\n",
      "avg loss: 1.309653878211975 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████▍            | 417/600 [01:11<00:40,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 415\n",
      "avg loss: 1.3073431253433228 accuracy: 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████▊            | 422/600 [01:11<00:22,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 420\n",
      "avg loss: 1.3481391668319702 accuracy: 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████████▏           | 427/600 [01:12<00:19,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 425\n",
      "avg loss: 1.3344523906707764 accuracy: 31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████▌           | 432/600 [01:13<00:18,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 430\n",
      "avg loss: 1.3168660402297974 accuracy: 38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████▊           | 437/600 [01:13<00:18,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 435\n",
      "avg loss: 1.2968922853469849 accuracy: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████▏          | 442/600 [01:14<00:17,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 440\n",
      "avg loss: 1.3241184949874878 accuracy: 32.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████▌          | 447/600 [01:14<00:17,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 445\n",
      "avg loss: 1.3012275695800781 accuracy: 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████▉          | 452/600 [01:15<00:16,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 450\n",
      "avg loss: 1.2847884893417358 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████▏         | 457/600 [01:15<00:15,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 455\n",
      "avg loss: 1.2644374370574951 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████▌         | 462/600 [01:16<00:15,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 460\n",
      "avg loss: 1.3073806762695312 accuracy: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████▉         | 467/600 [01:16<00:14,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 465\n",
      "avg loss: 1.3118581771850586 accuracy: 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████▎        | 472/600 [01:17<00:14,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 470\n",
      "avg loss: 1.314601182937622 accuracy: 37.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████▌        | 477/600 [01:18<00:13,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 475\n",
      "avg loss: 1.3044672012329102 accuracy: 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████▊        | 481/600 [01:18<00:15,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 480\n",
      "avg loss: 1.4269262552261353 accuracy: 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████▎       | 487/600 [01:20<00:24,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 485\n",
      "avg loss: 1.285253643989563 accuracy: 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████▌       | 491/600 [01:21<00:27,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 490\n",
      "avg loss: 1.2641642093658447 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████▉       | 497/600 [01:22<00:20,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 495\n",
      "avg loss: 1.2692644596099854 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████▏      | 501/600 [01:23<00:18,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500\n",
      "avg loss: 1.2946690320968628 accuracy: 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████▌      | 506/600 [01:24<00:24,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 505\n",
      "avg loss: 1.2675445079803467 accuracy: 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████▉      | 511/600 [01:25<00:16,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 510\n",
      "avg loss: 1.453860878944397 accuracy: 35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████▎     | 516/600 [01:26<00:22,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 515\n",
      "avg loss: 1.3832271099090576 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████▌     | 521/600 [01:28<00:22,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 520\n",
      "avg loss: 1.3401455879211426 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████     | 527/600 [01:29<00:17,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 525\n",
      "avg loss: 1.3338743448257446 accuracy: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▎    | 532/600 [01:30<00:09,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 530\n",
      "avg loss: 1.3001843690872192 accuracy: 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 536/600 [01:31<00:09,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 535\n",
      "avg loss: 1.361900806427002 accuracy: 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████▉    | 541/600 [01:32<00:15,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 540\n",
      "avg loss: 1.2537102699279785 accuracy: 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████▎   | 546/600 [01:33<00:14,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 545\n",
      "avg loss: 1.3000102043151855 accuracy: 37.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████▋   | 551/600 [01:35<00:10,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 550\n",
      "avg loss: 1.2917653322219849 accuracy: 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████▉   | 556/600 [01:36<00:11,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 555\n",
      "avg loss: 1.2859292030334473 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████▎  | 561/600 [01:37<00:11,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 560\n",
      "avg loss: 1.277706265449524 accuracy: 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████▋  | 566/600 [01:39<00:09,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 565\n",
      "avg loss: 1.3203575611114502 accuracy: 47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████  | 572/600 [01:40<00:05,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 570\n",
      "avg loss: 1.278803825378418 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████▍ | 577/600 [01:41<00:03,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 575\n",
      "avg loss: 1.2727084159851074 accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████▊ | 582/600 [01:42<00:02,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 580\n",
      "avg loss: 1.2281014919281006 accuracy: 47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████ | 587/600 [01:42<00:01,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 585\n",
      "avg loss: 1.2286983728408813 accuracy: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████▍| 592/600 [01:43<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 590\n",
      "avg loss: 1.237452507019043 accuracy: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████▊| 597/600 [01:43<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 595\n",
      "avg loss: 1.2395005226135254 accuracy: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [01:44<00:00,  5.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "from tqdm import tqdm\n",
    "criterion = nn.CrossEntropyLoss()#nn.NLLLoss()\n",
    "params = list(model.parameters())\n",
    "# params.extend(list(classifier.parameters()))\n",
    "optimizer = torch.optim.Adam(params,lr=0.01)\n",
    "\n",
    "num_epochs = 600\n",
    "\n",
    "true_labels = torch.randint(0, output_dim, (num_nodes,), dtype=torch.long).to(device)\n",
    "\n",
    "# print(true_labels)\n",
    "\n",
    "def save_gradient_hook(grad):\n",
    "    gradients.append(grad)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):#, desc = f\"epoch {epoch}/{num_epochs}\"):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "\n",
    "    predicted_labels = model(A_list, X)\n",
    "    \n",
    "    loss = criterion(predicted_labels, true_labels)\n",
    "\n",
    "    # for param in params:\n",
    "    #     param.register_hook(save_gradient_hook)\n",
    "    \n",
    "    # if i % batch_size == 0:\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    i += 1\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    _, predicted = torch.max(predicted_labels.data, 1)\n",
    "    total += true_labels.size(0)\n",
    "    correct += (predicted == true_labels).sum().item()\n",
    "    avg_loss = epoch_loss\n",
    "    accuracy = 100 * correct / total\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch: {epoch}\\navg loss: {avg_loss} accuracy: {accuracy}\")\n",
    "        # make_dot(predicted_labels, params=dict(list(model.named_parameters()))).render(\"r_gcn_torchviz\", format=\"png\")\n",
    "\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8288b8-687d-4645-b942-75ab8bebe744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c268095-1f9a-401d-aa33-b27ec4da3408",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92ff5184-21dd-40ee-ad5d-10a0e7632c6c",
   "metadata": {},
   "source": [
    "# I'm confused about the rdf stuff. I don't see any multimodal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a6513d3-0373-42a9-b114-907ea0e5fabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 0, 3, 2, 3, 2, 0, 1, 1, 2, 3, 1, 1, 2, 0, 1, 1, 3, 0, 2, 2, 1, 3,\n",
       "        1, 1, 3, 2, 0, 3, 1, 1, 3, 0, 2, 2, 0, 2, 2, 1, 2, 2, 3, 1, 3, 1, 3, 2,\n",
       "        1, 2, 1, 2, 2, 3, 2, 1, 0, 1, 3, 0, 1, 3, 1, 2, 3, 0, 2, 2, 0, 1, 1, 0,\n",
       "        3, 2, 1, 2, 0, 0, 0, 1, 2, 0, 0, 2, 3, 1, 1, 2, 3, 1, 3, 3, 3, 3, 3, 0,\n",
       "        0, 1, 1, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "365d0e07-a704-4346-b9b4-92d49d45becd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 0, 3, 0, 3, 1, 0, 1, 1, 1, 3, 1, 3, 1, 0, 1, 3, 3, 1, 1, 1, 1, 3,\n",
       "        1, 1, 3, 1, 0, 3, 3, 1, 3, 1, 1, 1, 0, 0, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3,\n",
       "        1, 1, 0, 1, 1, 3, 1, 1, 0, 1, 3, 1, 1, 3, 0, 1, 3, 1, 3, 0, 1, 1, 0, 0,\n",
       "        3, 3, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 3, 1, 0, 0, 3, 0, 3, 3, 1, 3, 3, 0,\n",
       "        0, 1, 1, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef63b0-c9f1-44b7-ae9e-09210b4395c9",
   "metadata": {},
   "source": [
    "# Create graph from n-triple file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b7f1d5-1f5a-44d8-b1f2-764a729e737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph\n",
    "import logging\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "data_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped.nt'\n",
    "folder = './Downloads/ml4g'\n",
    "\n",
    "def create_new_graph(path, batch_size = 1000, test=True):\n",
    "    logging.basicConfig(\n",
    "    filename='rdf_parsing_errors.log',\n",
    "    filemode='w')\n",
    "    \n",
    "    graph = Graph()\n",
    "    # graph = graph.parse(, format='nt')\n",
    "    batch_num = 0\n",
    "    i = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        while True:\n",
    "            batch = []\n",
    "            try:\n",
    "                [batch.append(next(f)) for j in range(batch_size)]\n",
    "                i += j\n",
    "            except:\n",
    "                pass\n",
    "            if not batch:\n",
    "                break\n",
    "            batch_num += 1\n",
    "            nt_string = ''.join(batch)\n",
    "            try:\n",
    "                graph.parse(data=nt_string, format='nt')\n",
    "                if test:\n",
    "                    graph = Graph()\n",
    "            except ParseError as e:\n",
    "                logging.error(f\"in batch: {batch_num}:\\npproblematic data:\\n\\n{batch}\\n\\n\")\n",
    "                check(batch, batch_num, test=test)\n",
    "\n",
    "            if batch_num == 5:\n",
    "                # print(batch)\n",
    "                pass\n",
    "            if batch_num % 10 == 0:\n",
    "                # clear_output()\n",
    "                \n",
    "                pass\n",
    "                # print(f\"{batch_num}/:o?\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "def check(batch, batch_num, test = True):\n",
    "    graph = Graph()\n",
    "    for i, line in enumerate(batch):\n",
    "        try:\n",
    "            graph.parse(line)\n",
    "        except Exception as e:\n",
    "            logging.error(f'in line: {i}:\\n{line}\\n{e}')\n",
    "graph=create_new_graph(data_loc, test=False)\n",
    "# graph = Graph()\n",
    "# graph.parse(data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4645d0-128d-406b-9fff-711c039c4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a7a1df-aa03-4867-b493-41b5e47c30d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777124"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from typing import defaultdict\n",
    "node_set = set()\n",
    "http_set_s = set()\n",
    "http_set_m = set()\n",
    "http_set_l = set()\n",
    "\n",
    "edge_set = set()\n",
    "string_set_s = set()\n",
    "string_set_m = set()\n",
    "string_set_l = set()\n",
    "\n",
    "image_set = set()\n",
    "num_set = set()\n",
    "poly_set = set()\n",
    "datetime_set = set()\n",
    "date_set = set()\n",
    "year_set = set()\n",
    "point_set = set()\n",
    "\n",
    "\n",
    "text_edge_set_s = set()\n",
    "text_edge_set_m = set()\n",
    "text_edge_set_l = set()\n",
    "\n",
    "image_edge_set = set()\n",
    "num_edge_set = set()\n",
    "spatial_edge_set = set()\n",
    "temporal_edge_set = set()\n",
    "encoder_map = defaultdict(list)\n",
    "\n",
    "i = 0\n",
    "\n",
    "dtype_set = set()\n",
    "\n",
    "def is_date(date_string):\n",
    "    try:\n",
    "        # datetime.strptime(date_string, '%Y-%m-%d')\n",
    "        rdflib.term.parse_datetime(date_string)\n",
    "        return 'datetime'\n",
    "    except ValueError:\n",
    "        try:\n",
    "            rdflib.term.parse_xsd_gyear(date_string)\n",
    "            return 'year'\n",
    "        except:\n",
    "            try:\n",
    "                rdflib.term.parse_xsd_date(date_string)\n",
    "                return 'date'\n",
    "            except:\n",
    "                return False\n",
    "\n",
    "def add_str_to_set(string,edge):\n",
    "    if len(string) < 20:\n",
    "        string_set_s.add(string)\n",
    "        text_edge_set_s.add(edge)\n",
    "    elif len(string) < 50:\n",
    "        string_set_m.add(string)\n",
    "        text_edge_set_m.add(edge)\n",
    "    else:\n",
    "        string_set_l.add(string)\n",
    "        text_edge_set_l.add(edge)\n",
    "\n",
    "for s,p,o in graph:\n",
    "    i+=1\n",
    "    pi = p.identifier\n",
    "    for node in [s,o]:\n",
    "        ni = node.identifier\n",
    "        node_set.add(ni)\n",
    "        try:\n",
    "            dtype = node.datatype.identifier\n",
    "            dtype_set.add(dtype)\n",
    "        except AttributeError:\n",
    "            dtype = ''\n",
    "        # if 'http' in ni[:200] and 'geonames' not in ni: #just add geonames to node set I think...\n",
    "        if 'http' in ni[:200]: #200, because images sometimes have kgbench url attached\n",
    "            if len(ni) < 20:\n",
    "                http_set_s.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "            elif len(ni) < 50:\n",
    "                http_set_m.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "            else:\n",
    "                http_set_l.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "\n",
    "        else:\n",
    "            if is_date(ni) and ('Year' in dtype or 'date' in dtype):\n",
    "                date_type = is_date(ni)\n",
    "                if date_type == 'datetime':\n",
    "                    datetime_set.add(ni)\n",
    "                elif date_type == 'year':\n",
    "                    year_set.add(ni)\n",
    "                elif date_type == 'date':\n",
    "                    date_set.add(ni)\n",
    "                    \n",
    "            elif node.isalnum():\n",
    "                if node.isnumeric():\n",
    "                    if node.isdigit():\n",
    "                        num_set.add(int(node.identifier))\n",
    "                        num_edge_set.add(pi)\n",
    "                    else:\n",
    "                        num_set.add(float(node.identifier))\n",
    "            elif ni.startswith('POINT') or ni.startswith('Point'): #didn't see any points, but according to the paper they can be included.\n",
    "                point_set.add(ni)\n",
    "                spatial_edge_set.add(pi)\n",
    "            elif node.isalpha(): #maybe elif maybe not dunno if it filters out strings with numbers\n",
    "                add_str_to_set(ni.pi)\n",
    "            elif ni.startswith('_9j_'):\n",
    "                image_set.add(ni) #might want to load this to hard drive if memory becomes an issue.\n",
    "                image_edge_set.add(pi)\n",
    "                # print(s,p,o) if i < 100 else 0\n",
    "            elif ni.startswith('POLYGON') or ni.startswith('Polygon'):\n",
    "                poly_set.add(ni)\n",
    "                spatial_edge_set.add(pi)\n",
    "                # print(s,p,o) if i < 100 else 0\n",
    "                \n",
    "                temporal_edge_set.add(pi)\n",
    "            elif ni.lower().startswith('multipolygon'):\n",
    "                poly_set.add(ni)\n",
    "                # print(s,p,o) if i < 100 else 0\n",
    "            elif ni.isascii():\n",
    "                add_str_to_set(ni,pi)\n",
    "                # print(s,p,o) if i < 100 else 0\n",
    "            elif ni.isprintable():\n",
    "                add_str_to_set(ascii(ni),pi) #don't know if it's necessary, but it probably can't hurt\n",
    "                # print(s,p,o) if i < 100 else 0\n",
    "            else: #all that's left seems to be monument stories and property description related text. \n",
    "                            #If there's an error later it's probably from here\n",
    "                add_str_to_set(ascii(ni),pi)\n",
    "                # print(s,p,o) if i < 100 else 0\n",
    "                \n",
    "                # print(ni.isalpha(),ni)\n",
    "                \n",
    "\n",
    "    edge_set.add(pi)\n",
    "\n",
    "    \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af346be1-7f10-4990-896b-feddd3dbe8e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtypes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdtypes\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtypes' is not defined"
     ]
    }
   ],
   "source": [
    "dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8493a-ae00-40b9-98bd-7eabfca3da5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64f06f12-1d9e-40ba-b63d-554304c76153",
   "metadata": {},
   "source": [
    "### maybe coalesce features on literal predicates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e48cd630-3bad-4de9-9299-82ff3807f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# doubles = []\n",
    "# literals = set()\n",
    "# decimals = set()\n",
    "\n",
    "# # think about how to properly create the feature matrix...\n",
    "# for s,p,o in graph:\n",
    "#     si,pi,oi = s.identifier, p.identifier, o.identifier\n",
    "#     if isinstance(o, rdflib.term.Literal):\n",
    "#         literals.add(o)\n",
    "#     if \"http://purl.org/dc/terms/\" in p:\n",
    "#         decimals.add((p, o))\n",
    "#     if s.isnumeric():\n",
    "#         decimals.append(s)\n",
    "#     if 'XSD.doubles' in pi:\n",
    "#         doubles.append(oi)\n",
    "#     elif 'XSD.float' in pi:\n",
    "#         floats.append(oi)\n",
    "#     elif 'XSD.decimal' in pi:\n",
    "#         # decimals.append(oi)\n",
    "#         pass\n",
    "#     elif 'XSD.boolean' in pi:\n",
    "#         bools.append(oi)# representation handled differently from reals\n",
    "#     elif 'XSD.time' in pi:\n",
    "#         times.append(oi)\n",
    "#     elif 'XSD.date' in pi:\n",
    "#         dates.append(oi)\n",
    "#     elif 'XSD.gMonth' in pi:\n",
    "#         months.append(oi)\n",
    "        \n",
    "# print(len(doubles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd296fc-e436-4153-9d1f-bee07e8ca7f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# literals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d562a2-13d7-495f-9c4d-c4feb8a1b1e0",
   "metadata": {},
   "source": [
    "# Redo of graph parsing to align with paper method..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf7e24-9e74-4adb-a909-80fc78210444",
   "metadata": {},
   "source": [
    "### rdflib parsing seems highly inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9241f847-b0dc-4f81-a0c9-2199598817a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes = set()\n",
    "# node_set = set()\n",
    "# edge_set = set()\n",
    "\n",
    "# node_dtypes = set()\n",
    "# instance_types = set()\n",
    "\n",
    "# #literals:\n",
    "# point_set = set()\n",
    "# poly_set = set()\n",
    "# string_set_s = set()\n",
    "# string_set_m = set()\n",
    "# string_set_l = set()\n",
    "# img_set = set()\n",
    "# max_str_len = 1000 # for example, lets see what happens to memory...\n",
    "# bool_set = set()\n",
    "# num_set = set()\n",
    "# date_set = set()\n",
    "# year_set = set()\n",
    "# uri_set = set() #maybe use a different encoder for uri if possible... \n",
    "#                 # in the paper it's not done this way, but I think using bert\n",
    "#                 # for non-uri strings makes sense, and the proposed method for uri strings...\n",
    "#                 # also the dimensionality of the uri-safe character vocab is lower than that of any text.\n",
    "# year_set_parsed = set()\n",
    "\n",
    "# def validate_type(func, string):\n",
    "#     try:\n",
    "#         func(string)\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# for s,p,o in graph:\n",
    "#     si,pi,oi = s.identifier, p.identifier, o.identifier #isinstance(node, rdflib.terms.Literal) vs Uriref does not split literals from nodes\n",
    "#     for node in [s,o]: #I think only o can be literal\n",
    "\n",
    "#         literal = node\n",
    "#         try:\n",
    "#             dtype = literal.datatype.identifier\n",
    "#         except:\n",
    "#             pass #bypass random meaningless error\n",
    "#         dtypes.add(dtype)\n",
    "#         li = literal.identifier\n",
    "#         if dtype == 'http://www.w3.org/2001/XMLSchema#string':\n",
    "#             if len(li) < 20:\n",
    "#                 string_set_s.add(li)\n",
    "#             elif len(li) < 50:\n",
    "#                 string_set_m.add(li)\n",
    "#             else:\n",
    "#                 string_set_l.add(li[:max_str_len])\n",
    "        # elif dtype =='http://www.opengis.net/ont/geosparql#wktLiteral' or pi == 'http://data.pdok.nl/def/pdok#asWKT-RD': #poly, point\n",
    "        #     if 'point' in li.lower():\n",
    "        #         point_set.add(li)\n",
    "        #     elif 'polygon' in li.lower():\n",
    "        #         poly_set.add(li)\n",
    "        # elif ('_9j_' in li): #dtype == 'http://kgbench.info/dt#base64Image' and # seems redundant\n",
    "        #     img_set.add(li)\n",
    "        # elif dtype == 'http://www.w3.org/2001/XMLSchema#anyURI' and 'http' in li:\n",
    "        #     uri_set.add(li)\n",
    "        # elif dtype == 'http://www.w3.org/2001/XMLSchema#boolean':\n",
    "        #     bool_set.add(li)\n",
    "        # elif dtype == 'http://www.w3.org/2001/XMLSchema#date' and validate_type(rdflib.term.parse_xsd_date, li):\n",
    "        #     date_set.add(li)\n",
    "        # elif dtype == 'http://www.w3.org/2001/XMLSchema#gYear' and validate_type(rdflib.term.parse_xsd_gyear, li):\n",
    "        #     year_set.add(li)\n",
    "        #     # except:\n",
    "        #     #     print((pi,li,dtype))\n",
    "        #     # \n",
    "        # elif dtype in ['http://www.w3.org/2001/XMLSchema#nonNegativeInteger', \n",
    "        #                'http://www.w3.org/2001/XMLSchema#positiveInteger']:\n",
    "        #     num_set.add(li)\n",
    "        # else:\n",
    "        #     node_set.add(li)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "15130f7f-6bdb-4a83-a416-bfc28aaa9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d33cade8-2a6a-4e41-be5b-6027c6531e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6fe61-6e17-4017-ae46-6bde6a305a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37348cd-1787-4344-96bd-ab27717f56a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb13fd-6d2a-4f4d-92a8-f2616a839edf",
   "metadata": {},
   "source": [
    "### relations mapping literals to nodes\n",
    "\n",
    "##### http://dbpedia.org/ontology/thumbnail --> img\n",
    "##### http://www.opengis.net/ont/geosparql#asWKT --> spatial \n",
    "##### http://purl.org/dc/terms/created --> temp \n",
    "##### http://www.w3.org/2006/vcard/ns#postal-code  --> str  postal code #postal codes don't have a seperate encoder yet\n",
    "##### http://www.w3.org/2006/vcard/ns#street-address  --> str adress\n",
    "##### https://data.labs.pdok.nl/rce/def/techniek --> str (technical photo description)\n",
    "##### http://purl.org/dc/terms/isPartOf --> str rare and unclear what it means\n",
    "##### https://data.labs.pdok.nl/rce/def/fotograaf --> str personal name\n",
    "##### http://purl.org/dc/terms/description --> str image description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c1f2dbe-df3b-421e-a165-16b8e26b118c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as trv\n",
    "\n",
    "\n",
    "# for i, modality_edge_set in enumerate([text_edge_set_s, image_edge_set, num_edge_set, spatial_edge_set, temporal_edge_set]):\n",
    "#     for edge in modality_edge_set:\n",
    "#         encoder_map[edge] = i \n",
    "\n",
    "# map to encoders later\n",
    "#^this logic doesn't necessarily work with other datasets, but I just don't understand how to do it differently.\n",
    "\n",
    "transform_temp = trv.Compose([\n",
    "    trv.Resize((224, 224)),\n",
    "    trv.ToTensor(), trv.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5], \n",
    "        std=[0.3, 0.3, 0.3])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ddc8a4-7f94-4e7d-91be-63fb572b0052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/royal-cookings/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.9192972649193174, 51.64911756496414)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from string import printable\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "node_map = {i:node for i,node in enumerate(node_set)}\n",
    "inv_node_map = {node:i for i,node in enumerate(node_set)}\n",
    "\n",
    "edge_map = {i:edge for i,edge in enumerate(edge_set)}\n",
    "inv_edge_map = {edge:i for i,edge in enumerate(edge_set)}\n",
    "\n",
    "\n",
    "def tokenize_string(s, character_map, max_len=5000):\n",
    "    #preprocess for text encoder[1](? layer temporal cnn -->embedding_dim)\n",
    "    s = s[:max_len]\n",
    "    #character level embeddings\n",
    "    tokens = [character_map[char] for char in s]\n",
    "    return tokens #\n",
    "\n",
    "def encode_image(encoded_str, transform):\n",
    "    #preprocess for encoder[2](2 layer cnn -->embedding_dim)\n",
    "    image, img_bytes = decode_base64_image(encoded_str)\n",
    "    return transform(image), img_bytes\n",
    "\n",
    "def encode_num(n, max_num, log=False):\n",
    "    \n",
    "    # read how it's done exactly.. cat(e(num),e(bool))\n",
    "    # maybe add log scale if max_num is really high...\n",
    "    \n",
    "    v = n/max_num\n",
    "    v = math.log(v) if log else v\n",
    "    return torch.tensor((v),dtype=torch.float32, device=device)\n",
    "\n",
    "def encode_polygon(poly, global_mean_x, global_mean_y, x_max, y_max):\n",
    "    #preprocess for spatial encoder[3](? layer temporal cnn -->embedding_dim) \n",
    "    poly_tensor_x = (global_mean_x-torch.tensor([ point[0] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/x_max\n",
    "    poly_tensor_y = (global_mean_y-torch.tensor([ point[1] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/y_max\n",
    "    # print(poly_tensor_x)\n",
    "    return torch.stack((poly_tensor_x,poly_tensor_y),dim=0)\n",
    "\n",
    "def encode_point(point, max_x, max_y):\n",
    "    #preprocess for spatial encoder[3](? layer temporal cnn -->embedding_dim)\n",
    "    \n",
    "    point = torch.tensor(point,dtype=torch.float32, device=device)\n",
    "    div = torch.tensor((max_x,max_y),dtype=torch.float32, device=device)\n",
    "    return point/div\n",
    "\n",
    "def encode_date(date):\n",
    "    #preprocess for temporal encoder[4](? layer ffnn -->embedding_dim)\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "        \n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    split_str = date.split('-')\n",
    "    years_str = split_str[0]\n",
    "    month_str = split_str[1]\n",
    "    day_str = split_str[2]\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    months = cyclical(int(month_str), 12)\n",
    "    days = cyclical(int(day_str), 31)\n",
    "    # print((centuries, decades, years, months, days))\n",
    "    # print(decades,years)\n",
    "    return torch.cat((centuries, decades, years, months, days), dim=0)\n",
    "\n",
    "def encode_year(year):\n",
    "    #preprocess for temporal encoder[4](? layer ffnn -->embedding_dim)\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "        \n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    return torch.cat((centuries, decades, years), dim=0)\n",
    "\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, strings, character_map, max_length=5000):\n",
    "        self.strings = strings\n",
    "        self.character_map = character_map\n",
    "        self.max_length = max_length\n",
    "        self.pad = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strings)\n",
    "\n",
    "    def __getitem__(self, i, pad=True):\n",
    "        s = self.strings[i]\n",
    "        tokens = tokenize_string(s, self.character_map, max_len=self.max_length)\n",
    "\n",
    "        return torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, encoded_images, transform):\n",
    "        self.encoded_images = encoded_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        encoded_str = self.encoded_images[i]\n",
    "        image, _ = decode_base64_image(encoded_str, log_note=f\"inside generator, image nr: {i}\")\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "class NumericalDataset(Dataset):\n",
    "    def __init__(self, numbers, log_scale=False):\n",
    "        self.numbers = numbers\n",
    "        self.log_scale = log_scale\n",
    "        self.max_num = max(numbers)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        n = self.numbers[i]\n",
    "        norm = n / self.max_num\n",
    "        norm = math.log(norm + 1e-8) if self.log_scale else norm\n",
    "        return torch.tensor([norm], dtype=torch.float32, device=device)\n",
    "\n",
    "class SpatialDataset(Dataset):\n",
    "    def __init__(self, spatial_data, global_mean_x, global_mean_y, x_max, y_max):\n",
    "        self.spatial_data = spatial_data\n",
    "        self.global_mean_x = global_mean_x\n",
    "        self.global_mean_y = global_mean_y\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "        self.max_len = max([len(data) for data in spatial_data])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spatial_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        spatial = self.spatial_data[i]\n",
    "        if isinstance(spatial, list):\n",
    "            x_coords = [(x - self.global_mean_x) / self.x_max for x, y in spatial]\n",
    "            y_coords = [(y - self.global_mean_y) / self.y_max for x, y in spatial]\n",
    "            spatial_tensor = torch.tensor(list(zip(x_coords, y_coords)), dtype=torch.float32, device=device)\n",
    "        elif isinstance(spatial, tuple):\n",
    "            x, y = spatial\n",
    "            spatial_tensor = torch.tensor([(x / self.x_max, y / self.y_max)], dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            spatial_tensor = torch.zeros((1, 2), dtype=torch.float32, device=device)\n",
    "        return spatial_tensor\n",
    "\n",
    "\n",
    "class TemporalDataset(Dataset):\n",
    "    # todo: check if there are other temporal feature types in the data. i.e. just years etc.\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates#torch.stack([get_dates(date) for date in dates], dim=0)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_date(date, max_len=500)\n",
    "        return date_feature\n",
    "\n",
    "def process_point(point_str, highest_x, highest_y):\n",
    "    if 'POINT' in point_str:\n",
    "        point_str = point_str.split('POINT(')[1].split('))')[0]\n",
    "    elif 'Point' in point_str:\n",
    "        point_str = point_str.split('Point(')[1].split('))')[0]\n",
    "    point_list = point_str.strip(')').strip('(').split()\n",
    "    point = tuple([float(coord) for coord in point_list])\n",
    "    point_x,point_y = point\n",
    "    highest_x = point_x if point_x > highest_x else highest_x\n",
    "    highest_y = point_y if point_y > highest_y else highest_y\n",
    "    return point, highest_x, highest_y\n",
    "    \n",
    "\n",
    "def get_num_data(poly_str, max_x, y_max):\n",
    "    # if 'POLYGON' in poly_str:\n",
    "    #     poly_str = poly_str.split('POLYGON ((')[1].split('))')[0]\n",
    "    # elif 'Polygon' in poly_str:\n",
    "    #     poly_str = poly_str.split('Polygon ((')[1].split('))')[0]\n",
    "    # elif 'MULTIPOLYGON' in poly_str:\n",
    "    #     poly_str = poly_str.split('MULTIPOLYGON (((')[1].split('))')[0]\n",
    "    # elif 'Multipolygon' in poly_str:\n",
    "    #     poly_str = poly_str.split('Multipolygon (((')[1].split('))')[0]\n",
    "    poly_str = poly_str.split('(')[-1].split(')')[0]\n",
    "    poly_combi_str_list = [poly for poly in poly_str.split(',')]\n",
    "    try:\n",
    "        poly_tupled = [(float(poly.split()[0]),float(poly.split()[1])) \n",
    "                       for poly in poly_combi_str_list]\n",
    "    except ValueError:\n",
    "        poly_tupled = [(float(poly.split()[0].strip(')').strip('(')),\n",
    "                     float(poly.split()[1].strip(')').strip('('))) \n",
    "                    for poly in poly_combi_str_list]\n",
    "    x_max, y_max = max([x for x,y in poly_tupled]), max([y for x, y in poly_tupled])\n",
    "    x_max =  x_max if x_max > max_x else max_x\n",
    "    y_max =  y_max if y_max > max_y else max_y\n",
    "    x_mean = sum([tup[0] for tup in poly_tupled])/len(poly_tupled)\n",
    "    y_mean = sum([tup[1] for tup in poly_tupled])/len(poly_tupled)\n",
    "    return poly_tupled, x_mean, y_mean, x_max, y_max\n",
    "\n",
    "global_mean_x = 0\n",
    "global_mean_y = 0\n",
    "\n",
    "# print(point)\n",
    "max_x = 0\n",
    "max_y = 0\n",
    "\n",
    "points_tupled = []\n",
    "for i, point in enumerate(point_set):\n",
    "    point_tupled,max_x,max_y = process_point(point,max_x,max_y)\n",
    "    points_tupled.append(point_tupled)\n",
    "\n",
    "\n",
    "polys_tupled = []\n",
    "x_max, y_max = 0,0\n",
    "for i, poly in enumerate(poly_set):\n",
    "    # if i < 100:\n",
    "    poly_tupled, x_mean, y_mean, x_max, y_max = get_num_data(poly, \n",
    "                                                             x_max, y_max)\n",
    "    global_mean_x += 1\n",
    "    global_mean_y += 1\n",
    "    polys_tupled.append(poly_tupled)\n",
    "i += 1\n",
    "print(poly_tupled[0])\n",
    "global_mean_x, global_mean_y = global_mean_x/i, global_mean_y/i\n",
    "\n",
    "\n",
    "character_map = {char:i for i,char in enumerate(printable)}\n",
    "character_map['\\x7f'] = 101\n",
    "\n",
    "text_dataset_s = TextDataset(list(string_set_s), character_map)\n",
    "text_dataset_m = TextDataset(list(string_set_m), character_map)\n",
    "text_dataset_l = TextDataset(list(string_set_l), character_map)\n",
    "\n",
    "\n",
    "image_dataset = ImageDataset(list(image_set), transform_temp)\n",
    "\n",
    "numerical_dataset = NumericalDataset(num_set, log_scale=True)\n",
    "\n",
    "spatial_dataset = SpatialDataset(polys_tupled + points_tupled, global_mean_x, global_mean_y, x_max, y_max)\n",
    "\n",
    "temporal_dataset = TemporalDataset(date_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09675e30-8e0a-484b-8949-c2ca0f0521f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m         img_file\u001b[38;5;241m.\u001b[39mwrite(image_bytes)\n\u001b[1;32m     30\u001b[0m image, image_bytes \u001b[38;5;241m=\u001b[39m encode_image(byte_str, transform_temp)\n\u001b[0;32m---> 31\u001b[0m \u001b[43msave_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m image\n",
      "Cell \u001b[0;32mIn[29], line 30\u001b[0m, in \u001b[0;36msave_img\u001b[0;34m(image_bytes)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_img\u001b[39m(image_bytes):\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfred.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "byte_str = \"\"\"_9j_4AAQSkZJRgABAQAAAQABAAD_2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL_2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL_wAARCADIAMYDASIAAhEBAxEB_8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL_8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4-Tl5ufo6erx8vP09fb3-Pn6_8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL_8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3-Pn6_9oADAMBAAIRAxEAPwCprVoYUDmLGeoYjIA9hWBHbLOwCRlSGBJPrXea6kQtGYtiQ8Llep5zgGuXdLxpIwwGwbQ2ByR_n-dAHuelqPsUYZgW2jJ7nAq2QAQTwSKpacFFlGTkcA8_SrDNkjJyBmgCpdOoYnOSR2NY05XyJGOSCMDPrV-4fbkBSQQeO5rJuZWNm25SCG5HoKAPPvF00kFhIyShCXA4GSfb2HSuStNd1C3Kxm5MqZIYE5yD1Fdb4vgFzp7omWYygqAOSP6VwLWDxZAjJweSD0oA7Ky1xL6QCX5cgABiARgngH0res9TidnCbgFIBU8ZA64_GvKBJNbSBgoGCRhhkHn-VdBpniWBGVZ0EJGASASPrxzQB61azpLaI0irJGOAc4I9jj0p1xZJMQNxKgcIQRg8etcrpXiSyRZJSzJGynCA7txJx2zg8-oq_wD8JLbIzBTK4BG1wMjp69aAJLiyltc4BZTgYx07ZH481C4LEA8kgEEGtq31KKeMAYdmAyufu57ntVTUEBuA6qFUAnBA4PtigCmHBBCtgAgDIOcYwMfrUlvKDqAXAII2hSCSMY5yDUTqyKDgHcwOPTNOtCZZyxKoRnGBjOBg9-en60Aa0wICFAMAEnH-fapLZmcgkAYAHOe57flUaxBII95IYgkHcDk5B59OaW2YNcADKgEnk-h60AOmJNsGOWJzk9-uetcdrj7JyMndnccDggdK7PG5QoAA5GB0_wA81yevRIt0wODwcd8UAGm3oRyobI8ssQTnJ_zj8qivEMkiYYH5QcgAc4GR_n0rKtWaEs4LbwflIIBAJx_WtX5nRmAYkLgcYPAwcfiD-dAE-naY76hbEszRZ3tkcE4P5V1kduiXKFUUHBIwQNvXqa57RbgGOMHJG3kknAORgc9_8K1Uu5FuRIylQRgnA4OB3H4UAdCoZgUds4OT06_5NFZ0d-H3bNxxjIooAzL-3kn8lQqyEsCy4Bz659O3NcncQONRk5JjVwAFOCef5cela_8AwkFoQWt5ltiyhSuAcnjJPt7VWs7yB7gKLhZVZsvgAsR2Oeg7UAer2ZBtVXp8ox-VTFCTkjGPf2pLJM24YDGQB9OKlQKM7iSTnmgDLuBtBLcnnGKxZ2L20gALAHJFbl8QvBxxnk1izyKljIWcBC2Dgc0AcNrYLxNgEAMBgEA_XmuUFtOGkxMucZIYZGOeP89q63UozcbgoPUYOcAjOP51jX2nsqloywjZcYAxzwPrgmgDmJ8u7EwxkcE8cD6f_XrPn2rlRCpkJyACQAPfNdNbwJEHViTIAcjsD2P5fzqr_ZrvJ5rKWDHO4nt1HXmgDHtoNRQh4Qy5BJUcD8a7HQor29CG6XYgx0AJPbNX9O0xBEiqoGBnGOhPbPX8637OzEAUbQQDk57DigC5bW8VtDtyST6HI6UStvUDIyTkA8ggdKmyHIXJKjoM85qKQA7ARgAkD86AKhGXJfHyYOMc9-1Fkn-kKpcE4J4OM8_T3qaQHLYOGxgEZOf85NNjQxXEbksSCSBwSBQBo7chVA4UnJI6f_XqRIsThy4y3I54HbH54pzozRgHAUHJA5yfT-tMHDAZJAwcnvzQBHI5QMSCcA4zxk1wuoXbvdNwc9eDxjH-NdxOn3hwck8Z6Vw2rWrrcbzjBzyB6mgBkE0krBlKhVbGfUn1_KtyyjJt7gKATjjJwDyelYWnKoLAkABgQCePX866nSlIt5CVAzntwOucdaAOft9WFuk0E2NhIAOf0_L-VbI1RbqERKoIGQcnkDoffNZlzpyT3JthuxI4w6kgryDjHfua2G0qOwmjRGIBTcWducnqMeg5NAE1o-IF-c7sc7TtI-vBopthG8YO4qAckHkcZ_H_ACKKAPLriweQhrS9iJCgnLgZP0NWbeW6tLu2AuFk3SqCAMHJI7irN14ahQhopmjJzwRjAziorLRNShvrdo5VeMTLnAzxkelAH0fZA_YQAcEgEflSsCGGBnA6VPbbBahRxwD-lNc4J2gE0AZU4ad2DAAEYGayLu3AtGBGRuzjGefpW1dy-VC77SccgDrWPJKstmxUkFjkjuPagDidUPkAnIBzxkkj6_8A1vWs4XaSRRqZA5IJIzgD_wCv9PWpPGtwYLcusY2BwMEnuPauStNWuVDMkWRjHCk9e59DyeaAOgjgtGuCxMjEEq2D1HYZ9aswWk8sgiYkorDGeSBwM-9Q6FCb2RNwdCSNzEYU8dgev1roruJrNS6qpjYAZBGRxnnvQBHDETIqIxIB6AEYx-H1rSm2IwYtsA55_wA9DVe2urRCzLMuABg8c-9aA1XS1jBleDkDO4D8snqOOg6UAMidHUMpBBOMgfnTZcBeSc5yABVK78R6NEpAmAkxkBFIB9sHj16d6gTxLpM8YZ5wpBGBtJIx1J_L3oAunCyKWHGM5zzj0qzbIjT5JAAHUnr16EcfyrBfxJYvIxaUshBw4UYB9wKINatyGf7UUyAOAcMO-ff8aAOtX94MA4Ge3YAdB2obbwrEqCQRz29KwV8TafF5cTSFVB3b8_eH8s8frVhNZ0y6mVVvFQE5JckD_P6UAajRCQkEdSc_zrE1TTRKEKKFAJJweuMf4Vt27xyoxilVweFZSMNzjP6U6RGdSuSMjBxQBw6WgEgVVJDNk7TyQSAfrXQ2YP2aRQAQHIx0yeMfTtUMtuttIrlc4JA-mc_jVuBBBb5YnJYsOSOgx0oAoFFGtWyPlQ7EjAyc44wPwH4Vf1uymub6NAFCqocYGSQc5HH41Lp0cLX7zsrb1UnLjhSDjIz35rUkNtLKJDPGqBSFIccj1x6f40AYltMqWy-ZIFUcBgaK6SCOzWBX8xNrc7lIBJ_PpRQB5VqEzSlsNkkAZPYDpUOnorXtv5mQ3mKARxxkVau1XzioAJPGe461Lp8KfaoAACxkXPHTkdKAPa1GYwqnAAA-tWpAI4wAAKhjQJGCMYwOnrSPMWG1h0PBoAzZwSzK2cAE1z8rgI4AIBY4OO1dHclRLIpODt4Getc1cYS2dt2FBJOfSgDifEkDX6GCJwp8wEE9wO3esCDw46c3V6ozwQpyRn8a7DUY1eFGiBVg3XHXOMACkGnLcG2ntUMwEoaVGYKAARwc9e3NAFeDwJaqimeeQqD124BGO-DmnHwJYOw8u-ZI2OAGGST6A556V03203NvMFAQo-0FwMHHfmqxhUoAQTj5sEcAjg4_P9KAMaLwLYzBdt6mSeSYjg_gDxXFap4A8Tf8JC9vBqKG1yGV1kICqTgHaT146Z5PevXLWCRIgzgjJwATz-P4VBMxGvi3U4Row7HHoCcfSgDkLH4XQxkNqGt3N2cYBWMKB-BJz-NaD_C_w9LIHW5vwx4-SRVH5BcV2JQRgEZzjgdv8iobncYSqAgjGUI9PTvQByjfCrRCcLc3wToQsqjJz3G3rU5-GWhNOHie7j2oAUMoZTjjJBGcn2NdGZ1ghBLgA4GSDnJ4xjFSoMxl5M4PAwefxoA5-P4daa4wLmUDoVxkfzqMfDCyG4rfTD0BX_69dZbzqkUjg7huIwDz_npUH2p9rEsSxPHPSgDC0zwuNOMkUWo3AIblSAFyeM9xUkr6vbTlZ4Z5yACro-VYZPAAAAPscVrQvvkk3Pk5HfHGBV6aQCNMHr69DQBw95PrETq4sBKgbJjBKsD9STn8qq3Or-Jb0quk-GCrIAGmuJQFUkc4wRmvQTtBQ4BBOMEdsdRVC9uGhuAUyAy5II47frQByg8MeJtSEY1PxDHCCc-XFESFPcA5HPH6VcT4eCQBLnxJfsDztQAD8yTWnHqEZwC-9xIMKR1GOxJ9-aspeIhZi6tjg44yMDjAzjigCrB8N9IjT97eX0oPTdKAf0FFbUV-HiDJgg_3T_nvmigDy-WTzDuAGCeuK19JjUGKYgMWdQMgYGCOayo1IlZwgKrnI64Jqzp-rG3vhbvEDG0q4GeQSQOD_SgD2UuyxgYyCB0qKUuHO0fU1dRR5CMQAcDpUEhIZjkdOM0AY-oiVpiFIAx1z1rn5gXt5Y2GcHJz39s1vX0reYflBBBHT2rGlRZLVwFIyCRkfrQBgy4EpgdwiPgnknAGD2pNLuUUtbAqxWQggk8qec4P1pFHnapBEMqCwBIGTkketaP9mwW801wmOUCjB6kHv-A_SgBt5P5csazkBpQAY15x6HPvj-VWYYgzqwbCAYCEDj2PtWPqSQ31xHHvKoWUA9SOfXj863Ft_L2YOQDjr79eaALsmSEA7Hg1HDaB_FqSFSwMPzDB44AAz78_lVTxB4gsvCmjm-uwJpHGIYRwWP8AQV5HFr_jrxffzXOlzPaQ_dLRHy0AHQZOSSM__qoA9-lEMShWBJYbcAZ59M-tY98EjkVULgHklxk88YyK8ug1_wCIXht0k1J11azBy8bkMwHcggZBx3Oa9JtNVsPFHhyHU9McgD5JEcYZGGMqR2I-voe9AFc3RRmLIWC5wPX3_Ko7fWXM7RtCTAYwxQnlST9fcH2p0iskTyl8KoJJGMgen1zx-Nc5bKZL5353bCRg9QD0OPb-VAHZhgbePa2QcnGeuKbzk5GCCcdwKhH7uCIAEfLkgY_z61Ij5GCeoIBI6GgB1q5F04JwBjGRzjFXZ2KwgnHHfHNZdo-bpwAcAAdfatCVQYgCuR0x2xigB7znfGuVwzYyTz0zxWdrhEXlkE52gEg4xxTbmWQavYqFzGWAJHqRj-WTTPE52CNcgZAPPIPAoA560lVZi2CXVjh-R6Dj6D3q7e3bKCY5C5B4wACQexwfX-tVFiURs6AEnABBzg9Tmob1VWUMpYBlDKD1PIyD7jHb696ANqyuQjFWkwhX5doyRz3_ADorMtL3zIy7oEQ9CACepooAx21CWJCAgVGO_O3rx6n05_GqVoxfVLeRzwJVPTknIqzNPcyr5TsPLjXy1BAHHfHrz361UtgF1W2GcKZVJyOnIoA-hoXzaq2OwFRyYJOcY9KISGt-CMAA8VG7bcsBnAoAxtQAFyoySMnHoOKyZUcIz-dgg8Dtit6coSwYAEngmsS7jABI5OeAelAHNzvJBd-erhGJJOD1wRjp0rUjRRapKcgMMlWJ4J579uaxbl2efaACxBBA_qO4raeNliigLEBQMkYO7Hrn8aAKy2SGTc4HlnG1gDkY5AH9a6GzgErjYjMccDGM-9Z6OGZIVPAOSB2_Lt2q_YzeVcKpY89R2x6f59KAPJfinLLe-JIrTcfLiAjUHpkkD-ddpPZDw74WhNlbKQpMQHphWJJx1JCk_U5rJ8c6E11fSzRj94p3KRzwec_gea67wtqemeItF-w6gSk8ZBkQEho3Bzkd8Hkg4xgkHjqAc54duLx9cTTbpo51lI3FgDgEEjBAHcDg9Rnpjm3ounf2B8TL3SLZglvfW4uRGR8odSAePcN-g9K6210HQtIuf7TN79oaMloxhAqnBGTtABIBIyemTjFYmiRzav4yu9cmUiMx-TBkYO3IyfXB4A9QM96ANHXtHBicAMDISSVOcEcmua0-xeO4mTBXC9xkkD-XfvXb65LNFFK0eCVBwDnGeAOPbn86wrI4uPML7iRg44z60ARynBQAfwgZJwP88VJuGAeMd8GmXQAkTAHHJx25NSgLtB4CkYxQBDagfbSoBwGAOexwK13AIQLxwAfrzWRagC-K9Bv6gcHArXcAAgLjB_HvQBXESvPCWAGGJHA5IBx_OsnxakjtGMcAAjnrwK2clWjxwCSSTnGMGsPxVcmO9RGzhkGMDuABQBjwGQsWwQpYqDg4PAB6e1WtTslMQlDEKcYTGeP8emahsHWchSWBUnaM-uOfb_8AVW1d7TZEAZAJGc9T3z-tAHOW8RMRUOAQRnBI7fSipLC4WElQOSMsQepBI_xooAp3SIl0zQ7SgO4HHGM8fhxWed0uqI8vJMq8gY5JHarD7y5VlK7TyCc5IJpQmbqAngh1JGehzkZ9-KAPcICFtyMcEDBprbQ5Gc8cg0tuS1vk4JAH8qSYguMYzigDKvEY3AAUYDZ_Ssm9z5DErjkjmtuUk78kbh6Vzc4mfzlZiCSSvpigDmzg3Dy7gpXgBh1raQYjjV1AZUHAJJzgf_X_ADrFtrSe51YW2ThuWJPQZ7fhmulMAWVizKrAZAI569PwoAdYvFIuWARyOFyCce_61oJBGzld-w5GDtz2HSoEhiYq0eARjIBHH19KeAxnywYAjG7jGKAE1G0hlljUMWcDGD9O5_pWFd6Tp5u8snkzjhXTIb8CMED8a6SQK8m_aSFAwR7cislw82thMjYIy-Mck4wDQBBb6PLPIoupZnRSCN7EgD6Enn_Oa6nTESIiJRsVMEYI5Oe4x7Vn2xkCCQ5Jxzk9B9Kv6W58-QNg4GARwefb6UAalzGs4KlQT1yRwR6D8656a0SCVkVBz0JGD_P2rpRgq4wxIGAD0z64rGuFMmCSFIGTnJJPegDnr_jZgAAg8-hyT_WmW77jtA6HIB607VHCRpgA4bBHsf8A9VVLVyZDweBnI_D_AOvQBPA_-luTgEuSQR9OtbGcwgtyTjPGaw7cj7XLnA-bufYGt7JKxkYycAemT1NAEUxYKhUjKg8Z7YJrmfFrYuoSSNwjUknr0HH6V10sQ2gHAG04xxxgiuH8dSCPU4wBgKqjB6YwOaAMW2d3vgq5G7gEHpXTXl25sU5JB5OT1PTp-HSuQhn2SKwIBwOhzn1rXmuDNp5UMRkAggAj_PNAFMTyJIRtzwc4A65oqJXyhLEE8DBbGOvHQ0UAWnX5s55zj1z35pYQXuYycZLA8jBzxnH5VMyBQWBDAHJA9cf4VWtnH2uFg27LjcAehyRj6UAe2wACyDg8lQMH6VBO-CCOoB6VNAVFkMnOAP5VDOSq7woCkdRQBkMSZmkZiF5BB6A1kXTgMSXHzEkYPJArauCRHIpiIJ5VwM596569EZkO4t8nTA65oAztHM8uqzrE4RihwzDOMEehrdjsJWcG5vWZhk_IgAPtznA9-tYmkoBrkxVTtCYJB6ZNdGAThiAB2Gc5oAcYGQHynAU9HJyBgdcAe2MUrl1iYAxM4GRgEfiTz2poQiAgMwUAggcCo8Mcgc5wMYyfQUAPheZwN8CKmc5WTJ-uCB-VV7idLfWYgTgSxMqnryCM8-wq0nmCQKAy57YwK5Dx2jNc6SsMzROWkBIYgtkAY46igDrUJLIsYYBhknOf0q7o5jieYzykMOVDdCCfp-lVLfMcaErkkc-_FSAruQsuTxg9fb_P1oA1LW5K3bxlNoK5Uk5PXOcAfX8qhvJR5hCgg55BGR-FMYkSRys2ABgFQASc9-vHNSXUYIEoOVYc4GDk96AOZ1VCQwweDkkcDt_jVS0jIYE_ebAyTWlqSErnB7jHSqdsBhQMg5x1oAbbD_TpRkj5hkD6CtxcFYgRwrZI9cHNZFuP9KlbGMsMAjnPTNa0IBYAEAY4A6mgCad9zjgcAjr2xXBePoib5SQfmjB_QV3km0FQc7iME_nXG-Oj5t8owAVjAx3HT_EUAcUHYeWCDhVxkdK0hLm32oVIwCCT1rNMXzkkE7R0PQ-1Wo0cxAAZK98dOPT04oAmjiVnYM2G7lcjP5fU0U6CNmkK-aVOMkD5fSigCxM0sSBIvmLMcnpxgA5pbNSJIQUBHmAcd-eue_Jz-FWJY0L_ALzcGBJBXg4xjp3qIPCtxCCGG2QZA4HA_nmgD2uEAxbTzwP5VBeHYqKoBGe_Sp4zmJMAAlRx-FQ3MeWG7nHIAoAxWjeaVpDLtHKlc8dK5e9IgecB1YHqTzmuqzGu87dxBOQfpXLXEKSpO7AqQx474oAbozql5J8wIMeCcdDng1rM4RgY5CW6H0ye4rnNJB-3zAE4CEAE9eRWnI4QIoJJBz6_h9KANe0lRVCkkt0JxgGrGGLMMkDIIJOTWXASdhOBk5I61sI6GY_KR8o9hmgBlpKsVwzOGYkEcnt-Ncr4wSWbVNNmRFJjdjjgDAIODXUbQ84AAIBxisXxLKkUungKGaS7EPPoRg_XoKAN0wKIo0YcgAjHXpSOpxt3M3OASelXXjTgOSSBweuR9KhbETEsQT1475oACSIAwwcgEjPJFWY2D2xXqRyP8KqB9y5yCccKeePpmpreQGMrgLuGcY6Y7CgDJvo2JJY8AZwf6VSt0YKBzwSCR2rSvFUsrMpBAIHJz6-tUIgY3Y88HIyOvagBFYC5myQQGA4-mauQNsuY-Tg8ZxWci_vbgHgkg57cir6DLBsc8HpjGf8A9VAF-Rlfk8kZI_OuI8Zyg6gM8BlAPqOB_gK69nwSSTkdOn-e1cR4vJfVgoOCoHGOvGKAMD5CgUgZGSW5PpWlaRo0bBhnnAI6Gs_DNgE5ABBA4AHFa1ghMXABGAAfx45oAYYRnc77AMqOOvSitKWzZyGABJ5JooAydQlMd0CgySODjgA9s_nzTYVaQRSkciTIB6gcf1FToFeaQSncCOCQO2QP6frTo5RFIUCbiWAUk9OefxxQB7NAR9nRhwSgP44qNpQ5IxyOuaLUjyExk5QZJPtTZAr5U8E9cdaAMe4TEsjEggkYGOlc5dwStJMArNkZ4HQCuouUwrFmHtjvXO3LFJHbeQdpGAcD3FAGNpoQao42ksUIAz06VfYg8EqR39RWbpjH-3CCDjBAJ6cVpMMTMAc5OR1NAFqE4UAckDJ46-hrVspiYyhySQec81jRsRIrrkZAGen6VesmxICp55yT_OgDSjQPONwIUn5iQcjPcetYHjGREXSdsQZv7Rj5JwQCDzx6YHFdBnC5bI78d_wrnfGBfzNKRl-YXiHB9CD_AI0AdKjkRAMA2CRknJHNVp3KPhjuRgcEnOMdv0qxLF5WCDjjkjgZ71RdGAcL82CGXJ69jigCxEVOC3I6A44PH_66toFRCQcEcHPU1Tt8sRwQR68DHcGryKBEVPIxx6DtxQBl3ODGGIOAc5B_D-tVgOCSMEEE49P_ANdWpwDGQRjjn8OapuxABX7p7DpntQBHGm6-uABwApIPuBVxwEZgTgYAB_z06Vn6bIZNUuxnjC9T7f8A1qtzk7mBGCAPw6_40AWR84U5wD69-lcV40eNdVDJnBUZ-oArsUIKovOAuQSe3AriPGcmdUG5eABwPp_n86AMmOMtliGK8ngdPrWxpyM0YBO07gMcDjk4_X_OKw4JQIgNwBbIXJP-eK6HSc7kYuGAAGR0P50AbccYX5im4Hsxxg0VoW0e9dwxzzyfpRQByJhBt8gkkN1z04659v61TkRjeQoo4JyAByTyR_OtOLatoyBcsVBbkcYxge_X9KiEZgmt3GDtAc4HI6nj_PagD1WzBW2jDdQg_lTLhgGB3YIPNELs1rGxySYwSR34qKcEorEjk80AZN5cogJVizE845ArDAe4lkDKDgHknqTW3JGyysAi7MHIA5rI2souZApXaSc-g7frQBjWEZTVlRlO4FiDjqO_1rWnQlt2cEHr1rMsZ2l1mAjaWUMCN2DjFa1ywMhCqQcYxn2oAZEWwCSenPH9K2baJY4POdtxI6AdDWKSI488nJ6Z_CtGzuGnjCMc4GQQOvYZoA1YmV2BIJJBGMVzXihGN7pxUqQLkMQRk9COD6D-tdJDyuQSADkA9qxdfhL6vYKW5UEgcDOc9vagDoZAskRZlySAeQB9c1mzqABg4PYir28tEA2CRnPNVLrBgZtqgggnHUD1oAIHJUAkFhxx1I9frV23dslWyc5OSOlZyASmM8IwGeD1561owsdo3DJ6AgYNAFKcEF-gIzx7YrOBYRDCjIGRz6Vr3SBmkzj8Py_pWaqBFk5zgnnrnnrQBm6a5TXLsE9cdenQVozEgyMDgEA5-hHb6GsaFwmuXDA4BwAcHqFFakpxbswIIPHPXtk0ALbu_nxoSBlSST0xkcY_GuR8cFjq7ZIAUKMYwDkZrrrIJJcRluXVcjmuS8bqTrMuQRhEOc5zwBmgDAiVS6bipAY8Z5HGa6bSAGwAASCDjBPArmrcB2Vs4IPI9hjNdZpEStIhj3AAAMCfb_8AUMfWgDq7ZC0IG5RjHXiim2cohJy-PlA5OP8APSigDkZFMfMYP3CrAdzzz-ff2FSQJC88bOFCAqm0ZzjIyT-v50y3AMSkZACEHjp0Ax-RocBGSQAEA44OckduOnSgD1LYEKquAoXAFMkRgoJAx24pLN_Ns42Ay5QHk-1WhIRGA4BGOBigDBeNjcyYO3PHWuduWl-13ETMSCcnPANddOIDcFkBB7gniuS1AxDUXwWJI5A7HtQBiaaBH4ijCkHBYEEYHT9a6GdC0mQADjPWub04s_iVDllAZie-cCuokwXBPBJxwaAKDucELgBegx0_xrR05HMRcgBCCCc8_wCf8KoTc8KxOTkZFaOnkhNgAwBycHNAGvakhlU5AOMg85rJ1nP_AAkVjhTtAILE8DOcAe5wfyrat0AXKgk46kflWB4hZ01zSo0Bbe7HOem0d_wJoA2GJVAABg8jFRYbaXJ5x1Izn8KlcNgNjAwRnqPWow-FAPcccc-tAFcCKNwzkD_ZI4z7f5xV2MkxkRt3yAc8Zqu6biCuNwPGen-etLkxgsOOMjA7elAFmUZU46lckjpmsmciK4PAIPJxxkf5NalvKJY1KgYwRjr3_wD11m3ykOc8HHB6ZoA5l5MaxMpIBDEc8EgYH8hWmtyDYjdjJPp_n0rFu2KavIRk85HAOeAP8amWUG1KAgMoBweuCcn_AD70Aa2lSGSfJPCqeMc54zWF4yKNqcyk4HlKASehwa0tFlczOq5OUbAP1_xH61i-L2YavKgA-ZQRkY59c0AYFlKMgE5IBwQOT_nFdZo04VkGWwBk5xjJ_wD1CuOgRVjVsZCk5Oeo_wA5rf0kSKwdWG3oAQCD6YoA7SMlnATagC-nvRT9LhMkQJ2jg5785ooA5-MRSQF1XaMBeDx0yT9elVgpVgRkFSMn1yRRRQB6fpYzp8JOT-7Bz36U922IpJIzxRRQBm3ETPcBgxAPp1rn71YY744Rmkwdx7Z7GiigDA0_MevxsCDvLblHGODW3dTlGChhgDJBHXOKKKAIC5bZjjGOM9a1dMYZZTjJIyOgxRRQB0cO3aBtyAQcg859q53xLIqapo8-duZHUYzznA_XI_OiigDTYBosYPQdO1VnQh1-Y8DnB7UUUAPTJbBBOPTvU7LuRjjIAOOcGiigBli-xguCMEg5A5qrqkREm4kAEgAHnPrRRQBxGpuPtbsMZDkEk_jgn8aapYBSqkgxDg5IABzx-RH50UUAbOgBmm3YGAh-YHrzWX4xjaTWX2jBWJWGe4AB_wDrUUUAc_bxJuIJKgA844Pt7cGui0y0CSAFF2smAMHGcg5FFFAHV2rFYwoC9MjKnpRRRQB__9k=\n",
    "https://data.labs.pdok.nl/rce/id/image/20324688 http://dbpedia.org/ontology/thumbnail _9j_4AAQSkZJRgABAQAAAQABAAD_2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL_2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL_wAARCADIAIIDASIAAhEBAxEB_8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL_8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4-Tl5ufo6erx8vP09fb3-Pn6_8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL_8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3-Pn6_9oADAMBAAIRAxEAPwD3FEwAD1pWjXHQVI4A5HWoyw6UAIqD0GRUyog5CgH1xUasM0_eMUAI6K4wyg_Wo1tYgPu5J9TT9-TTwc9KAK7QoGIAwDxTwFUBQOKkwST6U3ABoAjliVkwOD2qt5DknI4q8Oa5_wAT-IY9FtCiENdyDCJ6D1NAGL4u8QjTYGs7Vh9rcYJBzsB7_Wue8Eau1pfvYTufLuDuQt2f_wCuP1ArDmeS5neWZmeRzliTyc96fcSmWWGVVCvGoGV4JIyc_wAvyoA9cxTWqloOprq2lxzEgzKAsgHqO_49a0CtAEBFAXkE-tSFRnmpIkBOT0FADfLPoaKtbB6UUAX24UkmoAfmqVs4NNGDyRzQAxiAeKaXOetDkbqYTQAomHmlOdwUHOOMHI_pU6ydKy0nRtUnhAG-OKNmPf5i3f8A4DVwE0AWi9NDA1EDWNq3iKDSr6O3lzl03AgZA5xyaALmta1Do1kZXIaVuI0zyx_wryu-up768a5uD5kjnJOeB7D0q3qt_Lq2pPNI4ZQSEA6AVTRCJCGOADjpxQARPaxSFrmFnQA5VDg59RyOlS3UWlyRb7OabzMBvKlXsevPTjND20bxusrkIc4IGSPwqJ7aJFBjuUdVXBUghhn2P0oA0PDOo_2XqYVyRBOQrDPQ9j7V6QVyMgjB6Yrx58odygk9RXovhXVv7T0sRuf9IgG1gT1GODQBrkYpyvjtSsOTj1phGDQBLv8AeiouaKANQuCKj3EA4qEOxPFLlsZwcUAOYkmmcnik3c0uc96AOe06Yv411hQcqYYlA_3Qc_qxroxXH6BL5njDUW4xIJCPoGUD9K6q7uorG2aeZgqqMnPf2oAqa9qjaRpE11EqPOoHlRscBjkZHUdj_L1rzZr-bWZZL67eOOVl3LEWwxGOcDvUPiTXbrWdQUIMxB-ASCFUEEk-5yQPrmn2EELxQy3N15aALHwMlVCjnH-etADbdITDMZCQ-P3Z56_hUYDYwGIJA61PcW0UUzC2uVnjGMOBjI-lNiVsgEkL1JI7UASqG8vDEHtwKR4cZLKcEdaeXJbZEQoPcjJ_-tUU0k4IaZ_MQALuAwVwMD-VAEThcH0HtVrRNSbStWjmBJjYhXUdwf69D-FVZyQBggg9D61FII1t0K5EwYk8nHsaAPXg6SruRgwPORSEc1zfgiWafTrgzPI4DgrvJIAI7A9PwrpiDQA3FFPxRQAJOiziIn5ypYDHYED-oqwTkVyXizVrrRjBPabN5YKwdcgggk_qBWRB491EACW0t3HfGQf5mgD0Bl5z60ydjHbSuOCqE5-gJrkofHyEATaey-6Sg_oR_Wp5_G-nyWUyrDOJWQhVcDaSRjkjNAHN-FL9YvEcEjzjbIsvmEg9ME55HqB-VR-LPEc-q3H2W0BWEZwewHcn-grCjgFsW8rYbplAJycRqARk_mc9M_TObCWCz6cEtXkllZjuCqckg4P54NAEAt4YtOR4ZtzswJIPJ-Yd_ersSg2kZIOc9cegAqeLSEiso4bmZY9oAKINzcHp1wPz_CnSLFH5aQoyRjgZbJPueAM_QCgCvEuCxI4B6A9atZJUKOQeT_hUcIGCQAQDzxVhMCTAAwBnp7_zoAswQQOphLDfIvBzgA9h9c1XvLKeBSJ4yMA4AI5x1P0qSQw7WJA8zggY55681HJK11eqbp2CGPCkHABA6d-o_lQBQaIrHsyMHleenqKi2B5ER3CAsAXPQcjk1cn25jZCSucYPWqt2MxkgDBBHFAHpHhW3SDRUVZUlO4guhyCB0rZIwa5zwEAdClAGMTnjrjgV05Q0AR4op-2igDhPHLrJKsYIyrIcZ68NzXLrEDgj0rfvydTu577a6RqmcZAIUE8EEHnn8KxpLhIzkmEKThTIuCT2GaAGpbtKxWMAkEAD1JIGP1qaTS7yAgtAV5wC5AA9STSRkySoAEUlgFMRyWOQQAPTuT2HuQDYnS5KmBy4JwSHyDx7HmgCCOGytoJYipuJJANzZwM4OeepHOKkS4cRiKMLFHyAiDAI9-5_GqrQujEOxHepUQlgQSQKAJEQliSc47USg7o-gAJwKsJEAec5NR3AUGMA5O7GOlAC2ZwrEAHJPapG2-btUAEg4z9aZageUSOMseD9acQxugQMkqTx9aAElRngBXBkGcgHAxUAiLMglB2EjkAcHtmrCA5YFSMdhTST5ZOTgd8UAQXCAuEGQAfTt7VXuogISQQQKtS8mPJyeuc1FdAeQ5yOBQBueFdeXSAba5X_R5WDBx1UkAcjuOK9BjkjniWSJwyMMgg5BryLZmBDgnAGDWpouvXOkTbTuktiRujJ6e49DQB6ViistPFOklFPnNyPSigDymWdbmeTyw6BmBwGOADgkficn8aW6gD28YLkBW3YzyfYZ_nVK0LYG_gkAA9zg8Z-oIrWsoIk3TXBLuHICZyR6D2GcUAO0OwL3k0-zywWByuRjAB4PX0-pPNdNeWkA05btLtmugACPMyRzzg9ax4ZQ8QUgALkADjk5Ofrzj8BVOaV8RqowC2Rxzg0AaAub4glb6YqTxuCtx-INU77X20-VIp5Ecv3Nuhx6E8D9KtqcADPt7Vxnid2bX4QArAKvXHHNAHaLczlQfJs2BAPEJGR-DUzZJe3cNskEMchJwVY4PtyeKW3lP2ePGMbQP0qG4mZZ4njYqwYEEDkHNAE72ktjK0EwG8HJAORzz1qFiROpGeFIqxHcyzM7zyM7gkAtyeOlTJA9xMXjhdwFwSoyAaAKSSMC5GQDURci3Knvk1p_Y5gz7reQD12Gq5tmEDFoXBGeqmgClKcmIAdgDx7VFdoDA_GMDNW7hEiSNnBUDGSeAOKpX2oWsUDGOVHccgD5gSOe30oAtQQSvaoVQsAB0pkltcA5ETj8OlQ3uvaZcukqwvA4UBgo4JHcdKz5NegXlFuDj_AGiP60Aaeyb-63_fJorK_wCElH924_7-_wD1qKALUTxNexrtdcnchxgEYXBxj2PFW7VTJLMSSSZSSSeetZ1mf9NgYEABccdhhsfyq_Yjd5jYyTKRn8TQBYUqpKhg2CMbSCDwOQR1HHX3ps5AEBAxk8nHtT8AM6joHxj04qObpbjoQeKALSgEDBxnnr_9aqF_4XsdTvY72XU2hkVQCn2csOD6jrV5QOCMk1IBnqKADykhUIkgZFGA54BHrg1XlAaeJQRjI5z71l-KgRpqEMwO44wcdqzfCe5YZCzMx80HJJOOlAHSX88tjp9zcQpE8ykBBICVySOwIJ4z3qPw9qd5qcE5nWGJ1YbTBuXjnrknOeKh1tydLuF3hQXHJ6DkVW8Ms0UE4DhuRkg8d_6UAdSlyImKSXVwXUZYCQjA9etC6qvmrEtxc7nXcoM3UHoep9DXnfi-9uIL-MwyuhZQGw2Mj0NVVv4gHLGTdAoBOTnBOKAPQtW1kPpc5MskkZPlkFwec46Y_rXGtt-YADB6c4rmo7yU6oI1kbyXkyUPQ5710BcYOKAEYgKQDk-hHFViAepPPapGPOM_Wm5HX0oAbsHoKKdn_OKKANRbu7tLnIVJArZZSM8EnH8zW5pU9lPCC0jW7mTJyMqSSeB3FVobOe7BmW2IflSOhOMjn_Peon0S_WRJoIWR85IB4JoA35dNuI2kcJ5gMhAMZDAcDGcHIz71XktLl2hAtpjgnkRmq1omt2kUiokis7hmIYjP5Vau7zVnAjEtwgJAJLkdj05oAuJYXeM_Zpsf7hrkPEniWfRdQNsQE46OMEdPX61ufbdTRAGnmyc4XzjzjqaydT0q31W4W4vLOO4uCoyzuenvyKAOZu_Fg1KPyblwFJ42gZB9qbZa-mmLIiOMltwDDnHarOu6FY2ot5La0jgKgk7CTnPTqTTtF0aC9jE01rDN8wUmTqBj0_GgAbxKdVtJ7ZwpBAbI4III_wAa2_DJBtZSvcjnpk85_lVu10iHTj5lpY6coZQH3oCDg55BFXC8s2UtxaRqMcLGFye54HI5oA4vxmMalbjGQQKrGJGa7KgjcF_ABq6jWvCjamy3L30SOmAABkE1S1Cym0u0aUSQozZAZSGJODwAfYGgDjI3U6spA6ODz6YrolkycHtXIWyTTXjTyMW2tkkd-a6K3kZzkUAX-uDjikJwOhxTcsCOh9qUnjBzQA7cPX9KKblaKAO08L6sl5JNG42nIbGeeQB_MH866XCEYU5P1rza1uzaTRyQoqELgHkjtV2XxLqRbAuVUdchQf6UAd0yEkYOMg9Cev8AjWHq8WpowktWDquSQV56Vzra7qL4BvXyQORjAz7iom1e-wVa7kbkcgmgCGXUtSTAZ1V4ycAjoD2zVX7bqLoDvYk-g7Z6VbkuZXJYyMxOSSTn_PSkMvUKWPXJzigCrdtdT6ezXBYkHCk-lS6XpuoXdorWYcruOSGwAfrT7gySadMXlLKpJUHGFyBUNtaPfeXAuvPp6FQBCqg7jzk59f8ACgBmopq-nEi6L7CQAQ-QfyNZj6lcnK-c3UYOSMV1UVloWkwFLzXmnGcv5jA7j7gg1m3eqeBwSkazTSN0MOcg-2TigDEN3cOcNMxGPU0rTzOoDElSOh5qzNpccy-bYOzKRlUZgXH1Axn8KoeTdgMpjfC8HKkAfmKAAJEDnaAc9hjFIysDlWIx0xxTRIBkEnOfwp5KnBDcfWgB_wBquo8fMGwO4zT01GUYMkSse2DjFMY4YHaME_pTvlIJ6Ee1AE_9pr_zwP50VW2j1ooA2WJyGBwo4Hv-FRliELA4JPA9fwpuGUgoCCQCCACSP_retMZyCEAZieSO_vQBIJcggnAJOAO5wO341ds9Lvr6LfDGCnQknBrPwowQoBxznrxzz-dRRavfaVcma3nJUkgo4OGH07UAdInhvUT_AHE4xndSy-Hr6BQ4US8HIQ5Irlbnx5rIkKhmJIBOFOASAcDGOnSnaJ4l1nVNat4Z3mELP83UZA59aAN6ezuINLKso3MckdCB_jVP7LE8CSsAcHHB5B7c9utdD4gu4_KSFWUZALEn8hXMpcxI0kRlXaQTnPGf_rGgCCXTLCSRpHtQzdckZP61raXB4eEgikiWOboAQADj3xVFXdjgAs-B90Zz78VIdKmuUH-iSkg5BKnGPb079-9AG9LqnhzTfvPbKV7AAkfnWXefErSIQyQo056EAYFYU2gWxnP2m0IcHkSHt-PrUlrpOmiZVMSQ57sMj60ASSalaayvmNaRWhY5VkXOc-oH-FVJdGuFRnhAmB6GPnH4da6-28OWIVZC5kGMApgA_lWpbWVpbEGGBVI4DYyfzoA8ya3uo4g_lSFBnJZDgUxLklTlScDHB6V6vLLFEhMzRIncuQB-tctq8vhicmMgNMxwPs4wSf5GgDlvPH-TRVz-yIn-ZZE2nkZnjzj86KALDSsFZcgFfUdcd_T0qe202-njRxbOVkyc44x685qh5-7AALMDk4xz_hXY-Gb8XenrE4AkhJU_TjFAGOnhy_clmVQc5ALgAfgPqelTL4UncgvLApHbk5966onBIIwDQTkYNAHB6po39nygyhWBBKsBgEcf1q14aEUmvRAKuWJJ6-nT-tdRfW6Xdq0UyBlJGPUe4rndLK6HqxSdFHmMTFNj3ORQBF4js2GvzrvHl7VGO54rKgtogC7IGYHnJznFbmuzGfWZZdhwVXHIAyAB1JrKkkjiLM8kaqThhuyceowKAN3TvFdhcIkTMInHBBGPzH-FbSyiRQQxIPQg5FeTX9qnmu0Eu8KchhwRnpxWjoniC9sJkgnkEkfA3g8D2PvzQB6VJDDcx7JolkX0YZx9KxNR8LQ3KH7JII2AOEc8E5z1HNc_c_EEJuWJU4JAwCScfU1lzePb2WMGN2XJIwAByAP8aANQprWhOGUsY-CQSSpHsaq3_ifxFcsVt0WFT0xwT-XP5EVTi1me7UtLLI2AMgsSBVwSjaoUsRnGe2KAMKew1i7k3Xt84J9M5_U5qay0IW0nmmV3YnIJ9vp_nmt3JkUMSAAcMOo59KFAI3qxCk4GBx7nmgCn9k_2n_76P-FFWPtco43t-YooAikOxiyFQAcABeQB0z2yef8AJrQ8NX7Wmrlp2YJKh3E5I45zz7f4VUWeMRFCgZyQSNxHBHTHtWXKzITIoIYkkAZ9wKAPQZvFenLIQFkcAdVXGTn3xWbceO7OEnbCM9i8oU_kM157qBuJ5iFRgMKMcjOBjOPfGapf2VNKSz5GfbpQB3U_xBdyPKEIAP8ACpY_mSBWfPr9zqseDKSAd2NoXGT7ZrnItEmwmX4JwR71p2tmEDDcVx3J9-aALJLOS5ZndjnJPT-XXHpTzbkwlmAAOASSOOBx04_CrdvEpj-4pIIGM8nn_wCvUgjEjuu5SC2cdR0HrQBUSBZFjBViDzkDOfoe9R3Fovm8FVLKeoIBIHsME5B4AFXreBwdyHDR5XAOMjp_I0k6ANuUgkE5BPQd_wCdAGLJosAIf7xORtBIIx1GMU6DTVUiN4ioUkAquckjPOfpW-0Kzq4TLELkBAc5_Ciz0i7mYSLAFUvglztAIAzkn3zQBmQ2QQgsAoIyo6dO5q6AsSghxnHBwCCRnoe3_wBarcumWcU6G61K3jBTdiP5yORwPfmq9zdaRFGEt47q4lJ4LnCkZ6juD_KgBsaMY2TcGLAEk4BB6ke31GKuWelXckKPDauyAljlQFx1HzGov-EjngtyltaWtvhcfKm5umM5NZ1zqF5cqFnvJHwudhYhT2A44oA3ovD0vlJzb_dH_LYf40VymU_54v8An_8AXooAvqJTGQIYuCf4skA9xj0xj8KYEjSUFQrcYJOQPzP-BoooAJgouDgKobkjGVBz9OB3-tJsQxbAAzdz2B9MCiigBOpAIDKepORg446_54pYEXzHiYAAHJPcA9MD09_cUUUAaEdlJLEDFHJKSdo8pSeOxz0B-pqz_Yl6DMXEUaoASZnC8kHgY9gKKKAK9u-lW1w_2rUVKjokCknpjg8jt60y51fSYgVtNMuJyWDb5mxu9RgZoooAiHi3UVUxwrb2iAdI1BYAdOTk5rJuNTnnLuZ5JgWLFSxGSc5P1NFFAEUUpkAYxmMkZAB4x-FWnlbAYMAMYwTySaKKAIYovODBmcYySTgn271NJaKxwsoIGCAXzg_QgUUUAR7X_wCfmP8AMUUUUAf_2Q==\n",
    "https://data.labs.pdok.nl/rce/id/image/20636163 http://dbpedia.org/ontology/thumbnail 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\n",
    "\"\"\"\n",
    "\n",
    "def decode_base64_image(encoded_str,log_note='pass values to decode_base_64_jpg'): # - to +, _ to /\n",
    "    \"\"\"\n",
    "    encoded_str: url safe base 64 jpg string --> image bytes string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_bytes = base64.urlsafe_b64decode(encoded_str)\n",
    "        image_obj = Image.open(BytesIO(image_bytes))\n",
    "        image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "        return image, image_obj\n",
    "    except Exception as e:\n",
    "        logging.error(f\"{e} error encoding image at {log_note}\")\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "def save_bytes_to_jpg(image_bytes, item_num=0,folder='Downloads/ml4g/image_data/',name='Fred'):\n",
    "    filename = f'{folder}{name}_{item_num}.jpg'\n",
    "    with open(filename, 'wb') as img_file:\n",
    "        img_file.write(image_bytes)\n",
    "\n",
    "\n",
    "image, image_bytes = encode_image(byte_str, transform_temp)\n",
    "save_img(image_bytes)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ab0a94-1c17-4fb4-9e9f-ef3a746b1e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1895, -0.0327, -0.1373,  ...,  1.0784,  1.1307,  0.7908],\n",
       "         [ 0.0850,  0.1373, -0.0327,  ...,  1.0784,  1.1307,  0.8562],\n",
       "         [ 0.0065,  0.0196, -0.0850,  ...,  0.9216,  0.9739,  0.8170],\n",
       "         ...,\n",
       "         [ 0.0327,  0.2549,  0.1503,  ...,  0.6993,  0.7516,  0.8301],\n",
       "         [-0.0196,  0.2157,  0.1111,  ...,  0.5686,  0.6078,  0.7386],\n",
       "         [-0.1242,  0.0719, -0.0065,  ...,  0.6471,  0.6732,  0.8301]],\n",
       "\n",
       "        [[-0.1895, -0.0327, -0.1373,  ...,  1.0784,  1.1307,  0.7908],\n",
       "         [ 0.0850,  0.1373, -0.0327,  ...,  1.0784,  1.1307,  0.8562],\n",
       "         [ 0.0065,  0.0196, -0.0850,  ...,  0.9216,  0.9739,  0.8170],\n",
       "         ...,\n",
       "         [ 0.0327,  0.2549,  0.1503,  ...,  0.6993,  0.7516,  0.8301],\n",
       "         [-0.0196,  0.2157,  0.1111,  ...,  0.5686,  0.6078,  0.7386],\n",
       "         [-0.1242,  0.0719, -0.0065,  ...,  0.6471,  0.6732,  0.8301]],\n",
       "\n",
       "        [[-0.1895, -0.0327, -0.1373,  ...,  1.0784,  1.1307,  0.7908],\n",
       "         [ 0.0850,  0.1373, -0.0327,  ...,  1.0784,  1.1307,  0.8562],\n",
       "         [ 0.0065,  0.0196, -0.0850,  ...,  0.9216,  0.9739,  0.8170],\n",
       "         ...,\n",
       "         [ 0.0327,  0.2549,  0.1503,  ...,  0.6993,  0.7516,  0.8301],\n",
       "         [-0.0196,  0.2157,  0.1111,  ...,  0.5686,  0.6078,  0.7386],\n",
       "         [-0.1242,  0.0719, -0.0065,  ...,  0.6471,  0.6732,  0.8301]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd4067-852c-4807-bc0b-417be80a827f",
   "metadata": {},
   "source": [
    "# Convert raw values to consistent feature vectors for the encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae313d-9b47-4d52-9375-7994e6691bf7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e18f5-e220-4ce1-b8ce-4712518ea3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f373b9c-d818-4e40-96b7-f75b9d75ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_dict = defaultdict(lambda: {'text': [], 'image': [], 'num': [], 'spatial': [], 'temporal': []})\n",
    "\n",
    "# for s, p, o in graph:\n",
    "#     try:\n",
    "#         edge_type = inv_edge_map[p.identifier]\n",
    "#         node_id = inv_node_map[s.identifier]\n",
    "#         feature_id = inv_node_map[o.identifier]\n",
    "#     except KeyError:\n",
    "#         logging.error(f'Error in edge map creation: no matching key for triple {s.identifier}, {p.identifier}, {o.identifier} in node/edge maps' )\n",
    "    \n",
    "#     if edge_map[edge_type] == 'text':\n",
    "#         feature_dict[node_id]['text'].append(feature_id)\n",
    "#     elif edge_map[edge_type] == 'image':\n",
    "#         feature_dict[node_id]['image'].append(feature_id)\n",
    "#     elif edge_map[edge_type] == 'num':\n",
    "#         feature_dict[node_id]['num'].append(feature_id)\n",
    "#     elif edge_map[edge_type] == 'spatial':\n",
    "#         feature_dict[node_id]['spatial'].append(feature_id)\n",
    "#     elif edge_map[edge_type] == 'temporal':\n",
    "#         feature_dict[node_id]['temporal'].append(feature_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f36262-bc94-469f-b2e8-0cbba3f9e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv_edge_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab6e2a-45f8-408e-8ea0-ece3af571399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_dataset.max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665e545-55fd-4e43-b244-879232d93871",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b3e27-8047-46ca-8954-c4566f66aa2e",
   "metadata": {},
   "source": [
    "images.memorix.nlrcedownloadfullsize\n",
    "bag.basisregistraties.overheid.nlbagidgeometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34660fba-188d-4dfd-902e-b9c2173915e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_spatial(batch):\n",
    "   padded = nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "   padded = padded.permute(0, 2, 1)\n",
    "   return padded\n",
    "\n",
    "def collate_text(batch, min_size=12):\n",
    "    if batch[0].size(0) < min_size:\n",
    "        pad_num = min_size - batch[0].size(0)\n",
    "        pads = torch.zeros(pad_num, dtype=torch.long, device=device)\n",
    "        batch[0] = torch.cat([batch[0], pads], dim=0)\n",
    "    return nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "spatial_dataloader = DataLoader(spatial_dataset, batch_size=batch_size, collate_fn=collate_spatial)\n",
    "text_dataloader_s = DataLoader(text_dataset_s, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_m = DataLoader(text_dataset_m, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_l = DataLoader(text_dataset_l, batch_size=batch_size, collate_fn=collate_text)\n",
    "image_dataloader = DataLoader(image_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e1e85-5dd0-491e-b949-60ca053ce7f1",
   "metadata": {},
   "source": [
    "# edges --> adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebb130-d436-4b20-bf1b-6b25f81f1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxl = 0\n",
    "# for string in string_set_l:\n",
    "#     if len(string) > maxl:\n",
    "#         maxl = len(string)\n",
    "#         print(len(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba1742-58c3-401f-8d16-ac752dca57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import defaultdict\n",
    "# adjacency = defaultdict(list)\n",
    "# inv_edge_map\n",
    "\n",
    "# for s, p, o in graph:\n",
    "#     edge_id = inv_edge_map[p.identifier]\n",
    "#     s_id = inv_node_map[s.identifier]\n",
    "#     o_id = inv_node_map[o.identifier]\n",
    "#     adjacency[edge_id].append([s_id, o_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddefd962-7c32-4ebd-8ee1-e19679aebb07",
   "metadata": {},
   "source": [
    "# Set up encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7198e43b-a426-4ee6-8d74-6ea1b2652130",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import torch.functional as f\n",
    "import torch.nn as nn\n",
    "\n",
    "# ]1] temporal conv\n",
    "# Layer Filters Kernel Padding Pool\n",
    "# 1 64 7 3 max(2/2)\n",
    "# 2 64 7 3 max(2/2)\n",
    "# 3 64 7 3 -\n",
    "# 4 64 7 2 max(·)\n",
    "# Layer Dimensions\n",
    "# 5 512\n",
    "# 6 128\n",
    "# 7 128\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=102, dropout=0.4, size_type='medium'): #filters = 64\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.size_type = size_type.lower()\n",
    "        self.mlp_hidden_dim = 1024 if self.size_type == 'large' else 512 if self.size_type == 'medium' else 256\n",
    "        self.tcnn_hidden_dim = 128 if self.size_type == 'large' else 64 if self.size_type == 'medium' else 32\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            embed_dim, self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.pool1 = nn.MaxPool1d(2,2)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.pool2 = nn.MaxPool1d(2,2)\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm3 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        self.conv4 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=2)\n",
    "        \n",
    "        self.norm4 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop4 = nn.Dropout(dropout)\n",
    "        self.pool4 = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.lin1 = nn.Linear(self.tcnn_hidden_dim, self.mlp_hidden_dim)\n",
    "        self.lin2 = nn.Linear(self.mlp_hidden_dim, embed_dim)\n",
    "        self.lin3 = nn.Linear(embed_dim , embed_dim)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        residual = embedded\n",
    "        #conv\n",
    "        \n",
    "        x = self.conv1(embedded)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop4(x)\n",
    "        # if residual.size(1) != x.size(1):\n",
    "        #   residual = nn.utils.rnn.pad_sequence([residual, x], batch_first=True)\n",
    "        x = self.pool4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #ffnn\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        # x += residual\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "# [2] MobileNet \n",
    "# Type / Stride Filter Shape Input Size\n",
    "# Conv / s2 3 × 3 × 3 × 32 224 × 224 × 3\n",
    "# Conv dw / s1 3 × 3 × 32 dw 112 × 112 × 32\n",
    "# Conv / s1 1 × 1 × 32 × 64 112 × 112 × 32\n",
    "# Conv dw / s2 3 × 3 × 64 dw 112 × 112 × 64\n",
    "# Conv / s1 1 × 1 × 64 × 128 56 × 56 × 64\n",
    "# Conv dw / s1 3 × 3 × 128 dw 56 × 56 × 128\n",
    "# Conv / s1 1 × 1 × 128 × 128 56 × 56 × 128\n",
    "# Conv dw / s2 3 × 3 × 128 dw 56 × 56 × 128\n",
    "# Conv / s1 1 × 1 × 128 × 256 28 × 28 × 128\n",
    "# Conv dw / s1 3 × 3 × 256 dw 28 × 28 × 256\n",
    "# Conv / s1 1 × 1 × 256 × 256 28 × 28 × 256\n",
    "# Conv dw / s2 3 × 3 × 256 dw 28 × 28 × 256\n",
    "# Conv / s1 1 × 1 × 256 × 512 14 × 14 × 256\n",
    "# 5×\n",
    "# Conv dw / s1 3 × 3 × 512 dw 14 × 14 × 512\n",
    "# Conv / s1 1 × 1 × 512 × 512 14 × 14 × 512\n",
    "# Conv dw / s2 3 × 3 × 512 dw 14 × 14 × 512\n",
    "# Conv / s1 1 × 1 × 512 × 1024 7 × 7 × 512\n",
    "# Conv dw / s2 3 × 3 × 1024 dw 7 × 7 × 1024\n",
    "# Conv / s1 1 × 1 × 1024 × 1024 7 × 7 × 1024\n",
    "# Avg Pool / s1 Pool 7 × 7 7 × 7 × 1024\n",
    "# FC / s1 1024 × 1000 1 × 1 × 1024\n",
    "# Softmax / s1 Classifier 1 × 1 × 1000\n",
    "# Table 2. Resource Per Layer Type\n",
    "# Type Mult-Adds Parameters\n",
    "# Conv 1 × 1 94.86% 74.59%\n",
    "# Conv DW 3 × 3 3.06% 1.06%\n",
    "# Conv 3 × 3 1.19% 0.02%\n",
    "# Fully Connected 0.18% 24.33%\n",
    "\n",
    "#idea: pass identity vector with batches to encoders (don't process, just pass back), use that to map embeddings to nodes later\n",
    "class ImageEncoder(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 256, stride=2, alpha=0.5)\n",
    "        self.image6 = MobileBlock(256, 512, stride=1, alpha=0.5)\n",
    "        self.image7 = MobileBlock(512, 512, stride=2, alpha=0.5)\n",
    "        self.image8 = MobileBlock(512, 1024, stride=1, alpha=0.5)\n",
    "        self.middle_conv = nn.Sequential(*[MobileBlock(\n",
    "                1024, 1024, stride=1, alpha=0.5) for i in range(5)])\n",
    "        \n",
    "        self.image9 = MobileBlock(1024, 1024, stride=1, alpha=0.5)\n",
    "        \n",
    "        self.image10 = MobileBlock(1024, 2048, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(1024,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        # middle_conv = [MobileBlock() for i in range(5)]\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.image(batch)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.image6(x)\n",
    "        x = self.image7(x)\n",
    "        x = self.image8(x)\n",
    "        x = self.middle_conv(x)\n",
    "        x = self.image9(x)\n",
    "        x = self.image10(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "class MobileBlock(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_size, stride=1, normal_conv=False, alpha=0.5, dilation=1):\n",
    "        super(MobileBlock, self).__init__()\n",
    "        old_in_channels = in_channels\n",
    "        in_channels = int(alpha * in_channels)\n",
    "        embedding_size = int(alpha * embedding_size)\n",
    "        # depthwise conv\n",
    "        self.normal_conv = normal_conv\n",
    "        if self.normal_conv:\n",
    "            # in_channels = in_channels//2\n",
    "            pass\n",
    "        # 3x3x3x32\n",
    "        self.conv = nn.Conv2d(old_in_channels, in_channels, 3, padding=1, stride=2, dilation=1)\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, 3, stride=stride,\n",
    "                                   padding=1, groups=in_channels)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        # pointwise conv\n",
    "        self.pointwise = nn.Conv2d(in_channels, embedding_size, 1, stride=1, padding=0)\n",
    "        self.norm2 = nn.BatchNorm2d(embedding_size)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(self.conv.kernel_size)\n",
    "        if self.normal_conv:\n",
    "            x = self.conv(x)\n",
    "        # print(x.size())\n",
    "        x = self.depthwise(x)\n",
    "        # print(x.size())\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.pointwise(x)\n",
    "        # print(x.size())\n",
    "        x = self.norm2(x)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [3] temporal conv\n",
    "# layer filters kernel padding pool\n",
    "# 1 16 5 2 max(3/3)\n",
    "# 2 32 5 2 -\n",
    "# 3 64 5 2 avg(·)\n",
    "# layer dimensions\n",
    "# 4 512\n",
    "# 5 128\n",
    "# 6 128\n",
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128, dropout=0.2):\n",
    "        super(SpatialEncoder, self).__init__()\n",
    "        \n",
    "        # temp cnn\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=3, ceil_mode=True)\n",
    "        self.norm1 = nn.BatchNorm1d(16)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm1d(32)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        \n",
    "        # dense\n",
    "        self.lin1 = nn.Linear(64, 512)\n",
    "        self.lin2 = nn.Linear(512, 128)\n",
    "        self.lin3 = nn.Linear(128, embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, spatial):\n",
    "        # x = self.pad(spatial, batch_first=True)\n",
    "        x = self.conv1(spatial)\n",
    "        x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #flatten\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# [4] mlp h == in dim 1st col, out 2nd col\n",
    "# XSD:gYear 6 2\n",
    "# XSD:date 10 4\n",
    "# XSD:dateTime 14 6\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super(TemporalEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = input_dim\n",
    "        self.out_dim = 2 if input_dim <= 6 else 4 if input_dim <= 10 else 6\n",
    "\n",
    "    def forward():\n",
    "      pass\n",
    "\n",
    "#[5] one to one encoding for numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4538e5f3-8539-4d0b-880a-01cf56b4ab77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950acd50-b7a4-4182-80df-a12f636e2211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f265f500-083e-471c-8d03-8345a341150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_dataset.__getitem__(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7792a39-22ac-4b81-9c38-3ac48bb7313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in text_dataloader:\n",
    "#     print(batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8785c01e-3f34-43cb-904e-4e063711920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_s, text_encoder_m, text_encoder_l = TextEncoder(128, size_type='s'), TextEncoder(128, size_type='m'), TextEncoder(128, size_type='l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f57d2aca-5fd6-42a5-8cb7-4ae3acb67432",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5000])\n",
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m----> 6\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtext_encoder_l\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m sz \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[31], line 58\u001b[0m, in \u001b[0;36mTextEncoder.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     55\u001b[0m residual \u001b[38;5;241m=\u001b[39m embedded\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#conv\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# spatial = SpatialEncoder(2)\n",
    "# batch = [spatial_dataset.__getitem__(i) for i in range(3)]\n",
    "for i, batch in enumerate(text_dataloader_l):\n",
    "    if i % 1000 == 0:\n",
    "        print(batch.size())\n",
    "    out = text_encoder_l(batch)\n",
    "    sz = batch.size()\n",
    "    if i % 1000 == 0:\n",
    "        print(out.size())\n",
    "\n",
    "# [spatial_dataset.__getitem__(i).size() for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21db6219-511c-4659-a546-132917c96d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 53])\n",
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "spatial = SpatialEncoder(2)\n",
    "for i, batch in enumerate(spatial_dataloader):\n",
    "    if i % 1000 == 0:\n",
    "        print(batch.size())\n",
    "    out = spatial(batch)\n",
    "    sz = batch.size()\n",
    "    if i % 1000 == 0:\n",
    "        print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "815272d3-fa0e-44ff-a736-9248c9535840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691814b4-fccd-441a-9cf8-cde36cdd9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = ImageEncoder(128)\n",
    "for i, batch in enumerate(image_dataloader):\n",
    "    if i< 1000:\n",
    "        out = image_encoder(batch)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3aada308-e35b-429d-98f8-166c7713a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 56, 56])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada28a3f-51cb-41df-9385-e82033b45cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d83ed4-4541-4052-ae74-ed7505732a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_inputs = [string for string in ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd8c55-3d51-4b96-8b8c-f4d38741513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a9217-e178-4ecc-ae9d-41270a55e860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eac1826e-ef95-4c5b-8091-1a084293f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: map encodings to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb95125-0c9c-40db-8614-bcfdeebb3081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A = []\n",
    "# for i,edge in edge_map.items()[:2]:\n",
    "#     edge_class_adjacency = adjacency[i]\n",
    "#     edge_class_adjacency = torch.tensor(edge_class_adjacency, dtype=torch.long).t().to(device)\n",
    "#     values = torch.ones(edge_class_adjacency.size(1), device=device)\n",
    "#     A = torch.sparse_coo_tensor(edge_class_adjacency, values, (num_nodes, num_nodes), device=device)\n",
    "\n",
    "#     A = normalize_adjacency(A)\n",
    "#     A_list.append(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193278ef-f16f-4a1b-8845-0f103501bc9f",
   "metadata": {},
   "source": [
    " 'isalnum',\n",
    " 'isalpha',\n",
    " 'isascii',\n",
    " 'isdecimal',\n",
    " 'isdigit',\n",
    " 'isidentifier',\n",
    " 'islower',\n",
    " 'isnumeric',\n",
    " 'isprintable',\n",
    " 'isspace',\n",
    " 'istitle',\n",
    " 'isupper',"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
