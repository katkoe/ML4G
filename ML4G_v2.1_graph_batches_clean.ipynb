{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ef63b0-c9f1-44b7-ae9e-09210b4395c9",
   "metadata": {},
   "source": [
    "# Create graph from n-triple file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b7f1d5-1f5a-44d8-b1f2-764a729e737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph\n",
    "import logging\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "data_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped.nt'\n",
    "mini_loc = './Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_stripped_mini.nt'\n",
    "test_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_test_set.nt\"\n",
    "folder = './Downloads/ml4g'\n",
    "subsample_chance = 1\n",
    "def create_mini_nt(infile, outfile):\n",
    "    with open(infile, 'r', encoding='utf-8') as inp, open(outfile, 'w', encoding='utf-8') as out:\n",
    "        for i, line in enumerate(inp):\n",
    "            if random.random() <= subsample_chance:\n",
    "                out.write(line)\n",
    "\n",
    "\n",
    "\n",
    "def create_new_graph(path, batch_size = 10000, test=True):\n",
    "    \"\"\"\n",
    "    Reads .nt file to produce rdflib graph object containing all tuples.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "    filename='rdf_parsing_errors.log',\n",
    "    filemode='w')\n",
    "    \n",
    "    graph = Graph()\n",
    "    batch_num = 0\n",
    "    i = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        while True:\n",
    "            batch = []\n",
    "            try:\n",
    "                \n",
    "                [batch.append(next(f)) for j in range(batch_size)]\n",
    "                i += j\n",
    "            except:\n",
    "                pass\n",
    "            if not batch:\n",
    "                break\n",
    "            batch_num += 1\n",
    "            nt_string = ''.join(batch)\n",
    "            try:\n",
    "                graph.parse(data=nt_string, format='nt')\n",
    "                if test:\n",
    "                    graph = Graph()\n",
    "            except ParseError as e:\n",
    "                logging.error(f\"in batch: {batch_num}:\\npproblematic data:\\n\\n{batch}\\n\\n\")\n",
    "                check(batch, batch_num, test=test)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def check(batch, batch_num, test = True):\n",
    "    graph = Graph()\n",
    "    for i, line in enumerate(batch):\n",
    "        try:\n",
    "            graph.parse(line)\n",
    "        except Exception as e:\n",
    "            logging.error(f'in line: {i}:\\n{line}\\n{e}')\n",
    "\n",
    "def to_connected_small(g,subsample_chance=0.8, barrier=100,max_len=5000, verbose=True, save=True):\n",
    "    \"\"\"\n",
    "    Creates a small graph for development.\n",
    "    Uses similar logic to create_small_graphs()\n",
    "    \"\"\"\n",
    "    obj_set = set()\n",
    "    mini_graph = Graph()\n",
    "    i=0\n",
    "    j=0\n",
    "    for s,p,o in g:\n",
    "        if j > max_len:\n",
    "            break\n",
    "        i+=1\n",
    "        if random.random() >= subsample_chance and (i<barrier or s in obj_set or o in obj_set):\n",
    "            obj_set.add(s)\n",
    "            obj_set.add(o)\n",
    "            j+=1\n",
    "            mini_graph.add(triple=(s,p,o))\n",
    "    print(f\"{i}-->{j}\") if verbose else 0\n",
    "    return mini_graph\n",
    "# graph = to_connected_small(g)\n",
    "\n",
    "# g=create_new_graph(data_loc, test=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd57fe2-fe8d-461e-a2bc-5b6e20609a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pickle\n",
    "import os\n",
    "import rdflib\n",
    "import logging\n",
    "folder = './Downloads/ml4g'\n",
    "\n",
    "pickle_file_path = os.path.join(folder, 'rdf_graph.pkl')\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "\n",
    "graph = to_connected_small(graph, max_len=4000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146bf08-a367-406d-8a00-2f0c4c89d067",
   "metadata": {},
   "source": [
    "### grab or load train, test, val graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e939d7fe-7002-4273-a424-775c4cdda969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# with open('graph_test.pkl', 'wb' ) as f:\n",
    "#     pickle.dump(graph_test,f)\n",
    "\n",
    "with open('reproduction_stripped_graph.pkl', 'wb' ) as f:\n",
    "    pickle.dump(graph,f)\n",
    "\n",
    "# with open('graph_train.pkl', 'wb' ) as f:\n",
    "#     pickle.dump(graph_train,f)\n",
    "\n",
    "# with open('graph_val.pkl', 'wb' ) as f:\n",
    "#     pickle.dump(graph_val,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d5b489a-42cd-4f78-bd7d-d8945ea010ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import defaultdict\n",
    "test_label_set = set()\n",
    "train_label_set = set()\n",
    "val_label_set = set()\n",
    "\n",
    "\n",
    "test_node_label_map = defaultdict(list)\n",
    "train_node_label_map = defaultdict(list)\n",
    "val_node_label_map = defaultdict(list)\n",
    "\n",
    "\n",
    "test_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_test_set.nt\"\n",
    "train_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_train_set.nt\"\n",
    "val_loc = \"./Downloads/ml4g/dmg/mmkg/dmg/scripts/dmg777k_valid_set.nt\"\n",
    "\n",
    "graph_test = create_new_graph(test_loc, test=False)\n",
    "graph_train = create_new_graph(train_loc, test=False)\n",
    "graph_val = create_new_graph(val_loc, test=False)\n",
    "\n",
    "for s,p,o in graph_test:\n",
    "    test_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        test_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "for s,p,o in graph_train:\n",
    "    train_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        train_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "\n",
    "for s,p,o in graph_val:\n",
    "    val_label_set.add(o.identifier)\n",
    "    try:\n",
    "        node_id = inv_node_map[s.identifier]\n",
    "        val_node_label_map[node_id] = o.identifier\n",
    "    except: #not everything has to be in node_set with mini graph\n",
    "        pass\n",
    "\n",
    "subj_set = set()\n",
    "for s,p,o in graph_train:\n",
    "    subj_set.add(s.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefd34b-7e40-4583-8b6c-3468a39fa917",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49d778-bc05-44ee-a7f5-cf50c2b9eb2e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892957c9-1e49-4af8-92aa-1c1d62bb5fd0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# from rdflib import Graph\n",
    "# import random\n",
    "# import pickle\n",
    "# def create_small_graphs(g,subsample_chance=0.8, barrier=100,max_len=5000, verbose=True, save=True, graph_loc=f'{folder}/graphs/'):\n",
    "#     \"\"\"\n",
    "#     Creates batches to be processed by preprocess().\n",
    "#     Generator logic is currently unimplemented.\n",
    "#     \"\"\"\n",
    "#     done=False\n",
    "#     # while not done:\n",
    "#     obj_set = set()\n",
    "#     mini_graph = Graph()\n",
    "#     i=0\n",
    "#     j=0\n",
    "#     graph_num=0\n",
    "#     for s,p,o in g:\n",
    "#         if j > max_len:\n",
    "#             graph_name = f'graph_{graph_num}.pkl'\n",
    "#             if save:\n",
    "#                 print(f\"{i}-->{j}\") if verbose else 0\n",
    "#                 with open(graph_loc+graph_name, 'wb') as f:\n",
    "#                     pickle.dump(mini_graph, f)\n",
    "#             mini_graph = Graph()\n",
    "#             i,j = 0,0\n",
    "#             graph_num += 1\n",
    "#         i+=1\n",
    "#         if random.random() >= subsample_chance and (i<barrier or s in obj_set or o in obj_set):\n",
    "#             obj_set.add(s)\n",
    "#             obj_set.add(o)\n",
    "#             j+=1\n",
    "#             mini_graph.add(triple=(s,p,o))\n",
    "        \n",
    "        \n",
    "        \n",
    "#     # return mini_graph\n",
    "# create_small_graphs(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80590000-b2f4-4a06-80fc-d70ae965d536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a6ab5-ee2e-463d-a94e-875f1a04d676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa37dd0-337b-45ea-8ad0-50b9be965bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0fb2b0-b4fc-42b1-8a3c-75e7637b6bc0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import defaultdict\n",
    "import torchvision.transforms as trv\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from string import printable\n",
    "import pickle\n",
    "import base64\n",
    "\n",
    "def decode_base64_image(encoded_str,log_note='pass values to decode_base_64_jpg'): # - to +, _ to /\n",
    "        \"\"\"\n",
    "        encoded_str: url safe base 64 jpg string --> image bytes string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_bytes = base64.urlsafe_b64decode(encoded_str)\n",
    "            image_obj = Image.open(BytesIO(image_bytes))\n",
    "            image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "            return image, image_obj\n",
    "        except Exception as e:\n",
    "            # logging.error(f\"{e} error encoding image at {log_note}\")\n",
    "            print(e)\n",
    "            return None, None\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, strings, character_map, max_length=5000):\n",
    "        # self.strings = strings\n",
    "        self.character_map = character_map\n",
    "        self.max_length = max_length\n",
    "        self.strings = strings\n",
    "        self.ids = []\n",
    "        self.pad = False\n",
    "        ids = []\n",
    "        for string in strings:\n",
    "            try:\n",
    "                self.ids.append(inv_node_map[string])\n",
    "            except KeyError:\n",
    "                self.ids.append(0) #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strings)\n",
    "\n",
    "    def __getitem__(self, i, pad=True):\n",
    "        s = self.strings[i]\n",
    "        tokens = tokenize_string(s, self.character_map, max_len=self.max_length)\n",
    "\n",
    "        return torch.tensor([self.ids[i]]+tokens, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, encoded_images, transform):\n",
    "        self.encoded_images = encoded_images\n",
    "        self.transform = transform\n",
    "        self.ids = [inv_node_map[encoded_image] for encoded_image in encoded_images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        encoded_str = self.encoded_images[i]\n",
    "        image, _ = decode_base64_image(encoded_str, log_note=f\"inside generator, image nr: {i}\")\n",
    "        # print(f\"img:{image}\")\n",
    "        if image:\n",
    "            # print(image)\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # print(image)\n",
    "            image = torch.zeros(128)\n",
    "            # image = torch.zeros(3, 224, 224)\n",
    "        return self.ids[i], image\n",
    "\n",
    "\n",
    "class NumericalDataset(Dataset):\n",
    "    def __init__(self, numbers, log_scale=False):\n",
    "        self.numbers = numbers\n",
    "        self.log_scale = log_scale\n",
    "        try:\n",
    "            self.max_num = max(numbers)\n",
    "        except:\n",
    "            self.max_num = 0.1\n",
    "        self.ids = [inv_node_map[str(number)] for number in numbers]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        n = self.numbers[i]\n",
    "        norm = n / self.max_num\n",
    "        norm = math.log(norm + 1e-8) if self.log_scale else norm\n",
    "        return self.ids[i], torch.tensor([norm], dtype=torch.float32, device=device)\n",
    "\n",
    "class DateTimeDataset(Dataset):\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates\n",
    "        self.ids = [inv_node_map[date] for date in dates]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_date(date)\n",
    "        return self.ids[i], date_feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class YearDataset(Dataset):\n",
    "    def __init__(self, dates):\n",
    "        \n",
    "        self.dates = dates\n",
    "        self.ids = [inv_node_map[date] for date in dates]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        date = self.dates[i]\n",
    "        date_feature = encode_year(date)\n",
    "        return self.ids[i], date_feature\n",
    "\n",
    "\n",
    "def process_point(point_str, highest_x, highest_y):\n",
    "    if 'POINT' in point_str:\n",
    "        point_str = point_str.split('POINT(')[1].split('))')[0]\n",
    "    elif 'Point' in point_str:\n",
    "        point_str = point_str.split('Point(')[1].split('))')[0]\n",
    "    point_list = point_str.strip(')').strip('(').split()\n",
    "    point = [float(coord) for coord in point_list]\n",
    "    point.extend([0,0,0])\n",
    "    point = tuple(point)\n",
    "    \n",
    "    point_x,point_y,_,_,_ = point\n",
    "    highest_x = point_x if point_x > highest_x else highest_x\n",
    "    highest_y = point_y if point_y > highest_y else highest_y\n",
    "    return point, highest_x, highest_y\n",
    "\n",
    "    \n",
    "def process_multipoly(multipoly_str, max_x, max_y):\n",
    "    \"\"\"\n",
    "    Delegates polygon processing for multipolygons\n",
    "    \"\"\"\n",
    "    multipoly_str = multipoly_str.split('(((')[-1].split(')))')[0]\n",
    "    sub_polys = multipoly_str.split(') (')\n",
    "    multipoly = []\n",
    "    for i, sub_poly in enumerate(sub_polys):\n",
    "        aligned_sub_poly = 'POLYGON ((' + sub_poly + '))'\n",
    "        multipoly.append(get_num_data(aligned_sub_poly, max_x, max_y, last_multi=i==len(sub_polys)))\n",
    "    return multipoly\n",
    "        \n",
    "\n",
    "def get_num_data(poly_str, maximum_x, maximum_y, last_multi=False):\n",
    "    \"\"\"\n",
    "    Processes polygons\n",
    "    str --> tuples, min/max data\n",
    "    \"\"\"\n",
    "    poly_str = poly_str.split('(')[-1].split(')')[0]\n",
    "    poly_combi_str_list = [poly for poly in poly_str.split(',')]\n",
    "    try:\n",
    "        poly_tupled = [(float(poly.split()[0]),float(poly.split()[1]),1,0,0) if i < len(poly_combi_str_list)-1  else (float(poly.split()[0]),float(poly.split()[1]),0,0,1) if last_multi else (float(poly.split()[0]),float(poly.split()[1]),0,1,0)\n",
    "                       for i,poly in enumerate(poly_combi_str_list)]\n",
    "    except ValueError:\n",
    "        poly_tupled = [(float(poly.strip(')').strip('(').split()[0]),float(poly.strip(')').strip('(').split()[1]),1,0,0) if i < len(poly_combi_str_list)-1  else (float(poly.split()[0]),float(poly.split()[1]),0,0,1) if last_multi else (float(poly.split()[0]),float(poly.split()[1]),0,0,1)\n",
    "                       for i,poly in enumerate(poly_combi_str_list)]\n",
    "    x_max, y_max = max([x for x,y,_,_,_ in poly_tupled]), max([y for x, y,_,_,_ in poly_tupled])\n",
    "\n",
    "    x_max =  x_max if x_max > maximum_x else maximum_x\n",
    "    y_max =  y_max if y_max > maximum_y else maximum_y\n",
    "    x_mean = sum([tup[0] for tup in poly_tupled])/len(poly_tupled)\n",
    "    y_mean = sum([tup[1] for tup in poly_tupled])/len(poly_tupled)\n",
    "    return poly_tupled, x_mean, y_mean, x_max, y_max\n",
    "\n",
    "def tokenize_string(s, character_map, max_len=5000):\n",
    "    s = s[:max_len]\n",
    "    tokens = [character_map[char] for char in s]\n",
    "    return tokens #\n",
    "\n",
    "def encode_image(encoded_str, transform):\n",
    "    #preprocess for encoder[2](2 layer cnn -->embedding_dim)\n",
    "    image, img_bytes = decode_base64_image(encoded_str)\n",
    "    return transform(image), img_bytes\n",
    "\n",
    "def encode_num(n, max_num, log=False):\n",
    "    v = n/max_num\n",
    "    v = math.log(v) if log else v\n",
    "    return torch.tensor((v),dtype=torch.float32, device=device)\n",
    "\n",
    "def encode_polygon(poly, global_mean_x, global_mean_y, x_max, y_max):\n",
    "    poly_tensor_x = (global_mean_x-torch.tensor([ point[0] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/x_max\n",
    "    poly_tensor_y = (global_mean_y-torch.tensor([ point[1] for point in poly], \n",
    "                                                dtype=torch.float32, device=device))/y_max\n",
    "    return torch.stack((poly_tensor_x,poly_tensor_y),dim=0)\n",
    "\n",
    "def encode_point(point, max_x, max_y):\n",
    "    \n",
    "    point = torch.tensor(point,dtype=torch.float32, device=device)\n",
    "    div = torch.tensor((max_x,max_y),dtype=torch.float32, device=device)\n",
    "    return point/div\n",
    "\n",
    "def encode_date(date):\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    split_str = date.split('-')\n",
    "    years_str = split_str[0]\n",
    "    month_str = split_str[1]\n",
    "    day_str = split_str[2]\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    months = cyclical(int(month_str), 12)\n",
    "    days = cyclical(int(day_str), 31)\n",
    "    return torch.cat((centuries, decades, years, months, days), dim=0)\n",
    "\n",
    "def encode_year(year):\n",
    "    #preprocess for temporal encoder[4](? layer ffnn -->embedding_dim)\n",
    "    def cyclical(num, max_num, epsilon = 1e-8):\n",
    "        # cyclical: [sine((2pi * X)/max_num_of_cycle) cos((2pi * X)/max_num_of_cycle)]\n",
    "        return torch.tensor([math.sin((2 * math.pi * num)/max_num)+epsilon, \n",
    "                             math.cos((2 * math.pi * num)/max_num)+epsilon],\n",
    "                            dtype=torch.float32, device=device)\n",
    "        \n",
    "    def norm_cent(num):\n",
    "        # non-cyclical only centuries: normalized from -99 to 99 (-9999 bc to 9999 ac)\n",
    "        return torch.tensor((num + 99)/198,dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    years_str = year\n",
    "    centuries = norm_cent(int(years_str[:-2]))\n",
    "    decades = cyclical(int(years_str[-2]), 10)\n",
    "    years = cyclical(int(years_str[-1]), 10)\n",
    "    return torch.cat((centuries, decades, years), dim=0)\n",
    "\n",
    "\n",
    "class SpatialDataset(Dataset):\n",
    "    def __init__(self, spatial_data, global_mean_x, global_mean_y, x_max, y_max, ids):\n",
    "        self.spatial_data = spatial_data\n",
    "        self.global_mean_x = global_mean_x\n",
    "        self.global_mean_y = global_mean_y\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "        try:\n",
    "            self.max_len = max([len(data) for data in spatial_data])\n",
    "        except:\n",
    "            self.max_len = 0\n",
    "        self.ids = ids#[inv_node_map[spatial_datum] for spatial_datum in spatial_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spatial_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        spatial = self.spatial_data[i]\n",
    "        spatial_tensors = []\n",
    "        if isinstance(spatial, list):\n",
    "            for x,y,a,b,c in spatial:\n",
    "                \n",
    "                x = (x - self.global_mean_x) / self.x_max\n",
    "                y = (y - self.global_mean_y) / self.y_max\n",
    "                coord_tensor = torch.tensor([x,y,a,b,c], device=device)\n",
    "                spatial_tensors.append(coord_tensor)\n",
    "            spatial_tensor = torch.stack(spatial_tensors,dim=0)\n",
    "        elif isinstance(spatial, tuple):\n",
    "            x, y,_,_,_ = spatial\n",
    "            spatial_tensor = torch.tensor([(x / self.x_max, y / self.y_max,0,0,0)], dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            spatial_tensor = torch.zeros((1, 2), dtype=torch.float32, device=device)\n",
    "        return self.ids[i], spatial_tensor\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def save_bytes_to_jpg(image_bytes, item_num=0,folder='Downloads/ml4g/image_data/',name='Fred'):\n",
    "        filename = f'{folder}{name}_{item_num}.jpg'\n",
    "        with open(filename, 'wb') as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "        \n",
    "        class DateTimeDataset(Dataset):\n",
    "            def __init__(self, dates):\n",
    "                \n",
    "                self.dates = dates\n",
    "                self.ids = [inv_node_map[date] for date in dates]\n",
    "    \n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.dates)\n",
    "    \n",
    "        def __getitem__(self, i):\n",
    "            date = self.dates[i]\n",
    "            date_feature = encode_date(date)\n",
    "            return self.ids[i], date_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a7a1df-aa03-4867-b493-41b5e47c30d3",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/royal-cookings/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def preprocess(graph):\n",
    "    \"\"\"\n",
    "    graph --> datasets\n",
    "    \"\"\"\n",
    "    global inv_node_map \n",
    "    global node_map\n",
    "    global edge_map\n",
    "    global inv_edge_map\n",
    "    global device\n",
    "    global img_list\n",
    "    global transform_temp\n",
    "    \n",
    "    \n",
    "    node_set = set()\n",
    "    http_set_s = set()\n",
    "    http_set_m = set()\n",
    "    http_set_l = set()\n",
    "    \n",
    "    edge_set = set()\n",
    "    string_set_s = set()\n",
    "    string_set_m = set()\n",
    "    string_set_l = set()\n",
    "    \n",
    "    image_set = set()\n",
    "    num_set = set()\n",
    "    poly_set = set()\n",
    "    datetime_set = set()\n",
    "    date_set = set()\n",
    "    year_set = set()\n",
    "    point_set = set()\n",
    "    \n",
    "    text_edge_set_s = set()\n",
    "    text_edge_set_m = set()\n",
    "    text_edge_set_l = set()\n",
    "    \n",
    "    image_edge_set = set()\n",
    "    num_edge_set = set()\n",
    "    spatial_edge_set = set()\n",
    "    temporal_edge_set = set()\n",
    "    encoder_map = defaultdict(list)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    dtype_set = set()\n",
    "    def is_date(date_string):\n",
    "        try:\n",
    "            rdflib.term.parse_datetime(date_string)\n",
    "            return 'datetime'\n",
    "        except ValueError:\n",
    "            try:\n",
    "                rdflib.term.parse_xsd_gyear(date_string)\n",
    "                return 'year'\n",
    "            except:\n",
    "                try:\n",
    "                    rdflib.term.parse_xsd_date(date_string)\n",
    "                    return 'date'\n",
    "                except:\n",
    "                    return False\n",
    "\n",
    "    def add_str_to_set(string,edge):\n",
    "        if len(string) < 20:\n",
    "            string_set_s.add(string)\n",
    "            text_edge_set_s.add(edge)\n",
    "        elif len(string) < 50:\n",
    "            string_set_m.add(string)\n",
    "            text_edge_set_m.add(edge)\n",
    "        else:\n",
    "            string_set_l.add(string)\n",
    "            text_edge_set_l.add(edge)\n",
    "    \n",
    "    \n",
    "    for s,p,o in graph:\n",
    "        i+=1\n",
    "        pi = p.identifier\n",
    "        if s.identifier in subj_set:\n",
    "            pass\n",
    "        for node in [s,o]:\n",
    "            ni = node.identifier\n",
    "            node_set.add(ni)\n",
    "            try:\n",
    "                dtype = node.datatype.identifier\n",
    "                dtype_set.add(dtype)\n",
    "            except AttributeError:\n",
    "                dtype = ''\n",
    "            if 'http' in ni[:200]: #200, because images sometimes have kgbench url attached\n",
    "                if len(ni) < 20:\n",
    "                    http_set_s.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "                elif len(ni) < 50:\n",
    "                    http_set_m.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "                else:\n",
    "                    http_set_l.add(''.join(ni.split(':')[1].split('/')[:-1]))\n",
    "    \n",
    "            else:\n",
    "                if is_date(ni) and ('Year' in dtype or 'date' in dtype):\n",
    "                    date_type = is_date(ni)\n",
    "                    if date_type == 'datetime':\n",
    "                        datetime_set.add(ni)\n",
    "                    elif date_type == 'year':\n",
    "                        year_set.add(ni)\n",
    "                    elif date_type == 'date':\n",
    "                        date_set.add(ni)\n",
    "                        \n",
    "                elif node.isalnum():\n",
    "                    if node.isnumeric():\n",
    "                        if node.isdigit():\n",
    "                            num_set.add(int(node.identifier))\n",
    "                            num_edge_set.add(pi)\n",
    "                        else:\n",
    "                            num_set.add(float(node.identifier))\n",
    "                elif ni.startswith('POINT') or ni.startswith('Point'): #didn't see any points, but according to the paper they can be included.\n",
    "                    point_set.add(ni)\n",
    "                    spatial_edge_set.add(pi)\n",
    "                elif node.isalpha():\n",
    "                   add_str_to_set(ni, pi)\n",
    "                   image_set.add(ni)\n",
    "                elif ni.startswith('_9j_'):\n",
    "                    image_set.add(ni) #might want to load this to hard drive if memory becomes an issue.\n",
    "                    image_edge_set.add(pi)\n",
    "                elif ni.startswith('POLYGON') or ni.startswith('Polygon'):\n",
    "                    poly_set.add(ni)\n",
    "                    spatial_edge_set.add(pi)                \n",
    "                    temporal_edge_set.add(pi)\n",
    "                elif ni.lower().startswith('multipolygon'):\n",
    "                    poly_set.add(ni)\n",
    "                elif ni.isascii():\n",
    "                    add_str_to_set(ni,pi)\n",
    "                elif ni.isprintable():\n",
    "                    add_str_to_set(ascii(ni),pi) #don't know if it's necessary, but it probably can't hurt\n",
    "                else:\n",
    "                    add_str_to_set(ascii(ni),pi)\n",
    "    \n",
    "                    \n",
    "    \n",
    "        edge_set.add(pi)\n",
    "\n",
    "    \n",
    "    transform_temp = trv.Compose([\n",
    "        trv.Resize((224, 224)),\n",
    "        trv.ToTensor(), trv.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], \n",
    "            std=[0.3, 0.3, 0.3])]) \n",
    "    \n",
    "\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    node_map = {i:node for i,node in enumerate(node_set)}\n",
    "    inv_node_map = {node:i for i,node in enumerate(node_set)}\n",
    "    \n",
    "    edge_map = {i:edge for i,edge in enumerate(edge_set)}\n",
    "    inv_edge_map = {edge:i for i,edge in enumerate(edge_set)}\n",
    "\n",
    "    \n",
    "    global_mean_x = 0\n",
    "    global_mean_y = 0\n",
    "    \n",
    "    max_x = 0\n",
    "    max_y = 0\n",
    "    \n",
    "    \n",
    "    points_tupled = []\n",
    "    point_ids = []\n",
    "    for i, point in enumerate(point_set):\n",
    "        point_tupled,max_x,max_y = process_point(point,max_x,max_y)\n",
    "        points_tupled.append(point_tupled)\n",
    "        point_ids.append(inv_node_map[point])\n",
    "    \n",
    "    \n",
    "    polys_tupled = []\n",
    "    poly_ids = []\n",
    "    x_max, y_max = 0,0\n",
    "    done = False\n",
    "    multipolys_tupled = []\n",
    "    for i, poly in enumerate(poly_set):\n",
    "        if 'multi' in poly.lower():\n",
    "            multipolys_tupled.append(process_multipoly(poly, x_max, y_max))\n",
    "            \n",
    "        poly_tupled, x_mean, y_mean, x_max, y_max = get_num_data(poly, \n",
    "                                                                 x_max, y_max)\n",
    "        global_mean_x += 1\n",
    "        global_mean_y += 1\n",
    "        polys_tupled.append(poly_tupled)\n",
    "        poly_ids.append(inv_node_map[poly])\n",
    "    i += 1\n",
    "    global_mean_x, global_mean_y = global_mean_x/i, global_mean_y/i\n",
    "    \n",
    "    \n",
    "    character_map = {char:i for i,char in enumerate(printable)}\n",
    "    character_map['\\x7f'] = 101\n",
    "    \n",
    "    text_dataset_s = TextDataset(list(string_set_s), character_map)\n",
    "    text_dataset_m = TextDataset(list(string_set_m), character_map)\n",
    "    text_dataset_l = TextDataset(list(string_set_l), character_map)\n",
    "\n",
    "    img_list = list(image_set)\n",
    "    image_dataset = ImageDataset(img_list, transform_temp)\n",
    "    numerical_dataset = NumericalDataset(list(num_set), log_scale=True)\n",
    "    \n",
    "    spatial_dataset = SpatialDataset(polys_tupled + points_tupled, global_mean_x, global_mean_y, x_max, y_max, ids=point_ids + poly_ids)\n",
    "    \n",
    "    datetime_dataset = DateTimeDataset(list(datetime_set))\n",
    "    year_dataset = YearDataset(list(year_set))\n",
    "    return text_dataset_s, text_dataset_m, text_dataset_l, image_dataset , numerical_dataset, spatial_dataset, datetime_dataset, year_dataset, node_map, edge_map, inv_node_map, inv_edge_map\n",
    "        \n",
    "text_dataset_s, text_dataset_m, text_dataset_l, image_dataset , numerical_dataset, spatial_dataset, datetime_dataset, year_dataset,node_map, edge_map, inv_node_map, inv_edge_map = preprocess(graph)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd4067-852c-4807-bc0b-417be80a827f",
   "metadata": {},
   "source": [
    "# Convert raw values to consistent feature vectors for the encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34660fba-188d-4dfd-902e-b9c2173915e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_spatial(batch):\n",
    "    nodes_ids, batch = zip(*batch)\n",
    "    padded = nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "    padded = padded.permute(0, 2, 1)\n",
    "    return nodes_ids, padded\n",
    "\n",
    "def collate_text(batch, min_size=20):\n",
    "    if batch[0].size(0) < min_size:\n",
    "        pad_num = min_size - batch[0].size(0)\n",
    "        pads = torch.zeros(pad_num, dtype=torch.long, device=device)\n",
    "        batch[0] = torch.cat([batch[0], pads], dim=0)\n",
    "    return nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "spatial_dataloader = DataLoader(spatial_dataset, batch_size=batch_size, collate_fn=collate_spatial)\n",
    "text_dataloader_s = DataLoader(text_dataset_s, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_m = DataLoader(text_dataset_m, batch_size=batch_size, collate_fn=collate_text)\n",
    "text_dataloader_l = DataLoader(text_dataset_l, batch_size=2, collate_fn=collate_text) #it crashes every time it starts :'(\n",
    "image_dataloader = DataLoader(image_dataset, batch_size=2)\n",
    "datetime_dataloader = DataLoader(datetime_dataset, batch_size=batch_size)\n",
    "year_dataloader = DataLoader(year_dataset, batch_size=batch_size)\n",
    "numerical_dataloader = DataLoader(numerical_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b8883-95ca-42a2-b1d3-f9c2383108be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1e1e85-5dd0-491e-b949-60ca053ce7f1",
   "metadata": {},
   "source": [
    "# Multimodal Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7198e43b-a426-4ee6-8d74-6ea1b2652130",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch.functional as f\n",
    "import torch.nn as nn\n",
    "\n",
    "# ]1] temporal conv\n",
    "# Layer Filters Kernel Padding Pool\n",
    "# 1 64 7 3 max(2/2)\n",
    "# 2 64 7 3 max(2/2)\n",
    "# 3 64 7 3 -\n",
    "# 4 64 7 2 max(·)\n",
    "# Layer Dimensions\n",
    "# 5 512\n",
    "# 6 128\n",
    "# 7 128\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=102, dropout=0.4, size_type='medium'): #filters = 64\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.size_type = size_type.lower()\n",
    "        self.mlp_hidden_dim = 1024 if self.size_type == 'large' else 512 if self.size_type == 'medium' else 256\n",
    "        self.tcnn_hidden_dim = 128 if self.size_type == 'large' else 64 if self.size_type == 'medium' else 32\n",
    "        self.dilation_vals = (1,1,1) if self.size_type == 'small' else (2,4,8)\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) #used embedding dict instead of ohe to save space\n",
    "                                                            #sparse should probably work too\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            embed_dim, self.tcnn_hidden_dim , kernel_size=7, padding=3, dilation=1)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.pool1 = nn.MaxPool1d(2,2)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.pool2 = nn.MaxPool1d(2,2)\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=3)\n",
    "\n",
    "        self.norm3 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        self.conv4 = nn.Conv1d(\n",
    "            self.tcnn_hidden_dim , self.tcnn_hidden_dim , kernel_size=7, padding=2)#, dilation=self.dilation_vals[2])\n",
    "        \n",
    "        self.norm4 = nn.BatchNorm1d(self.tcnn_hidden_dim )\n",
    "        self.drop4 = nn.Dropout(dropout)\n",
    "        self.pool4 = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.lin1 = nn.Linear(self.tcnn_hidden_dim, self.mlp_hidden_dim)\n",
    "        self.lin2 = nn.Linear(self.mlp_hidden_dim, embed_dim)\n",
    "        self.lin3 = nn.Linear(embed_dim , embed_dim)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        residual = embedded\n",
    "        #conv\n",
    "        \n",
    "        x = self.conv1(embedded)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop4(x)\n",
    "        # if residual.size(1) != x.size(1):\n",
    "        #   residual = nn.utils.rnn.pad_sequence([residual, x], batch_first=True)\n",
    "        x = self.pool4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #ffnn\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        # x += residual\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "# [2] MobileNet \n",
    "# Type / Stride Filter Shape Input Size\n",
    "# Conv / s2 3 × 3 × 3 × 32 224 × 224 × 3\n",
    "# Conv dw / s1 3 × 3 × 32 dw 112 × 112 × 32\n",
    "# Conv / s1 1 × 1 × 32 × 64 112 × 112 × 32\n",
    "# Conv dw / s2 3 × 3 × 64 dw 112 × 112 × 64\n",
    "# Conv / s1 1 × 1 × 64 × 128 56 × 56 × 64\n",
    "# Conv dw / s1 3 × 3 × 128 dw 56 × 56 × 128\n",
    "# Conv / s1 1 × 1 × 128 × 128 56 × 56 × 128\n",
    "# Conv dw / s2 3 × 3 × 128 dw 56 × 56 × 128\n",
    "# Conv / s1 1 × 1 × 128 × 256 28 × 28 × 128\n",
    "# Conv dw / s1 3 × 3 × 256 dw 28 × 28 × 256\n",
    "# Conv / s1 1 × 1 × 256 × 256 28 × 28 × 256\n",
    "# Conv dw / s2 3 × 3 × 256 dw 28 × 28 × 256\n",
    "# Conv / s1 1 × 1 × 256 × 512 14 × 14 × 256\n",
    "# 5×\n",
    "# Conv dw / s1 3 × 3 × 512 dw 14 × 14 × 512\n",
    "# Conv / s1 1 × 1 × 512 × 512 14 × 14 × 512\n",
    "# Conv dw / s2 3 × 3 × 512 dw 14 × 14 × 512\n",
    "# Conv / s1 1 × 1 × 512 × 1024 7 × 7 × 512\n",
    "# Conv dw / s2 3 × 3 × 1024 dw 7 × 7 × 1024\n",
    "# Conv / s1 1 × 1 × 1024 × 1024 7 × 7 × 1024\n",
    "# Avg Pool / s1 Pool 7 × 7 7 × 7 × 1024\n",
    "# FC / s1 1024 × 1000 1 × 1 × 1024\n",
    "# Softmax / s1 Classifier 1 × 1 × 1000\n",
    "# Table 2. Resource Per Layer Type\n",
    "# Type Mult-Adds Parameters\n",
    "# Conv 1 × 1 94.86% 74.59%\n",
    "# Conv DW 3 × 3 3.06% 1.06%\n",
    "# Conv 3 × 3 1.19% 0.02%\n",
    "# Fully Connected 0.18% 24.33%\n",
    "\n",
    "#idea: pass identity vector with batches to encoders (don't process, just pass back), use that to map embeddings to nodes later\n",
    "class ImageEncoder(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 256, stride=2, alpha=0.5)\n",
    "        self.image6 = MobileBlock(256, 512, stride=1, alpha=0.5)\n",
    "        self.image7 = MobileBlock(512, 512, stride=2, alpha=0.5)\n",
    "        self.image8 = MobileBlock(512, 1024, stride=1, alpha=0.5)\n",
    "        self.middle_conv = nn.Sequential(*[MobileBlock(\n",
    "                1024, 1024, stride=1, alpha=0.5) for i in range(5)])\n",
    "        \n",
    "        self.image9 = MobileBlock(1024, 1024, stride=1, alpha=0.5)\n",
    "        \n",
    "        self.image10 = MobileBlock(1024, 2048, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(1024,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        # middle_conv = [MobileBlock() for i in range(5)]\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.image(batch)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.image6(x)\n",
    "        x = self.image7(x)\n",
    "        x = self.image8(x)\n",
    "        x = self.middle_conv(x)\n",
    "        x = self.image9(x)\n",
    "        x = self.image10(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        # x = self.norm(x) #batch size is too small on my system due to computational constraints\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class MiniImgEnc(nn.Sequential):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(MiniImgEnc, self).__init__()\n",
    "        self.image = MobileBlock(3, 32, normal_conv=True, stride=1, alpha=0.5)\n",
    "        self.image2 = MobileBlock(32, 64, stride=1, alpha=0.5)\n",
    "        self.image3 = MobileBlock(64, 128, stride=2, alpha=0.5)\n",
    "        self.image4 = MobileBlock(128, 256, stride=1, alpha=0.5)\n",
    "        self.image5 = MobileBlock(256, 512, stride=2, alpha=0.5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin1 = nn.Linear(256,embed_dim)\n",
    "        self.norm = nn.BatchNorm1d(embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image(x)\n",
    "        x = self.image2(x)\n",
    "        x = self.image3(x)\n",
    "        x = self.image4(x)\n",
    "        x = self.image5(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        # x = self.lin2(x)\n",
    "        # x = self.norm(x)\n",
    "        return self.act(x)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class MobileBlock(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_size, stride=1, normal_conv=False, alpha=0.5, dilation=1):\n",
    "        super(MobileBlock, self).__init__()\n",
    "        old_in_channels = in_channels\n",
    "        in_channels = int(alpha * in_channels)\n",
    "        embedding_size = int(alpha * embedding_size)\n",
    "        # depthwise conv\n",
    "        self.normal_conv = normal_conv\n",
    "        if self.normal_conv:\n",
    "            # in_channels = in_channels//2\n",
    "            pass\n",
    "        # 3x3x3x32\n",
    "        self.conv = nn.Conv2d(old_in_channels, in_channels, 3, padding=1, stride=2, dilation=1)\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, 3, stride=stride,\n",
    "                                   padding=1, groups=in_channels)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        # pointwise conv\n",
    "        self.pointwise = nn.Conv2d(in_channels, embedding_size, 1, stride=1, padding=0)\n",
    "        self.norm2 = nn.BatchNorm2d(embedding_size)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.normal_conv:\n",
    "            x = self.conv(x)\n",
    "        x = self.depthwise(x)\n",
    "        # x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.pointwise(x)\n",
    "        # x = self.norm2(x) #normalization disabled due to computational constraints\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [3] temporal conv\n",
    "# layer filters kernel padding pool\n",
    "# 1 16 5 2 max(3/3)\n",
    "# 2 32 5 2 -\n",
    "# 3 64 5 2 avg(·)\n",
    "# layer dimensions\n",
    "# 4 512\n",
    "# 5 128\n",
    "# 6 128\n",
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128, dropout=0.2):\n",
    "        super(SpatialEncoder, self).__init__()\n",
    "        \n",
    "        # temp cnn\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=3, ceil_mode=True)\n",
    "        self.norm1 = nn.BatchNorm1d(16)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm1d(32)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "        \n",
    "        # dense\n",
    "        self.lin1 = nn.Linear(64, 512)\n",
    "        self.lin2 = nn.Linear(512, 128)\n",
    "        self.lin3 = nn.Linear(128, embed_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, spatial):\n",
    "        # x = self.pad(spatial, batch_first=True)\n",
    "        x = self.conv1(spatial)\n",
    "        x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.drop1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #flatten\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# [4] mlp h == in dim 1st col, out 2nd col\n",
    "# XSD:gYear 6 2\n",
    "# XSD:date 10 4\n",
    "# XSD:dateTime 14 6\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super(TemporalEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = input_dim\n",
    "        self.out_dim = 2 if input_dim <= 6 else 4 if input_dim <= 9 else 6 #I messed something up but its very specific...\n",
    "                                                                        #norm_cent returns a digit so idk if that should be 2?\n",
    "                                                                        #there's no sin/cos, so it seems weird to encode it digitwise\n",
    "        num_layers = 2\n",
    "        self.mlp = nn.Sequential(*[nn.Linear(input_dim, input_dim), \n",
    "                                   nn.ReLU(),nn.BatchNorm1d(input_dim)]*num_layers)\n",
    "        self.out = nn.Linear(input_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "\n",
    "#[5] one to one encoding for numerical\n",
    "class NumericalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NumericalEncoder, self).__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8c0f3-7864-4598-a4e8-ecc714a2e6ae",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46ff724b-ac91-443b-8c30-fa9f606d0359",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "text_encoder_s, text_encoder_m, text_encoder_l = TextEncoder(128, size_type='s'), TextEncoder(128, size_type='m'), TextEncoder(128, size_type='m')\n",
    "datetime_encoder = TemporalEncoder(9)\n",
    "spatial_encoder = SpatialEncoder(5)\n",
    "year_encoder = TemporalEncoder(5)\n",
    "image_encoder = ImageEncoder(128)\n",
    "numerical_encoder = NumericalEncoder()\n",
    "\n",
    "\n",
    "class MultiModalEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_matrix,\n",
    "                 encoders_config, dropout=False):\n",
    "        super(MultiModalEncoder, self).__init__()\n",
    "\n",
    "        # self.dataloaders = [config['dataloader'] for config in encoders_config]\n",
    "        self.encoders_config = encoders_config\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.column_map = {'text':(0,128),\n",
    "                           'image':(128,256),\n",
    "                           'numerical':(256,257),\n",
    "                           'spatial':(257, 385),\n",
    "                           'datetime':(385,389),\n",
    "                           'year':(389,391)}\n",
    "\n",
    "        self.encoders = nn.ModuleList([config['encoder'] for config in encoders_config])\n",
    "        self.encoder_dataloaders = [config['dataloader'] for config in encoders_config]\n",
    "        \n",
    "        self.x_indices = []#torch.zeros(8, device=device)\n",
    "        self.y_indices = []\n",
    "        self.values = []#torch.zeros((8,128), device=device)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.num_nodes = feature_matrix.size()[0]\n",
    "\n",
    "    def forward(self):\n",
    "        embed_dim = self.column_map['year'][-1]\n",
    "        out_feature_matrix = torch.zeros((self.num_nodes, embed_dim), dtype=torch.float32, device=self.feature_matrix.device)\n",
    "        for encoder, config in zip(self.encoders,self.encoders_config):\n",
    "            if random.random()>self.dropout:\n",
    "                continue\n",
    "            self.x_indices = []\n",
    "            self.y_indices = []\n",
    "            self.values = []\n",
    "            name = config['name']\n",
    "            #encoder = #config['encoder'].to(device)\n",
    "            dataloader = config['dataloader']\n",
    "            # print(f\"encoder: {name}\")\n",
    "            i = 0\n",
    "            # if 'image' not in name:\n",
    "            for j, batch in enumerate(dataloader):\n",
    "                # if random.random()>self.dropout:\n",
    "                #     x,y = self.column_map[name]\n",
    "                #     output_dim = y-x\n",
    "                #     embeddings = torch.zeros(y-x, dtype=torch.float32,device=device).unsqueeze(0)\n",
    "                #     continue\n",
    "                    \n",
    "                if 'text' in name:\n",
    "                    cols = self.column_map['text']\n",
    "                    node_ids = batch[:, 0]\n",
    "                    features = batch[:, 1:]\n",
    "                else:\n",
    "                    cols = self.column_map[name]\n",
    "                    node_ids, features = batch\n",
    "                    node_ids = torch.tensor(node_ids,device=device)\n",
    "                    # node_ids = node_ids.clone().detach().to(device)\n",
    "\n",
    "                # I confused myself. passing the node_ids through the forward pass was completely unnecessary.\n",
    "                # change to sparse and use automatic column assignment base on thingy\n",
    "                try:\n",
    "                    embeddings = encoder(features)\n",
    "                    # print(f\"{name}: {embeddings}\")\n",
    "                except ValueError: #if batch size is 0 or 1 when it's supposed to be more.\n",
    "                    # print('err')\n",
    "                    x,y = self.column_map[name]\n",
    "                    output_dim = y-x\n",
    "                    embeddings = torch.zeros(y-x, dtype=torch.float32,device=device).unsqueeze(0)\n",
    "\n",
    "                # batch_size = node_ids.size(0)\n",
    "                # y_cols = torch.arange(cols[0], cols[1], device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "                # embedding_size = embeddings.size(1)\n",
    "                # x_rows = node_ids.unsqueeze(1).repeat(1, embedding_size)\n",
    "                # self.x_indices.append(x_rows)\n",
    "                # self.y_indices.append(y_cols)\n",
    "                # self.values.append(embeddings.view(-1))\n",
    "                i+=1\n",
    "\n",
    "        # else:\n",
    "        #     for img in dataloader:\n",
    "        #         pass\n",
    "                try:\n",
    "                    # x = torch.cat(self.x_indices).reshape(-1)\n",
    "                    # y = torch.cat(self.y_indices).reshape(-1)\n",
    "                    # indices = torch.stack([x, y], dim=0)\n",
    "                    # values = torch.cat(self.values)\n",
    "                    # self.feature_matrix = torch.zeros((self.num_nodes, 396), dtype=torch.float32, device=device)\n",
    "                    # self.feature_matrix.index_put_(indices, values)\n",
    "                    out_feature_matrix[node_ids, cols[0]:cols[1]] = embeddings\n",
    "                    torch.cuda.empty_cache()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        return out_feature_matrix\n",
    "\n",
    "\n",
    "encoders_config = [{'name': 'text_s', 'encoder': text_encoder_s, 'dataloader': text_dataloader_s},\n",
    "                   {'name': 'text_m', 'encoder': text_encoder_m, 'dataloader': text_dataloader_m},\n",
    "                   {'name': 'text_l', 'encoder': text_encoder_l, 'dataloader': text_dataloader_l},\n",
    "                   {'name': 'image', 'encoder': image_encoder, 'dataloader': image_dataloader},\n",
    "                   {'name': 'numerical', 'encoder': numerical_encoder, 'dataloader': numerical_dataloader},\n",
    "                   {'name': 'spatial', 'encoder': spatial_encoder, 'dataloader': spatial_dataloader},\n",
    "                   {'name': 'datetime', 'encoder': datetime_encoder, 'dataloader': datetime_dataloader},\n",
    "                   {'name': 'year', 'encoder': year_encoder, 'dataloader': year_dataloader},]\n",
    "\n",
    "\n",
    "num_nodes = len(node_map)\n",
    "\n",
    "embed_size = 128 + 128 + 1 + 128 + 4 + 2\n",
    "\n",
    "feature_matrix = torch.zeros((num_nodes, embed_size), dtype=torch.float32, device='cpu')#.to_sparse()\n",
    "\n",
    "\n",
    "\n",
    "# multi_modal_encoder = MultiModalEncoder(\n",
    "#     feature_matrix=feature_matrix,\n",
    "#     encoders_config=encoders_config, dropout=0.2).to(device)\n",
    "\n",
    "\n",
    "# feature_matrix = multi_modal_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db8252c-67bd-4177-9651-31692750eb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3936"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fd9b6-b914-44b8-bdc4-c565e0906194",
   "metadata": {},
   "source": [
    "# Connect to R-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef175e6b-b693-4709-ba84-f3bf9761ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(node_map.keys())\n",
    "\n",
    "indices = torch.zeros((1),device=device)\n",
    "indices = torch.arange(0, length, device=device).repeat(2, 1)\n",
    "\n",
    "values = torch.ones(length, device=device)\n",
    "identity = torch.sparse_coo_tensor(indices, values, (length, length), device=device).to_dense()\n",
    "feature_matrix_cat = torch.cat((identity,feature_matrix),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87512148-5532-423a-b7b4-999c6ede7bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment for sparse\n",
    "\n",
    "from typing import defaultdict\n",
    "# adjacency = defaultdict(torch.sparse_coo_tensor)\n",
    "graph_size = max(node_map.keys()) +1\n",
    "adjacency = defaultdict(lambda: torch.zeros((graph_size, graph_size), device=device))\n",
    "\n",
    "\n",
    "# inv_edge_map\n",
    "indices_x = defaultdict(list)\n",
    "indices_y = defaultdict(list)\n",
    "\n",
    "values = defaultdict(list)\n",
    "i=0\n",
    "for s, p, o in graph:\n",
    "    edge_id = inv_edge_map[p.identifier]\n",
    "    s_id = inv_node_map[s.identifier]\n",
    "    o_id = inv_node_map[o.identifier]\n",
    "    \n",
    "    # indices_x[edge_id].append(s_id)\n",
    "    # indices_y[edge_id].append(o_id)\n",
    "    # values[edge_id].append(1)\n",
    "    adjacency[edge_id][s_id, o_id] = 1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# for edge_id in indices_x.keys():\n",
    "#     ind_x, ind_y = torch.tensor(indices_x[edge_id]), torch.tensor(indices_y[edge_id])\n",
    "#     ind = torch.stack((ind_x, ind_y))\n",
    "#     val = values[edge_id]\n",
    "#     adjacency[edge_id] = torch.sparse_coo_tensor(indices=ind,values=val,size=(graph_size,graph_size),device=device)\n",
    "    \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d193de63-3b42-44db-9d23-de87405de429",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dense\n",
    "def normalize_adjacency(A, self=True): #gotta still add self-loops. See if it works without first.\n",
    "    if self:\n",
    "        A = A + torch.eye(A.size(0), device=device)\n",
    "    degree = A.sum(dim=1)\n",
    "    d_inv_sqrt = torch.pow(degree, -0.5)\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0\n",
    "    return A * d_inv_sqrt.unsqueeze(1) * d_inv_sqrt.unsqueeze(0)\n",
    "\n",
    "for i, key in enumerate(adjacency.keys()):\n",
    "    A = adjacency[key]\n",
    "    adjacency[key] = normalize_adjacency(A)\n",
    "    del A\n",
    "    # print(f'Great success! {i}')\n",
    "    k = key\n",
    "adjacency[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888377b-352a-4f80-9e0e-1fc5d6d88f3f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05eeb229-0bab-46de-980e-34b2f4a2ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save non-error version\n",
    "def block_diag (weights):\n",
    "    block_diag_matrix = torch.block_diag(*weights)\n",
    "    return block_diag_matrix\n",
    "    \n",
    "\n",
    "class R_GCNLayer(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, num_relations, block_split):\n",
    "        super(R_GCNLayer, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.num_relations = num_relations\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        max_block_size = 1\n",
    "        for i in range(1,11):\n",
    "            if x_dim % i == 0 and y_dim % i == 0:\n",
    "                max_block_size +=1 \n",
    "        x_block_size = x_dim // block_split\n",
    "        y_block_size = y_dim // block_split\n",
    "        self.W = nn.ParameterList()\n",
    "        for _ in range(num_relations):\n",
    "            wr = nn.ParameterList()\n",
    "            for i in range(block_split):\n",
    "                w = nn.Parameter(torch.randn(x_block_size, y_block_size)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                wr.append(w)\n",
    "            x_remainder = x_dim % block_split\n",
    "            y_remainder = y_dim % block_split\n",
    "            if  x_remainder > 0 and y_remainder > 0:\n",
    "                w = nn.Parameter(torch.randn(x_remainder, y_remainder)) \n",
    "                nn.init.kaiming_uniform_(w, a=gain)\n",
    "                wr.append(w)\n",
    "            self.W.append(wr)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(y_dim))\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, A, X):\n",
    "        device = next(self.parameters()).device\n",
    "        aggregated = torch.zeros((X.size(0), self.y_dim), device=device)\n",
    "        for r in range(self.num_relations):\n",
    "            X = X.to_dense()\n",
    "            if X.is_sparse:\n",
    "                diag = block_diag(self.W[r]).to_dense()\n",
    "                correct_indices = (indices >= 0).all(dim=0)\n",
    "                X = X.coalesce()\n",
    "                if diag.is_sparse: #this really defeats the point of using sparse matrices, but torch.sparse only supports sparse, dense mm\n",
    "                    diag = diag.to_dense()\n",
    "                    \n",
    "                weighted = torch.matmul(X, diag)\n",
    "            else:\n",
    "                weighted = torch.matmul(X, block_diag(self.W[r])) \n",
    "            if A[r].is_sparse:\n",
    "                transformed = torch.matmul(A[r], weighted)\n",
    "            else:\n",
    "                transformed = torch.matmul(A[r], weighted)\n",
    "            aggregated += transformed\n",
    "        aggregated += self.bias\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "\n",
    "class R_GCN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, y_dim, max_k_hop, num_relations):\n",
    "        super(R_GCN, self).__init__()\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.num_relations = num_relations\n",
    "        self.x_dim = x_dim\n",
    "        self.block_split = 1\n",
    "        self.bases = 1\n",
    "        self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations, self.bases) for _ in range(max_k_hop)])\n",
    "        # self.gcns = nn.ModuleList([R_GCNLayer(x_dim, h_dim, num_relations) for _ in range(max_k_hop)])\n",
    "        self.final_r_gcn = R_GCNLayer(h_dim, y_dim,num_relations, self.bases )\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.norm = nn.BatchNorm1d(y_dim)\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        # if X.size(0) % 2 !=0:\n",
    "        #     X = X[:-1,:]\n",
    "        # outer layers\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(self.max_k_hop):\n",
    "            H = self.gcns[i](A, X)\n",
    "            H = self.act(H)\n",
    "            H = self.drop(H)\n",
    "        Y = self.final_r_gcn(A, H)\n",
    "        out = self.act(Y)\n",
    "        out = self.norm(out)\n",
    "        out = self.soft(out)\n",
    "        return out#nn.LogSoftmax(Y)\n",
    "\n",
    "#feature_matrix.size(0)+1\n",
    "num_classes=5\n",
    "# r_gcn = R_GCN(feature_matrix_cat.size(1), 32, num_classes, max_k_hop=2, num_relations=i+1).to(device)\n",
    "# out = r_gcn([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], feature_matrix_cat)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3316a6c-75ac-4126-9291-f84f19333d76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MR_GCN(nn.Module):\n",
    "    def __init__(self, A_list, eye, num_nodes, rgcn_inp_dim, embed_size=396, num_classes=5, dropout=False):\n",
    "        super(MR_GCN, self).__init__()\n",
    "        \n",
    "        self.A_list = A_list\n",
    "        self.embed_size = embed_size\n",
    "        # self.embedding_matrix = torch.zeros((num_nodes, embed_size), dtype=torch.float32, device=device)#.to_sparse()\n",
    "        self.embedding_matrix = nn.Parameter(torch.zeros((num_nodes, embed_size), dtype=torch.float32, device=device))\n",
    "        self.MME= MultiModalEncoder(feature_matrix=self.embedding_matrix, encoders_config=encoders_config,dropout=dropout).to(device)\n",
    "        self.graph_dim = A_list[0].size(0)\n",
    "        self.R_GCN = R_GCN(rgcn_inp_dim, 128, num_classes, max_k_hop=2, num_relations=len(A_list)).to(device)\n",
    "        self.eye = eye\n",
    "        self.feature_matrix = []\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        X = self.MME()\n",
    "        X = torch.cat((self.eye,X), dim=1)\n",
    "        # print((self.eye.size(),X.size()),)\n",
    "        # print(self.A_list[0].size(),X.size())\n",
    "        X = self.R_GCN(self.A_list, X)\n",
    "        return X\n",
    "mr_gcn = MR_GCN([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], identity, num_nodes, feature_matrix_cat.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a992be4-5f05-41ef-8ae7-fdbd1537c6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1222, 0.0959, 0.1532, 0.4728, 0.1559],\n",
       "        [0.0554, 0.2027, 0.3138, 0.3393, 0.0889],\n",
       "        [0.2204, 0.0655, 0.5415, 0.0661, 0.1065],\n",
       "        ...,\n",
       "        [0.1947, 0.1527, 0.2440, 0.1603, 0.2482],\n",
       "        [0.5401, 0.0879, 0.1405, 0.0887, 0.1429],\n",
       "        [0.0696, 0.5957, 0.0718, 0.1898, 0.0731]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_gcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c272ea1-1625-4f5c-9c78-bd430ca27928",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "labels = list(train_label_set|val_label_set|test_label_set)\n",
    "num_classes = len(labels)\n",
    "ohe = torch.eye(num_classes)\n",
    "label_to_ohe = {label:ohe[i] for i, label in enumerate(labels)}\n",
    "label_to_num = {label:i for i, label in enumerate(labels)}\n",
    "\n",
    "labels = torch.full((num_nodes,), -1, dtype=torch.long, device=device)\n",
    "\n",
    "train_labels = labels#.clone()\n",
    "for node_id, label in train_node_label_map.items():\n",
    "    train_labels[node_id] = label_to_num[label]\n",
    "\n",
    "train_mask = labels>=0\n",
    "\n",
    "test_labels = labels#.clone()\n",
    "for node_id, label in test_node_label_map.items():\n",
    "    test_labels[node_id] = label_to_num[label]\n",
    "\n",
    "test_mask = labels>=0\n",
    "\n",
    "val_labels = labels#.clone()\n",
    "for node_id, label in val_node_label_map.items():\n",
    "    val_labels[node_id] = label_to_num[label]\n",
    "\n",
    "val_mask = labels>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7033ae56-b0e6-4912-8dca-a153791ccce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de6f23bd-f92c-43fa-b1dd-e44d3288f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_node_label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fcc809-6731-481a-8daf-72cb6ab69a3f",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "### The Multimodal Encoder is currently not connected to the computation graph. \n",
    "### I connecting it by changing some operations around, but it seems to be a fundamental issue.\n",
    "### I likely misunderstood the encoding step and implemented it in a way that's non-differentiable.\n",
    "### The computation graph shows that only the R-GCN part is connected, so it is not learning effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e36100-8d42-4ce0-85c9-52586c5c64cd",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "avg loss: 1.6335207223892212, val_loss: 1.5334715843200684 val accuracy: 0.38738738738738737\n",
      "epoch: 100\n",
      "avg loss: 1.0069875717163086, val_loss: 1.4171501398086548 val accuracy: 0.5675675675675675\n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "plt.style.use(\"bmh\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "dropout = 0.2\n",
    "num_epochs = 200\n",
    "mr_gcn = MR_GCN([normalize_adjacency(adjacency[relation_id]) for relation_id in adjacency.keys()], \n",
    "                identity, num_nodes, \n",
    "                feature_matrix_cat.size(1), \n",
    "                num_classes=num_classes, \n",
    "                dropout=dropout)\n",
    "\n",
    "params = list(mr_gcn.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "alpha = 0.01\n",
    "optimizer = torch.optim.Adam(params,lr=alpha)\n",
    "\n",
    "\n",
    "val_accs = []\n",
    "test_accs = []\n",
    "best_val_acc = 0\n",
    "configs = [False, 0.2, 0.35, 0.5]\n",
    "number_of_runs = range(1,4) # 3 runs\n",
    "for config in configs:\n",
    "    dropout = config\n",
    "    for run in number_of_runs:\n",
    "        for epoch in range(num_epochs):\n",
    "            mr_gcn.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = mr_gcn()\n",
    "            loss = criterion(outputs[train_mask], labels[train_mask])\n",
    "            \n",
    "            loss.backward()\n",
    "            for p in mr_gcn.parameters():\n",
    "                pass\n",
    "            optimizer.step()\n",
    "            \n",
    "            mr_gcn.eval()\n",
    "        \n",
    "            make_dot(outputs, params=dict(list(mr_gcn.named_parameters()))).render(\"mr_gcn_torchviz\", format=\"png\")\n",
    "            with torch.no_grad():\n",
    "                val_outputs = mr_gcn()\n",
    "                val_loss = criterion(val_outputs[val_mask], labels[val_mask])\n",
    "                \n",
    "                _, predicted = torch.max(val_outputs[val_mask], dim=1)\n",
    "                correct = (predicted == labels[val_mask]).sum().item()\n",
    "                tot = val_mask.sum().item()\n",
    "                if tot:\n",
    "                    val_acc = correct / tot\n",
    "                    val_accs.append(val_acc)\n",
    "                if epoch % 100 == 0 and tot:\n",
    "                    print(f\"epoch: {epoch}\\navg loss: {loss}, val_loss: {val_loss} val accuracy: {val_acc}\")\n",
    "    \n",
    "            # if val_acc > best_val_acc:\n",
    "            #     mr_gcn.save()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_outputs = mr_gcn()\n",
    "            test_loss = criterion(test_outputs[test_mask], labels[test_mask])\n",
    "            \n",
    "            _, predicted = torch.max(test_outputs[test_mask], dim=1)\n",
    "            correct = (predicted == labels[test_mask]).sum().item()\n",
    "            tot = test_mask.sum().item()\n",
    "            if tot:\n",
    "                test_acc = correct / tot\n",
    "                test_accs.append((test_acc, config))\n",
    "    \n",
    "        print(f\"final epoch: {epoch}\\navg loss: {loss}, test_loss: {test_loss} test accuracy: {test_acc}\")\n",
    "        dropout_tag = 'no ' if not dropout else dropout\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(range(len(val_accs)), val_accs, label=f\"Validation Accuracy with {dropout_tag} Dropout\")\n",
    "        plt.xlabel(\"Epochs\", color='black')\n",
    "        plt.ylabel(\"Validation Accuracy\", color='black')\n",
    "        plt.title(f\"Validation Accuracy by Epoch with {dropout_tag} dropout\", color='black')\n",
    "        plt.legend(labelcolor='black')\n",
    "        plt.grid()\n",
    "        plt.savefig(f\"plots/run{run}_validation_accuracy_{dropout_tag}_dropout.png\")\n",
    "        with open('results/results.txt','a') as f:\n",
    "            f.write(f\"Results on the test set with a Modality Encoder Dropout of {dropout_tag} after training for {num_epochs} epochs using Adam with a learning rate of {alpha}: {test_acc}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9ec31-41e0-4451-a130-7a31a5786854",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_cat.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fe3c0-10fc-4c95-b1aa-ec647fffe185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44648230-bc52-40ef-959a-7cd42bdc2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff676a-92b5-42c7-913d-b1749b020d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "4942+391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23957d2-bb0b-42af-bd96-c096369ad221",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fede4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
