{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42273873-942b-48f5-a6d9-b19b525497ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.nn.functional import relu, prelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29cc76-c077-4441-b274-b5bf299d3e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a3804df-1178-401e-b62e-c4862f6a3d4c",
   "metadata": {},
   "source": [
    "# Graph search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29c1948-345b-4ebd-bc9c-2fecd4fffa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: collect all incoming edges for coalescence (search how they do it so it's consistent)\n",
    "#todo: after implementing message pass algorithm, move it to forward pass (if possible... and if it doesn't overcomplicate modularly changing out attention structures...)\n",
    "\n",
    "embedding_size = 5\n",
    "class GraphSearch:\n",
    "    def __init__(self, graph, protocol='DFS'):\n",
    "        self.graph = graph\n",
    "        self.protocol = protocol\n",
    "        self.visited = set() #<1 remove or adjust depending on walk style in message passing alg...\n",
    "    \n",
    "    def step(self, buffer, coalesce_func=lambda x:x):\n",
    "        if not buffer:\n",
    "            return None, buffer\n",
    "        \n",
    "        if self.protocol == 'BFS':\n",
    "            current_id = buffer.popleft()\n",
    "        else:\n",
    "            current_id = buffer.pop()\n",
    "        print(current_id)\n",
    "        if current_id in self.visited: #<1\n",
    "            return None, buffer\n",
    "\n",
    "        self.visited.add(current_id)\n",
    "        current = self.graph[current_id]\n",
    "        embedding, neighbours = current\n",
    "        embedding = coalesce(embedding)\n",
    "        print(f\"Visiting node: {current_id}\")\n",
    "        \n",
    "        for neighbour in neighbours:\n",
    "            if neighbour not in self.visited and neighbour not in buffer:\n",
    "                buffer.append(neighbour)\n",
    "                neighbour_embedding, _ = self.graph[neighbour]\n",
    "                print(f\"Added node {neighbour} to buffer.\")\n",
    "        \n",
    "        return current, buffer\n",
    "\n",
    "graph = {\n",
    "    1: [torch.rand(embedding_size), (2, 3)],\n",
    "    2: [torch.rand(embedding_size), (4, 5)],\n",
    "    3: [torch.rand(embedding_size), (6,)],\n",
    "    4: [torch.rand(embedding_size), (7,)],\n",
    "    5: [torch.rand(embedding_size), (8, 9)],\n",
    "    6: [torch.rand(embedding_size), (10,)],\n",
    "    7: [torch.rand(embedding_size), (1,)],\n",
    "    8: [torch.rand(embedding_size), (6, 10)],\n",
    "    9: [torch.rand(embedding_size), (7,)],\n",
    "    10: [torch.rand(embedding_size), (5,)]\n",
    "}\n",
    "\n",
    "edge_1 = torch.tensor([1.9,2.7,3.5],dtype=torch.float16)\n",
    "edge_2 = torch.tensor([9.1,10.3,12.5],dtype=torch.float16)\n",
    "edge_3 = torch.tensor([9.1,10.3,12.5],dtype=torch.float16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186637d3-124d-4793-ba9c-eee99f87bbf4",
   "metadata": {},
   "source": [
    "### cashew: try graphgym package :)\n",
    "# Proposed GNN architecture:\n",
    "## Transformation block >>>\n",
    "### * linear\n",
    "### * batch norm * <  my intuition is that this could replace mean() operation, just use vectorized sum() to reduce computational complexity.\n",
    "### * dropout * < On linear layer in the message function\n",
    "### * activation * < parametric relu = max(x,0) + alpha * min(x,0) ... alpha is trainable.\n",
    "### * attention * < I'm not sure if relational weights as in RGCN fall into this category. If they are complementary in any way.\n",
    "## <<< End transformation block\n",
    "### * aggregation by some problem dependant function i.e. mean(), min/max/avg..._pooling(), lstm(cat(edge_embeddings)) ...\n",
    "#### aggregation note: inverted degree matrix * adjacency matrix = avg(adjacency matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c958c6-c608-46dc-9be5-7f95a50582cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5be97b1f-8733-4f96-860c-8f66a81e8b90",
   "metadata": {},
   "source": [
    "# classical GCN:\n",
    "\n",
    "## important design choice here... use batch norm after each layer? Normalize explicitly?\n",
    "## messages = layer weight * normalized messages from prev layers\n",
    "## aggregation = sum(messages) --> relu\n",
    "\n",
    "### todo: add weighted average method. I.e. learnable row vector of size feature dim (must vectorize torch.mean explicitly for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0813b833-f070-43cf-9d95-0233534591f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aggregate(features): #this creates an aggregation directly as a matrix op\n",
    "#     agg = torch.stack(features)\n",
    "#     return agg, torch.mean(agg, dim=0)\n",
    "\n",
    "# def coalesce(features):\n",
    "\n",
    "# def normalize(messages):\n",
    "#     num = len(messages)\n",
    "    \n",
    "\n",
    "# def relu(features):\n",
    "#     return relu(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205882e8-df3d-407f-9a5d-69668314c372",
   "metadata": {},
   "source": [
    "# GraphSAGE\n",
    "## aggregate incoming messages (can be mean(messages, dim=0), can be a max pooling on mlp(message), can be LSTM(shuffle(messages) as mini-batch), can be sum without average (maybe this leads to batch norm later?)\n",
    "## concat current node message --> relu --> send\n",
    "\n",
    "## Uses L2 Norm as root squared error of embeddings at every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a23ab-9bd8-45a4-bfb1-5615bd73e494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7de03bc5-9d73-44ba-ad3c-dfde101d4eb7",
   "metadata": {},
   "source": [
    "# GAT\n",
    "\n",
    "# Architectural notes:\n",
    "\n",
    "## GCN, but vector weighting matrix is learned (which nodes to attend to and ./ unsure./ which parts of the embedding vectors/features to attend to \\.\\.\n",
    "\n",
    "### how can we handle permutation invariance?\n",
    "\n",
    "### seems that attention weights are graph conditional on search algorithm dependant\n",
    "\n",
    "### ^ wrong. It's a function of (and on) embeddings of different node embeddings at the previous step.\n",
    "\n",
    "### softmax ()\n",
    "\n",
    "### parameter matrix a can be a parameter matrix on a learned single layer mlp that processes the concatenated input vectors.\n",
    "\n",
    "### parameter matrix a is learned together with weight matrix w.\n",
    "\n",
    "### multi-head attention, multiple relu(a) matrices.\n",
    "\n",
    "### each a is initialized randomly, then aggregated to produce a single output\n",
    "\n",
    "### can be parallellized worker per message.\n",
    "\n",
    "### sparse matrix... fixed number of parameters.\n",
    "\n",
    "# Implications/discussion\n",
    "\n",
    "### asymmetric importance weighting\n",
    "\n",
    "### weights are still independant of graph size, even though more complex analysis of the graph can be performed.\n",
    "\n",
    "### graph attention mechanism scales linearly in graph size due to locality\n",
    "\n",
    "### cool visualization of attention mechanism (implicit clustering) cora citation paper\n",
    "\n",
    "### improved performance over GCN in some cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94542b15-3eab-42a7-bbe3-7410cf035385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d035a923-461f-4dc5-8453-d3faa6f66f60",
   "metadata": {},
   "source": [
    "# Test stuff and transform into main() below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02939e66-4811-4786-b771-757b216d38cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coalesce' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcoalesce\u001b[49m((edge_1,edge_2, edge_3))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coalesce' is not defined"
     ]
    }
   ],
   "source": [
    "coalesce((edge_1,edge_2, edge_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b54ad-f0b5-49db-994d-7dffbae80077",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = GraphSearch(graph, protocol='BFS')\n",
    "\n",
    "walk = True\n",
    "lambda_ = 0.1\n",
    "node_id = 1\n",
    "stack = deque([node_id])\n",
    "print(stack)\n",
    "while walk:\n",
    "    node_id, stack = dfs.step(stack)\n",
    "    if random.random() < lambda_:\n",
    "        walk = False\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e046b-f5f5-4bed-884a-d37f0cafeb28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca6b15-fc89-41ee-b82f-8646e4a46bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30936e-d213-408c-9413-af1b62066920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b355090-c13a-4404-823a-60f59a9b1a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
