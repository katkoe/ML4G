{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42273873-942b-48f5-a6d9-b19b525497ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.nn.functional import relu, prelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29c1948-345b-4ebd-bc9c-2fecd4fffa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: collect all incoming edges for coalescence (search how they do it so it's consistent)\n",
    "#todo: after implementing message pass algorithm, move it to forward pass (if possible... and if it doesn't overcomplicate modularly changing out attention structures...)\n",
    "#todo: try graph step passing new node and computational dependency subtrees--> trim subtrees and extend undiscovered. intuition: like a net with a ball (at new node), \n",
    "# cut off at k-hop depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9005177-8653-4c14-9a67-f12cc9a68b89",
   "metadata": {},
   "source": [
    "# GraphSearch for single hop message passing\n",
    "### new approach: graph class with nodes with child and parent links (for step and computational dependency subtree creation)\n",
    "###               step() can be a random (for now uniform random) selection from children. \n",
    "#### todo: implement k-hop dependency subtree creation and think about computational efficiency... Don't want to have exponential search overhead with hops..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115bb309-17b5-4ecf-844a-f0e9b89beaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea: could it be possible to completely decouple the procedurality of the network and just assume it converges to something useful? \n",
    "#... Just randomly (as a function of local in/out degree) sample a layer from the network each time and just keep walking... \n",
    "#... Maybe this still allows for multi-granularity analysis, just less organized. This idea relies on some notion of convergence I think. \n",
    "#todo: inverse relations are added to the predicate set...\n",
    "#todo: make training script\n",
    "#todo: can enhance expressiveness by making transformation & aggregation steps 3 layer mlp's each. (in the case of the r-gcn these are the embedding layers)\n",
    "# other option: can add mlp layers before and after the gnn, as pre-processing layers.\n",
    "# skip connections can be used to reduce oversmoothing I.e. k-hop 3 gets preprocessed and passed past k-hop 2 as well as into it (duplication) and just gets summed together with k-hop2 outputs\n",
    "# note that R-GCN uses normalized sum aggregation Also has edge dropout before batch norm\n",
    "# R-GCN uses full batch adam (rmsprop+momentum) for 400 epochs\n",
    "# R-GCN activation = relu, but relu-->relu-->softmax for entity classification makes sense ofcourse.\n",
    "# can kind of see embeddings as eigenvectors of the implications of the structure of the graph under random walk\n",
    "# check spectral node representation... its equal to the svd\n",
    "# graph laplacian  adj matrix (alternative repr for adj matrix)--> decomp\n",
    "# inverse relations and equality relations \n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import prelu, relu\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "EMBEDDING_SIZE = 5\n",
    "DIM_W = 5\n",
    "MAX_K_HOP = 3\n",
    "\n",
    "# def message():\n",
    "\n",
    "#end to end... some choice for type(input) in inputs module dict(input)(input)\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    #todo: handle size 0 batches\n",
    "    def __init__(self, x_dim ,y_dim, unique_labels, num_mlp_layers=1):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.edge_label_weights = nn.ModuleDict({label: nn.Linear(x_dim, y_dim) for label in unique_labels})\n",
    "        self.y_dim = y_dim\n",
    "        mlp_layers = []\n",
    "        for _ in range(3):\n",
    "            mlp_layers.append(nn.Linear(y_dim, y_dim))\n",
    "            mlp_layers.append(nn.PReLU())\n",
    "            mlp_layers.append(nn.Dropout(p=0.2))\n",
    "            mlp_layers.append(nn.LayerNorm(y_dim))\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, layer_node_batch, agg_method=torch.sum):\n",
    "           batch_size = len(layer_node_batch)\n",
    "           non_linear = []\n",
    "\n",
    "           for i, node in enumerate(layer_node_batch):\n",
    "               messages = node.collect_neighbours()\n",
    "               transformed_messages = defaultdict(list)\n",
    "\n",
    "               if messages:\n",
    "                   for message in messages:\n",
    "                       for parent_id, (embedding, edge_label, receiver_node_id) in message.items():\n",
    "                           transformed = self.edge_label_weights[edge_label](embedding)\n",
    "                           transformed_messages[i].append(transformed)\n",
    "\n",
    "                   aggregated = agg_method(torch.stack(transformed_messages[i])) + self.mlp(node.embedding)\n",
    "                   non_linear_i = self.prelu(aggregated)\n",
    "               else:\n",
    "                   transformed = torch.zeros(self.y_dim, dtype=torch.float32)\n",
    "                   aggregated = self.mlp(node.embedding)\n",
    "                   non_linear_i = self.prelu(aggregated)\n",
    "\n",
    "               non_linear.append(non_linear_i)\n",
    "\n",
    "           return torch.stack(non_linear)\n",
    "\n",
    "class R_GCN(nn.Module): #change to include GCN layers and assign them batches (split them up in forward and handle them sequentially including node update)\n",
    "    #it's a dependency bottleneck\n",
    "    def __init__(self, x_dim, y_dim, graph, max_k_hop):\n",
    "        super(R_GCN, self).__init__()\n",
    "        self.graph = graph\n",
    "        self.max_k_hop =max_k_hop\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCNLayer(x_dim, y_dim, graph.unique_labels) \n",
    "            for _ in range(max_k_hop + 1)\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def forward(self, node_batch, agg_method=torch.sum): #maybe handle k_hops outside? have it be a single layer...\n",
    "        mini_batch = defaultdict(list)\n",
    "        \n",
    "        for node, k in node_batch:\n",
    "            layer = k\n",
    "            mini_batch[layer].append(node)\n",
    "            \n",
    "        updated_embeddings = {}\n",
    "        for layer in reversed(list(mini_batch.keys())):\n",
    "            batch_size = len(mini_batch[layer])\n",
    "            # print(f'layer {layer} processing {batch_size} items...')\n",
    "            batch = mini_batch[layer]\n",
    "            embeddings = self.layers[layer](batch)\n",
    "            for i, node in enumerate(batch):\n",
    "                try:\n",
    "                    updated_embeddings[node.id] = embeddings[i]\n",
    "                except:\n",
    "                    pass\n",
    "            for node in mini_batch[layer]:\n",
    "                node.embedding = updated_embeddings[node.id]\n",
    "        return torch.stack([updated_embeddings[node.id] for node,k in node_batch], dim=0)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id_):\n",
    "        self.id = id_\n",
    "        self.embedding = torch.rand(EMBEDDING_SIZE, dtype=torch.float32, requires_grad=True)\n",
    "        self.features = {}\n",
    "        self.parents = {}\n",
    "        self.children = {}\n",
    "        self.out_edges = []\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def collect_neighbours(self):\n",
    "        local_tree = []\n",
    "        for parent, labels in self.parents.items():\n",
    "            for label in labels:\n",
    "                local_tree.append({parent.id: (parent.embedding, label, self.id)})\n",
    "        return local_tree\n",
    "                \n",
    "    \n",
    "    def print_out_degree(self):\n",
    "        out_edges = [(label,target.id) for label,target in self.out_edges]\n",
    "        print(f\"Out edges: {out_edges}\") if out_edges else print(f'Node {self.id} has no children')\n",
    "\n",
    "    def adjust_embeddings(self, new_embedding):\n",
    "        self.embedding = new_embedding\n",
    "\n",
    "    def add_parent(self, parent_node, label):\n",
    "        if parent_node not in self.parents:\n",
    "            self.parents[parent_node] = []\n",
    "        self.parents[parent_node].append(label)\n",
    "\n",
    "    def add_child(self, child_node, label):\n",
    "        if child_node not in self.children:\n",
    "            self.children[child_node] = []\n",
    "        self.children[child_node].append(label)\n",
    "        self.add_out_edge(label, child_node)\n",
    "\n",
    "    def add_out_edge(self, label, target):\n",
    "        self.out_edges.append((label, target))\n",
    "\n",
    "    def step(self):\n",
    "        if not self.children:\n",
    "            return None\n",
    "        edge_label, target_node = random.choice(self.out_edges)\n",
    "        return (target_node, edge_label)\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.label_weights = {}\n",
    "        self.unique_labels = set()\n",
    "        self.inverse_labels = True\n",
    "\n",
    "    def add_node(self, id_):\n",
    "        if id_ not in self.nodes:\n",
    "            self.nodes[id_] = Node(id_)\n",
    "\n",
    "    def add_edge(self, from_id, to_id, label):\n",
    "        if from_id not in self.nodes:\n",
    "            self.add_node(from_id)\n",
    "        if to_id not in self.nodes:\n",
    "            self.add_node(to_id)\n",
    "            \n",
    "        \n",
    "        from_node = self.nodes[from_id]\n",
    "        to_node = self.nodes[to_id]\n",
    "        from_node.add_child(to_node, label)\n",
    "        to_node.add_parent(from_node, label)\n",
    "        self.unique_labels.add(label)\n",
    "        if self.inverse_labels: #learnable inverse predicate relations from the R-GCN paper.\n",
    "            inverse_label = label + ':inv:'\n",
    "            from_node.add_child(to_node, inverse_label)\n",
    "            to_node.add_parent(from_node, inverse_label)\n",
    "            self.unique_labels.add(inverse_label)\n",
    "\n",
    "    def initialize_label_weights(self):\n",
    "        for label in list(self.unique_labels):\n",
    "            self.label_weights[label] = torch.rand(DIM_W,dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    def get_parents(self, id_):\n",
    "        node = self.nodes.get(id_)\n",
    "        return {parent.id: labels for parent, labels in node.parents.items()}\n",
    "\n",
    "    def get_children(self, id_):\n",
    "        node = self.nodes.get(id_)\n",
    "        return {child.id: labels for child, labels in node.children.items()}\n",
    "\n",
    "# pipeline: bfs_dep_tree --> loop (embedding = forward(collect_neighbours(bfs_dep_tree.get.pop()))  )\n",
    "# where the first item in bfs_dep_tree list is the node and the second is the gcn layer to be used.\n",
    "# collect neighbours also collects the edges for edge weights...\n",
    "#todo: create inverse relations for all relations in the graph (for example by annotating them with '-' and using - weights for them. \n",
    "#set rdfs:a weights to identity matrix??\n",
    "\n",
    "#idea save a queue snapshot when depth k-1 switches to depth k to pass with step() to next node.... i don't know how to find the edges after that...\n",
    "def bfs_dep_tree(start_node, max_depth):\n",
    "    queue = deque([(start_node, 0)])\n",
    "    dep_tree = []\n",
    "    visited = set()\n",
    "    \n",
    "    \n",
    "    while queue:\n",
    "        current_node, depth = queue.popleft()\n",
    "        if depth > max_depth:\n",
    "            break\n",
    "        # if depth == max_depth-1:\n",
    "        #     save_visited = visited\n",
    "        visited.add(current_node)\n",
    "        dep_tree.append((current_node, max(1,depth)))\n",
    "        for parent in current_node.parents:\n",
    "            if parent not in visited:\n",
    "                queue.append((parent, depth + 1))\n",
    "    return dep_tree#, save_visited\n",
    "\n",
    "def bfs_dep_tree_computation_skip(start_tree, start_node, start_edge, max_depth): \n",
    "    #same as bfs_dep_tree, but skipping redundant computation.\n",
    "    #intuition: like taking the dependency tree of the last node which is connected \n",
    "    #           and trimming the leaves and adding it to the next node, allowing us to skip\n",
    "    #           computation for the edge we just came from.\n",
    "    skip_edge = start_edge\n",
    "    queue = deque([(start_node,0)])\n",
    "    visited = set()\n",
    "    dep_tree = []\n",
    "\n",
    "    while queue:\n",
    "        current_node, depth =  queue.popleft()\n",
    "        if depth > max_depth-1:\n",
    "            save_visited = visited\n",
    "        if depth > max_depth:\n",
    "            break\n",
    "        visited.add(current_node)\n",
    "        if current_node.id in skip_edge: #logic incomplete... now it skips some edges to the node it came from???? im not sure about this...\n",
    "            continue\n",
    "        dep_tree.append((current_node, max(1,depth)))\n",
    "        for parent in current_node.parents:\n",
    "            if parent not in visited:\n",
    "                queue.append((parent, depth + 1))\n",
    "    return dep_tree, save_visited\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39da86-ddf0-49a5-99a1-01b0564a3bda",
   "metadata": {},
   "source": [
    "## Test some stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d0ac13-767d-47ac-86b6-22955bfdec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=Graph()\n",
    "nodes=[]\n",
    "num_nodes = 1000\n",
    "for n in range(0,num_nodes):\n",
    "    graph.add_node(n)\n",
    "    nodes.append(n)\n",
    "\n",
    "preds = [\"geometry:triangle\",\"rdfs:subClassOf\",\"FOAF:likes\",\"rdfs:subClassOf\",\"rdfs:domain\",\"FOAF:knows\",\"rdfs:isDefinedBy\"]\n",
    "for e in range(4000):\n",
    "    obj = random.choice(nodes)\n",
    "    subj = random.choice([node for node in nodes if node != obj])\n",
    "    pred = random.choice(preds)\n",
    "    graph.add_edge(obj,subj,pred)\n",
    "graph.initialize_label_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb594bf-c58a-400e-893e-1a836146a1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20d5981e-6176-4ef2-9b9b-4e19e1c750ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A = graph.nodes[0]\n",
    "# node = A\n",
    "# k_hops = 3\n",
    "\n",
    "# dep_tree = bfs_dep_tree(node, k_hops)\n",
    "# # dep_tree[-1][0].parents\n",
    "# # set([dep[0].id for dep in dep_tree])\n",
    "# print(dep_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6290f1b1-84f7-43a7-a9ce-a8ac7c277877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: 828: embedding: tensor([nan, nan, nan, nan, nan], grad_fn=<SelectBackward0>)\n",
      "node: 828: embedding: tensor([nan, nan, nan, nan, nan], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "graph.initialize_label_weights()\n",
    "target_node = graph.nodes[0]\n",
    "k_hops = 5\n",
    "r_gcn = R_GCN(x_dim=EMBEDDING_SIZE, y_dim=EMBEDDING_SIZE, graph=graph, max_k_hop=k_hops)\n",
    "WALK = 50\n",
    "node_list = [node for i, node in graph.nodes.items()]\n",
    "#I think node.step() can be replaced by a uniform random selection from the tree. This would also limit thingy... \n",
    "#  the computation skip on the dep tree. Just fix this later maybe. \n",
    "#Can also just construct the dataset and load it using a generator. \n",
    "#Dataset creation should be paralellizable with distributed or mpi if python has that.\n",
    "###check if dataset creation for GCN uses (informed)random walk or some random selection from list of nodes.\n",
    "###I think it's parralel full batch creation... \n",
    "#for i in range(WALK):\n",
    "while node_list:\n",
    "    target_node = node_list.pop()\n",
    "    r =  random.random() < 0.01\n",
    "    #if i == 0:\n",
    "    batch = bfs_dep_tree(target_node, k_hops)#,start_tree\n",
    "    # else:\n",
    "    #     skip_edge = (start_edge, last_node)\n",
    "    #     batch, step_queue = bfs_dep_tree_computation_skip(start_tree, target_node, skip_edge, k_hops)\n",
    "    print(f\"node: {target_node.id}: embedding: {target_node.embedding}\") if r else 5\n",
    "    updated_embedding = r_gcn(batch)\n",
    "    print(f\"node: {target_node.id}: embedding: {target_node.embedding}\") if r else 5\n",
    "    if r:\n",
    "        break\n",
    "    # try:\n",
    "    #     last_node = target_node\n",
    "    #     target_node, edge_label = target_node.step()\n",
    "    # except:\n",
    "    #     try:\n",
    "    #         last_node = target_node\n",
    "    #         target_node, edge_label = target_node.step()\n",
    "    #     except:\n",
    "    #         print(target_node.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c265e-18d6-4d61-8ccd-a21dff8a539a",
   "metadata": {},
   "source": [
    "## will make some parallel processing functions here once I understand the full training process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c62421a-0e2a-40f6-a0fc-0cd8d455453c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2787165626.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    workers = [Process(target=worker_num. args=)]\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "folder_loc = \"./ml4g/generator_batch\" \n",
    "from pickle import pickle\n",
    "from multiprocessing import Process, Queue\n",
    "import os\n",
    "\n",
    "def pkld(var,path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(var, f)\n",
    "\n",
    "def pkll(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        var = pickle.load(f)\n",
    "    return var\n",
    "\n",
    "def create_generator_folder(folder_loc): #consider modular full batch layer-wise processing if memory dep < some safety margin of ram/vram, etc\n",
    "    #else: parallel process micro batches if feasible\n",
    "    i = 0\n",
    "    while node_list:\n",
    "        target_node = node_list.pop()\n",
    "        #if i == 0:\n",
    "        batch, _ = bfs_dep_tree(target_node, k_hops)\n",
    "        pkld(batch, folder_loc+f\"/micro_batch_{i}.pkl\")\n",
    "        i += 1\n",
    "\n",
    "def generate_full_batch(folder_loc):\n",
    "    return [batch for batch in generate_micro_batches]\n",
    "\n",
    "\n",
    "def generate_micro_batches(folder_loc):\n",
    "    files = [file for file in os.listdir(folder_loc) if file.endswith('.pkl')]\n",
    "    for pkl in files:\n",
    "        yield pkll(pkl)\n",
    "\n",
    "def distribute_work(num_workers, micro_batches):\n",
    "    tasks = Queue()\n",
    "    output = Queue()\n",
    "    for task in micro_batches:\n",
    "        tasks.put(task)\n",
    "        forward_micro_batch(worker_num, task)\n",
    "    workers = [Process(target=worker_num. args=)]\n",
    "\n",
    "def forward_micro_batch(worker_num, micro_batch):\n",
    "    updated_embedding = await(r_gcn(micro_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de034418-e741-4f35-b776-a4c3c29aaa0f",
   "metadata": {},
   "source": [
    "## RDFS Process to Graph() object below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ec560-5ad6-47cd-850e-d3c4517fac33",
   "metadata": {},
   "outputs": [],
   "source": [
    ":)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186637d3-124d-4793-ba9c-eee99f87bbf4",
   "metadata": {},
   "source": [
    "### cashew: try graphgym package :)\n",
    "# Proposed GNN architecture:\n",
    "## Transformation block >>>\n",
    "### * linear\n",
    "### * batch norm * <  my intuition is that this could replace mean() operation, just use vectorized sum() to reduce computational complexity.\n",
    "### * dropout * < On linear layer in the message function\n",
    "### * activation * < parametric relu = max(x,0) + alpha * min(x,0) ... alpha is trainable.\n",
    "### * attention * < I'm not sure if relational weights as in RGCN fall into this category. If they are complementary in any way.\n",
    "## <<< End transformation block\n",
    "### * aggregation by some problem dependant function i.e. mean(), min/max/avg..._pooling(), lstm(cat(edge_embeddings)) ...\n",
    "#### aggregation note: inverted degree matrix * adjacency matrix = avg(adjacency matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c958c6-c608-46dc-9be5-7f95a50582cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5be97b1f-8733-4f96-860c-8f66a81e8b90",
   "metadata": {},
   "source": [
    "# classical GCN:\n",
    "\n",
    "## important design choice here... use batch norm after each layer? Normalize explicitly?\n",
    "## messages = layer weight * normalized messages from prev layers\n",
    "## aggregation = sum(messages) --> relu\n",
    "\n",
    "### todo: add weighted average method. I.e. learnable row vector of size feature dim (must vectorize torch.mean explicitly for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0813b833-f070-43cf-9d95-0233534591f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "205882e8-df3d-407f-9a5d-69668314c372",
   "metadata": {},
   "source": [
    "# GraphSAGE\n",
    "## aggregate incoming messages (can be mean(messages, dim=0), can be a max pooling on mlp(message), can be LSTM(shuffle(messages) as mini-batch), can be sum without average (maybe this leads to batch norm later?)\n",
    "## concat current node message --> relu --> send\n",
    "\n",
    "## Uses L2 Norm as root squared error of embeddings at every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a23ab-9bd8-45a4-bfb1-5615bd73e494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7de03bc5-9d73-44ba-ad3c-dfde101d4eb7",
   "metadata": {},
   "source": [
    "# GAT\n",
    "\n",
    "# Architectural notes:\n",
    "\n",
    "## GCN, but vector weighting matrix is learned (which nodes to attend to and ./ unsure./ which parts of the embedding vectors/features to attend to \\.\\.\n",
    "\n",
    "### how can we handle permutation invariance?\n",
    "\n",
    "### seems that attention weights are graph conditional on search algorithm dependant\n",
    "\n",
    "### ^ wrong. It's a function of (and on) embeddings of different node embeddings at the previous step.\n",
    "\n",
    "### softmax ()\n",
    "\n",
    "### parameter matrix a can be a parameter matrix on a learned single layer mlp that processes the concatenated input vectors.\n",
    "\n",
    "### parameter matrix a is learned together with weight matrix w.\n",
    "\n",
    "### multi-head attention, multiple relu(a) matrices.\n",
    "\n",
    "### each a is initialized randomly, then aggregated to produce a single output\n",
    "\n",
    "### can be parallellized worker per message.\n",
    "\n",
    "### sparse matrix... fixed number of parameters.\n",
    "\n",
    "# Implications/discussion\n",
    "\n",
    "### asymmetric importance weighting\n",
    "\n",
    "### weights are still independant of graph size, even though more complex analysis of the graph can be performed.\n",
    "\n",
    "### graph attention mechanism scales linearly in graph size due to locality\n",
    "\n",
    "### cool visualization of attention mechanism (implicit clustering) cora citation paper\n",
    "\n",
    "### improved performance over GCN in some cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94542b15-3eab-42a7-bbe3-7410cf035385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d035a923-461f-4dc5-8453-d3faa6f66f60",
   "metadata": {},
   "source": [
    "# Test stuff and transform into main() below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b355090-c13a-4404-823a-60f59a9b1a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
